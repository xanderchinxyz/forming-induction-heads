{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bfcbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "from model import AttentionOnlyTransformer\n",
    "from generate_data import generate_rrt\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(67)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "SEQ_LEN = 20\n",
    "MAX_CONTEXT_LEN = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e12a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: torch.Size([32, 80]), Prefix Length: 5\n",
      "First sequence sample (first 10 tokens):\n",
      "tensor([120, 167, 140,  36, 246, 120, 167, 140,  36, 246])\n"
     ]
    }
   ],
   "source": [
    "seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=5)\n",
    "\n",
    "print(f\"Dataset Shape: {data.shape}, Prefix Length: {seq_len}\")\n",
    "print(f\"First sequence sample (first 10 tokens):\\n{data[0, :10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fea9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 65,536\n",
      "\n",
      "Model architecture:\n",
      "AttentionOnlyTransformer(\n",
      "  (token_embedding): Embedding(256, 64)\n",
      "  (attention_blocks): ModuleList(\n",
      "    (0-1): 2 x AttentionBlock(\n",
      "      (W_qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "      (W_o): Linear(in_features=64, out_features=64, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (unembed): Linear(in_features=64, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = AttentionOnlyTransformer(\n",
    "    vocab_size=256,\n",
    "    d_model=64,\n",
    "    n_layers=NUM_LAYERS,\n",
    "    n_heads=4,\n",
    "    max_context_len=MAX_CONTEXT_LEN,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nModel architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021916e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=5.8052, Acc=0.63%, Induction Acc=0.58%\n",
      "Epoch 10: Loss=5.7559, Acc=0.24%, Induction Acc=0.21%\n",
      "Epoch 20: Loss=5.7032, Acc=0.71%, Induction Acc=0.74%\n",
      "Epoch 30: Loss=5.7337, Acc=0.12%, Induction Acc=0.11%\n",
      "Epoch 40: Loss=5.6927, Acc=0.00%, Induction Acc=0.00%\n",
      "Epoch 50: Loss=5.7095, Acc=0.51%, Induction Acc=0.58%\n",
      "Epoch 60: Loss=5.6723, Acc=0.59%, Induction Acc=0.64%\n",
      "Epoch 70: Loss=5.6804, Acc=0.51%, Induction Acc=0.69%\n",
      "Epoch 80: Loss=5.6735, Acc=0.47%, Induction Acc=0.53%\n",
      "Epoch 90: Loss=5.6710, Acc=0.12%, Induction Acc=0.11%\n",
      "Epoch 100: Loss=5.6566, Acc=0.36%, Induction Acc=0.32%\n",
      "Epoch 110: Loss=5.6449, Acc=0.36%, Induction Acc=0.48%\n",
      "Epoch 120: Loss=5.6389, Acc=0.40%, Induction Acc=0.48%\n",
      "Epoch 130: Loss=5.6425, Acc=0.40%, Induction Acc=0.48%\n",
      "Epoch 140: Loss=5.6304, Acc=0.40%, Induction Acc=0.53%\n",
      "Epoch 150: Loss=5.6132, Acc=0.67%, Induction Acc=0.74%\n",
      "Epoch 160: Loss=5.5946, Acc=0.47%, Induction Acc=0.53%\n",
      "Epoch 170: Loss=5.6337, Acc=0.95%, Induction Acc=1.06%\n",
      "Epoch 180: Loss=5.5992, Acc=1.19%, Induction Acc=1.32%\n",
      "Epoch 190: Loss=5.5888, Acc=1.23%, Induction Acc=1.48%\n",
      "Epoch 200: Loss=5.5734, Acc=1.23%, Induction Acc=1.48%\n",
      "Epoch 210: Loss=5.5930, Acc=0.67%, Induction Acc=0.90%\n",
      "Epoch 220: Loss=5.5567, Acc=1.66%, Induction Acc=2.12%\n",
      "Epoch 230: Loss=5.5755, Acc=2.10%, Induction Acc=2.65%\n",
      "Epoch 240: Loss=5.5338, Acc=2.18%, Induction Acc=2.65%\n",
      "Epoch 250: Loss=5.5780, Acc=2.10%, Induction Acc=2.65%\n",
      "Epoch 260: Loss=5.5344, Acc=1.86%, Induction Acc=2.38%\n",
      "Epoch 270: Loss=5.5083, Acc=2.65%, Induction Acc=3.28%\n",
      "Epoch 280: Loss=5.4884, Acc=2.89%, Induction Acc=3.50%\n",
      "Epoch 290: Loss=5.5051, Acc=2.53%, Induction Acc=3.18%\n",
      "Epoch 300: Loss=5.5304, Acc=2.85%, Induction Acc=3.44%\n",
      "Epoch 310: Loss=5.4977, Acc=2.41%, Induction Acc=2.91%\n",
      "Epoch 320: Loss=5.4868, Acc=3.05%, Induction Acc=3.71%\n",
      "Epoch 330: Loss=5.4638, Acc=3.56%, Induction Acc=4.40%\n",
      "Epoch 340: Loss=5.4648, Acc=3.28%, Induction Acc=4.13%\n",
      "Epoch 350: Loss=5.4336, Acc=2.89%, Induction Acc=3.55%\n",
      "Epoch 360: Loss=5.4588, Acc=3.44%, Induction Acc=4.18%\n",
      "Epoch 370: Loss=5.4663, Acc=3.80%, Induction Acc=4.87%\n",
      "Epoch 380: Loss=5.3955, Acc=4.11%, Induction Acc=5.35%\n",
      "Epoch 390: Loss=5.4163, Acc=3.88%, Induction Acc=4.98%\n",
      "Epoch 400: Loss=5.4302, Acc=3.96%, Induction Acc=4.98%\n",
      "Epoch 410: Loss=5.3795, Acc=3.68%, Induction Acc=4.82%\n",
      "Epoch 420: Loss=5.3954, Acc=3.76%, Induction Acc=4.71%\n",
      "Epoch 430: Loss=5.4141, Acc=3.96%, Induction Acc=5.03%\n",
      "Epoch 440: Loss=5.3781, Acc=4.35%, Induction Acc=5.46%\n",
      "Epoch 450: Loss=5.3732, Acc=4.55%, Induction Acc=5.77%\n",
      "Epoch 460: Loss=5.3384, Acc=5.54%, Induction Acc=6.94%\n",
      "Epoch 470: Loss=5.3436, Acc=4.71%, Induction Acc=5.93%\n",
      "Epoch 480: Loss=5.3434, Acc=4.47%, Induction Acc=5.67%\n",
      "Epoch 490: Loss=5.3217, Acc=5.42%, Induction Acc=6.73%\n",
      "Epoch 500: Loss=5.3232, Acc=5.34%, Induction Acc=6.73%\n",
      "Epoch 510: Loss=5.3174, Acc=5.06%, Induction Acc=6.67%\n",
      "Epoch 520: Loss=5.3023, Acc=4.94%, Induction Acc=6.41%\n",
      "Epoch 530: Loss=5.3111, Acc=5.14%, Induction Acc=6.51%\n",
      "Epoch 540: Loss=5.3179, Acc=5.58%, Induction Acc=7.10%\n",
      "Epoch 550: Loss=5.2582, Acc=5.58%, Induction Acc=6.89%\n",
      "Epoch 560: Loss=5.2558, Acc=5.81%, Induction Acc=7.57%\n",
      "Epoch 570: Loss=5.2879, Acc=4.79%, Induction Acc=6.09%\n",
      "Epoch 580: Loss=5.2278, Acc=6.37%, Induction Acc=8.16%\n",
      "Epoch 590: Loss=5.2328, Acc=4.94%, Induction Acc=6.25%\n",
      "Epoch 600: Loss=5.2258, Acc=5.97%, Induction Acc=7.63%\n",
      "Epoch 610: Loss=5.2418, Acc=5.62%, Induction Acc=7.31%\n",
      "Epoch 620: Loss=5.2034, Acc=6.41%, Induction Acc=8.10%\n",
      "Epoch 630: Loss=5.1942, Acc=6.53%, Induction Acc=8.47%\n",
      "Epoch 640: Loss=5.2058, Acc=5.54%, Induction Acc=7.26%\n",
      "Epoch 650: Loss=5.1990, Acc=5.89%, Induction Acc=7.68%\n",
      "Epoch 660: Loss=5.1805, Acc=5.62%, Induction Acc=7.15%\n",
      "Epoch 670: Loss=5.1244, Acc=6.69%, Induction Acc=8.58%\n",
      "Epoch 680: Loss=5.1330, Acc=7.40%, Induction Acc=9.43%\n",
      "Epoch 690: Loss=5.1396, Acc=7.24%, Induction Acc=9.32%\n",
      "Epoch 700: Loss=5.1108, Acc=6.92%, Induction Acc=8.85%\n",
      "Epoch 710: Loss=5.1121, Acc=6.84%, Induction Acc=8.90%\n",
      "Epoch 720: Loss=5.0711, Acc=8.31%, Induction Acc=10.75%\n",
      "Epoch 730: Loss=5.0734, Acc=7.83%, Induction Acc=10.12%\n",
      "Epoch 740: Loss=5.0643, Acc=8.35%, Induction Acc=10.86%\n",
      "Epoch 750: Loss=5.0542, Acc=9.26%, Induction Acc=12.08%\n",
      "Epoch 760: Loss=5.0374, Acc=7.75%, Induction Acc=9.96%\n",
      "Epoch 770: Loss=5.0538, Acc=7.04%, Induction Acc=9.00%\n",
      "Epoch 780: Loss=4.9639, Acc=9.97%, Induction Acc=12.76%\n",
      "Epoch 790: Loss=5.0262, Acc=8.86%, Induction Acc=11.65%\n",
      "Epoch 800: Loss=4.9847, Acc=8.43%, Induction Acc=10.81%\n",
      "Epoch 810: Loss=4.9475, Acc=9.77%, Induction Acc=12.66%\n",
      "Epoch 820: Loss=5.0183, Acc=7.95%, Induction Acc=10.43%\n",
      "Epoch 830: Loss=4.9261, Acc=10.13%, Induction Acc=13.19%\n",
      "Epoch 840: Loss=4.9134, Acc=8.94%, Induction Acc=11.49%\n",
      "Epoch 850: Loss=4.9318, Acc=10.17%, Induction Acc=13.08%\n",
      "Epoch 860: Loss=4.9611, Acc=9.69%, Induction Acc=12.45%\n",
      "Epoch 870: Loss=4.9508, Acc=9.57%, Induction Acc=12.45%\n",
      "Epoch 880: Loss=4.8532, Acc=10.64%, Induction Acc=13.67%\n",
      "Epoch 890: Loss=4.9089, Acc=9.93%, Induction Acc=12.71%\n",
      "Epoch 900: Loss=4.8898, Acc=11.04%, Induction Acc=14.35%\n",
      "Epoch 910: Loss=4.8550, Acc=11.04%, Induction Acc=14.46%\n",
      "Epoch 920: Loss=4.8434, Acc=10.44%, Induction Acc=13.51%\n",
      "Epoch 930: Loss=4.7801, Acc=11.27%, Induction Acc=14.51%\n",
      "Epoch 940: Loss=4.8125, Acc=11.83%, Induction Acc=15.47%\n",
      "Epoch 950: Loss=4.7296, Acc=11.47%, Induction Acc=14.94%\n",
      "Epoch 960: Loss=4.7080, Acc=13.53%, Induction Acc=17.64%\n",
      "Epoch 970: Loss=4.8027, Acc=12.58%, Induction Acc=16.42%\n",
      "Epoch 980: Loss=4.7050, Acc=12.70%, Induction Acc=16.53%\n",
      "Epoch 990: Loss=4.7207, Acc=14.00%, Induction Acc=18.27%\n",
      "Epoch 1000: Loss=4.6217, Acc=15.43%, Induction Acc=19.86%\n",
      "Epoch 1010: Loss=4.6834, Acc=14.32%, Induction Acc=18.59%\n",
      "Epoch 1020: Loss=4.6333, Acc=13.65%, Induction Acc=17.90%\n",
      "Epoch 1030: Loss=4.6270, Acc=14.79%, Induction Acc=19.33%\n",
      "Epoch 1040: Loss=4.6153, Acc=13.21%, Induction Acc=17.37%\n",
      "Epoch 1050: Loss=4.5672, Acc=13.84%, Induction Acc=17.85%\n",
      "Epoch 1060: Loss=4.5643, Acc=15.07%, Induction Acc=19.81%\n",
      "Epoch 1070: Loss=4.5328, Acc=15.35%, Induction Acc=20.23%\n",
      "Epoch 1080: Loss=4.5470, Acc=17.33%, Induction Acc=22.67%\n",
      "Epoch 1090: Loss=4.5464, Acc=15.86%, Induction Acc=20.71%\n",
      "Epoch 1100: Loss=4.5022, Acc=18.00%, Induction Acc=23.57%\n",
      "Epoch 1110: Loss=4.4747, Acc=16.85%, Induction Acc=21.93%\n",
      "Epoch 1120: Loss=4.3870, Acc=19.26%, Induction Acc=25.48%\n",
      "Epoch 1130: Loss=4.3883, Acc=20.06%, Induction Acc=26.38%\n",
      "Epoch 1140: Loss=4.3479, Acc=18.12%, Induction Acc=24.10%\n",
      "Epoch 1150: Loss=4.3329, Acc=20.33%, Induction Acc=26.54%\n",
      "Epoch 1160: Loss=4.3540, Acc=18.00%, Induction Acc=23.68%\n",
      "Epoch 1170: Loss=4.3221, Acc=20.06%, Induction Acc=26.32%\n",
      "Epoch 1180: Loss=4.2552, Acc=21.84%, Induction Acc=28.76%\n",
      "Epoch 1190: Loss=4.2218, Acc=22.07%, Induction Acc=29.08%\n",
      "Epoch 1200: Loss=4.1778, Acc=22.11%, Induction Acc=29.03%\n",
      "Epoch 1210: Loss=4.1371, Acc=23.89%, Induction Acc=31.67%\n",
      "Epoch 1220: Loss=4.1076, Acc=24.49%, Induction Acc=32.10%\n",
      "Epoch 1230: Loss=4.0741, Acc=24.29%, Induction Acc=32.10%\n",
      "Epoch 1240: Loss=4.0898, Acc=26.46%, Induction Acc=34.48%\n",
      "Epoch 1250: Loss=3.9841, Acc=27.25%, Induction Acc=35.86%\n",
      "Epoch 1260: Loss=3.9029, Acc=27.57%, Induction Acc=36.23%\n",
      "Epoch 1270: Loss=3.8958, Acc=29.00%, Induction Acc=38.29%\n",
      "Epoch 1280: Loss=3.9152, Acc=28.28%, Induction Acc=37.39%\n",
      "Epoch 1290: Loss=3.9098, Acc=28.80%, Induction Acc=38.14%\n",
      "Epoch 1300: Loss=3.7771, Acc=32.67%, Induction Acc=43.38%\n",
      "Epoch 1310: Loss=3.6809, Acc=33.47%, Induction Acc=44.17%\n",
      "Epoch 1320: Loss=3.6848, Acc=33.07%, Induction Acc=43.75%\n",
      "Epoch 1330: Loss=3.6603, Acc=33.70%, Induction Acc=44.60%\n",
      "Epoch 1340: Loss=3.5888, Acc=35.96%, Induction Acc=47.67%\n",
      "Epoch 1350: Loss=3.5301, Acc=37.10%, Induction Acc=49.10%\n",
      "Epoch 1360: Loss=3.4783, Acc=37.90%, Induction Acc=50.26%\n",
      "Epoch 1370: Loss=3.4972, Acc=38.81%, Induction Acc=51.64%\n",
      "Epoch 1380: Loss=3.3268, Acc=42.17%, Induction Acc=56.14%\n",
      "Epoch 1390: Loss=3.3414, Acc=41.14%, Induction Acc=54.50%\n",
      "Epoch 1400: Loss=3.2426, Acc=43.16%, Induction Acc=57.26%\n",
      "Epoch 1410: Loss=3.2042, Acc=45.81%, Induction Acc=60.59%\n",
      "Epoch 1420: Loss=3.1084, Acc=47.07%, Induction Acc=62.34%\n",
      "Epoch 1430: Loss=3.1273, Acc=45.69%, Induction Acc=60.96%\n",
      "Epoch 1440: Loss=3.0225, Acc=48.93%, Induction Acc=65.10%\n",
      "Epoch 1450: Loss=2.9432, Acc=51.23%, Induction Acc=67.85%\n",
      "Epoch 1460: Loss=2.9760, Acc=51.54%, Induction Acc=68.38%\n",
      "Epoch 1470: Loss=2.8660, Acc=53.16%, Induction Acc=70.60%\n",
      "Epoch 1480: Loss=2.8780, Acc=52.85%, Induction Acc=70.23%\n",
      "Epoch 1490: Loss=2.6442, Acc=57.16%, Induction Acc=76.06%\n",
      "Epoch 1500: Loss=2.6901, Acc=57.20%, Induction Acc=75.95%\n",
      "Epoch 1510: Loss=2.6691, Acc=58.31%, Induction Acc=77.28%\n",
      "Epoch 1520: Loss=2.5721, Acc=59.81%, Induction Acc=79.45%\n",
      "Epoch 1530: Loss=2.6187, Acc=59.45%, Induction Acc=79.08%\n",
      "Epoch 1540: Loss=2.6120, Acc=60.21%, Induction Acc=80.19%\n",
      "Epoch 1550: Loss=2.5208, Acc=60.25%, Induction Acc=80.19%\n",
      "Epoch 1560: Loss=2.4796, Acc=61.39%, Induction Acc=81.62%\n",
      "Epoch 1570: Loss=2.3874, Acc=64.08%, Induction Acc=85.28%\n",
      "Epoch 1580: Loss=2.4332, Acc=62.26%, Induction Acc=82.94%\n",
      "Epoch 1590: Loss=2.4168, Acc=63.05%, Induction Acc=83.85%\n",
      "Epoch 1600: Loss=2.2827, Acc=64.83%, Induction Acc=86.33%\n",
      "Epoch 1610: Loss=2.3127, Acc=65.86%, Induction Acc=87.61%\n",
      "Epoch 1620: Loss=2.2560, Acc=66.22%, Induction Acc=87.92%\n",
      "Epoch 1630: Loss=2.2219, Acc=66.89%, Induction Acc=89.30%\n",
      "Epoch 1640: Loss=2.1944, Acc=67.52%, Induction Acc=89.88%\n",
      "Epoch 1650: Loss=2.1024, Acc=69.03%, Induction Acc=91.79%\n",
      "Epoch 1660: Loss=2.1102, Acc=69.78%, Induction Acc=93.17%\n",
      "Epoch 1670: Loss=2.1132, Acc=69.11%, Induction Acc=92.11%\n",
      "Epoch 1680: Loss=2.0281, Acc=69.70%, Induction Acc=92.85%\n",
      "Epoch 1690: Loss=2.1533, Acc=68.51%, Induction Acc=90.68%\n",
      "Epoch 1700: Loss=2.0924, Acc=69.54%, Induction Acc=92.43%\n",
      "Epoch 1710: Loss=2.0603, Acc=69.50%, Induction Acc=92.43%\n",
      "Epoch 1720: Loss=2.0349, Acc=70.61%, Induction Acc=93.91%\n",
      "Epoch 1730: Loss=1.9827, Acc=71.16%, Induction Acc=94.60%\n",
      "Epoch 1740: Loss=2.0221, Acc=70.93%, Induction Acc=94.07%\n",
      "Epoch 1750: Loss=1.9576, Acc=71.32%, Induction Acc=94.92%\n",
      "Epoch 1760: Loss=1.9705, Acc=71.80%, Induction Acc=95.29%\n",
      "Epoch 1770: Loss=1.9855, Acc=70.89%, Induction Acc=94.49%\n",
      "Epoch 1780: Loss=1.9864, Acc=71.28%, Induction Acc=94.70%\n",
      "Epoch 1790: Loss=1.9209, Acc=72.27%, Induction Acc=96.08%\n",
      "Epoch 1800: Loss=1.8928, Acc=72.59%, Induction Acc=96.40%\n",
      "Epoch 1810: Loss=1.8393, Acc=73.34%, Induction Acc=97.25%\n",
      "Epoch 1820: Loss=1.9101, Acc=72.90%, Induction Acc=96.66%\n",
      "Epoch 1830: Loss=1.8866, Acc=72.90%, Induction Acc=96.77%\n",
      "Epoch 1840: Loss=1.8380, Acc=72.94%, Induction Acc=96.88%\n",
      "Epoch 1850: Loss=1.8288, Acc=73.58%, Induction Acc=97.46%\n",
      "Epoch 1860: Loss=1.8184, Acc=73.50%, Induction Acc=97.40%\n",
      "Epoch 1870: Loss=1.8264, Acc=72.98%, Induction Acc=97.09%\n",
      "Epoch 1880: Loss=1.8017, Acc=73.93%, Induction Acc=97.99%\n",
      "Epoch 1890: Loss=1.8178, Acc=73.30%, Induction Acc=97.25%\n",
      "Epoch 1900: Loss=1.7679, Acc=73.85%, Induction Acc=97.99%\n",
      "Epoch 1910: Loss=1.7387, Acc=74.49%, Induction Acc=98.57%\n",
      "Epoch 1920: Loss=1.7525, Acc=74.72%, Induction Acc=98.73%\n",
      "Epoch 1930: Loss=1.7902, Acc=73.97%, Induction Acc=97.83%\n",
      "Epoch 1940: Loss=1.7014, Acc=75.12%, Induction Acc=99.31%\n",
      "Epoch 1950: Loss=1.7178, Acc=74.68%, Induction Acc=98.83%\n",
      "Epoch 1960: Loss=1.7223, Acc=74.80%, Induction Acc=98.83%\n",
      "Epoch 1970: Loss=1.7415, Acc=74.88%, Induction Acc=98.94%\n",
      "Epoch 1980: Loss=1.7238, Acc=74.96%, Induction Acc=98.89%\n",
      "Epoch 1990: Loss=1.7062, Acc=75.16%, Induction Acc=99.10%\n",
      "Epoch 2000: Loss=1.6956, Acc=74.96%, Induction Acc=98.62%\n",
      "Epoch 2010: Loss=1.6903, Acc=75.24%, Induction Acc=99.26%\n",
      "Epoch 2020: Loss=1.7033, Acc=75.16%, Induction Acc=98.99%\n",
      "Epoch 2030: Loss=1.7046, Acc=74.68%, Induction Acc=98.46%\n",
      "Epoch 2040: Loss=1.6690, Acc=75.36%, Induction Acc=99.58%\n",
      "Epoch 2050: Loss=1.6669, Acc=75.16%, Induction Acc=98.94%\n",
      "Epoch 2060: Loss=1.6876, Acc=75.12%, Induction Acc=99.10%\n",
      "Epoch 2070: Loss=1.6179, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 2080: Loss=1.6266, Acc=75.47%, Induction Acc=99.42%\n",
      "Epoch 2090: Loss=1.6398, Acc=75.44%, Induction Acc=99.47%\n",
      "Epoch 2100: Loss=1.6525, Acc=75.67%, Induction Acc=99.68%\n",
      "Epoch 2110: Loss=1.6561, Acc=75.08%, Induction Acc=99.10%\n",
      "Epoch 2120: Loss=1.6469, Acc=75.32%, Induction Acc=99.31%\n",
      "Epoch 2130: Loss=1.6281, Acc=75.91%, Induction Acc=100.00%\n",
      "Epoch 2140: Loss=1.6309, Acc=75.51%, Induction Acc=99.52%\n",
      "Epoch 2150: Loss=1.6246, Acc=75.67%, Induction Acc=99.68%\n",
      "Epoch 2160: Loss=1.6476, Acc=75.24%, Induction Acc=98.99%\n",
      "Epoch 2170: Loss=1.6258, Acc=75.44%, Induction Acc=99.58%\n",
      "Epoch 2180: Loss=1.6006, Acc=75.91%, Induction Acc=99.95%\n",
      "Epoch 2190: Loss=1.5955, Acc=75.47%, Induction Acc=99.52%\n",
      "Epoch 2200: Loss=1.5944, Acc=75.63%, Induction Acc=99.42%\n",
      "Epoch 2210: Loss=1.6018, Acc=75.63%, Induction Acc=99.47%\n",
      "Epoch 2220: Loss=1.6018, Acc=75.83%, Induction Acc=99.68%\n",
      "Epoch 2230: Loss=1.6056, Acc=75.55%, Induction Acc=99.58%\n",
      "Epoch 2240: Loss=1.5964, Acc=75.71%, Induction Acc=99.74%\n",
      "Epoch 2250: Loss=1.5726, Acc=75.67%, Induction Acc=99.63%\n",
      "Epoch 2260: Loss=1.5851, Acc=75.47%, Induction Acc=99.42%\n",
      "Epoch 2270: Loss=1.6004, Acc=75.55%, Induction Acc=99.42%\n",
      "Epoch 2280: Loss=1.5713, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 2290: Loss=1.5715, Acc=75.75%, Induction Acc=99.74%\n",
      "Epoch 2300: Loss=1.5631, Acc=75.79%, Induction Acc=99.89%\n",
      "Epoch 2310: Loss=1.5869, Acc=75.71%, Induction Acc=99.68%\n",
      "Epoch 2320: Loss=1.5652, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 2330: Loss=1.5757, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 2340: Loss=1.5675, Acc=75.75%, Induction Acc=99.68%\n",
      "Epoch 2350: Loss=1.5718, Acc=75.75%, Induction Acc=99.84%\n",
      "Epoch 2360: Loss=1.5811, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 2370: Loss=1.5378, Acc=75.71%, Induction Acc=99.68%\n",
      "Epoch 2380: Loss=1.5502, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 2390: Loss=1.5393, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 2400: Loss=1.5579, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 2410: Loss=1.5471, Acc=75.71%, Induction Acc=99.74%\n",
      "Epoch 2420: Loss=1.5339, Acc=75.91%, Induction Acc=99.74%\n",
      "Epoch 2430: Loss=1.5346, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 2440: Loss=1.5159, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 2450: Loss=1.5233, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 2460: Loss=1.5378, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 2470: Loss=1.5280, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 2480: Loss=1.5302, Acc=75.87%, Induction Acc=99.68%\n",
      "Epoch 2490: Loss=1.5036, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 2500: Loss=1.5304, Acc=75.79%, Induction Acc=99.74%\n",
      "Epoch 2510: Loss=1.5317, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 2520: Loss=1.5241, Acc=75.75%, Induction Acc=99.52%\n",
      "Epoch 2530: Loss=1.5226, Acc=75.87%, Induction Acc=99.89%\n",
      "Epoch 2540: Loss=1.5113, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 2550: Loss=1.5228, Acc=75.95%, Induction Acc=99.79%\n",
      "Epoch 2560: Loss=1.5142, Acc=76.03%, Induction Acc=99.84%\n",
      "Epoch 2570: Loss=1.5176, Acc=75.79%, Induction Acc=99.68%\n",
      "Epoch 2580: Loss=1.5199, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 2590: Loss=1.5228, Acc=75.59%, Induction Acc=99.52%\n",
      "Epoch 2600: Loss=1.4991, Acc=75.87%, Induction Acc=99.89%\n",
      "Epoch 2610: Loss=1.5118, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 2620: Loss=1.5101, Acc=76.07%, Induction Acc=99.95%\n",
      "Epoch 2630: Loss=1.5127, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 2640: Loss=1.4930, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 2650: Loss=1.5071, Acc=75.87%, Induction Acc=99.68%\n",
      "Epoch 2660: Loss=1.4837, Acc=75.91%, Induction Acc=99.95%\n",
      "Epoch 2670: Loss=1.4987, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 2680: Loss=1.4915, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 2690: Loss=1.4822, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 2700: Loss=1.4872, Acc=75.75%, Induction Acc=99.68%\n",
      "Epoch 2710: Loss=1.4945, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 2720: Loss=1.4950, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 2730: Loss=1.4829, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 2740: Loss=1.4862, Acc=75.67%, Induction Acc=99.63%\n",
      "Epoch 2750: Loss=1.4818, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 2760: Loss=1.4794, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 2770: Loss=1.4870, Acc=76.19%, Induction Acc=100.00%\n",
      "Epoch 2780: Loss=1.4755, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 2790: Loss=1.4873, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 2800: Loss=1.4853, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 2810: Loss=1.4799, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 2820: Loss=1.4747, Acc=75.79%, Induction Acc=99.84%\n",
      "Epoch 2830: Loss=1.4842, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 2840: Loss=1.4802, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 2850: Loss=1.4757, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 2860: Loss=1.4818, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 2870: Loss=1.4750, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 2880: Loss=1.4717, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 2890: Loss=1.4829, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 2900: Loss=1.4757, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 2910: Loss=1.4768, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 2920: Loss=1.4642, Acc=76.19%, Induction Acc=100.00%\n",
      "Epoch 2930: Loss=1.4784, Acc=75.87%, Induction Acc=99.89%\n",
      "Epoch 2940: Loss=1.4618, Acc=76.03%, Induction Acc=99.89%\n",
      "Epoch 2950: Loss=1.4839, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 2960: Loss=1.4703, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 2970: Loss=1.4467, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 2980: Loss=1.4547, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 2990: Loss=1.4442, Acc=76.07%, Induction Acc=99.79%\n",
      "Epoch 3000: Loss=1.4625, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3010: Loss=1.4494, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3020: Loss=1.4650, Acc=75.79%, Induction Acc=99.68%\n",
      "Epoch 3030: Loss=1.4539, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3040: Loss=1.4742, Acc=75.87%, Induction Acc=99.84%\n",
      "Epoch 3050: Loss=1.4468, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 3060: Loss=1.4489, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 3070: Loss=1.4286, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 3080: Loss=1.4376, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 3090: Loss=1.4508, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3100: Loss=1.4471, Acc=75.87%, Induction Acc=99.84%\n",
      "Epoch 3110: Loss=1.4511, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 3120: Loss=1.4537, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 3130: Loss=1.4357, Acc=76.03%, Induction Acc=99.89%\n",
      "Epoch 3140: Loss=1.4460, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3150: Loss=1.4420, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 3160: Loss=1.4544, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 3170: Loss=1.4423, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3180: Loss=1.4331, Acc=76.15%, Induction Acc=100.00%\n",
      "Epoch 3190: Loss=1.4492, Acc=75.87%, Induction Acc=99.89%\n",
      "Epoch 3200: Loss=1.4398, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3210: Loss=1.4362, Acc=76.03%, Induction Acc=99.89%\n",
      "Epoch 3220: Loss=1.4351, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3230: Loss=1.4439, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3240: Loss=1.4384, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 3250: Loss=1.4383, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 3260: Loss=1.4252, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3270: Loss=1.4471, Acc=76.15%, Induction Acc=100.00%\n",
      "Epoch 3280: Loss=1.4384, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3290: Loss=1.4369, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3300: Loss=1.4398, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3310: Loss=1.4308, Acc=76.15%, Induction Acc=100.00%\n",
      "Epoch 3320: Loss=1.4150, Acc=76.15%, Induction Acc=100.00%\n",
      "Epoch 3330: Loss=1.4283, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3340: Loss=1.4154, Acc=76.15%, Induction Acc=100.00%\n",
      "Epoch 3350: Loss=1.4277, Acc=76.19%, Induction Acc=100.00%\n",
      "Epoch 3360: Loss=1.4299, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3370: Loss=1.4373, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3380: Loss=1.4326, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3390: Loss=1.4231, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3400: Loss=1.4216, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3410: Loss=1.4327, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3420: Loss=1.4179, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3430: Loss=1.4252, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3440: Loss=1.4210, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3450: Loss=1.4279, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 3460: Loss=1.4298, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3470: Loss=1.4251, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3480: Loss=1.4131, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3490: Loss=1.4319, Acc=75.79%, Induction Acc=99.58%\n",
      "Epoch 3500: Loss=1.4194, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3510: Loss=1.4140, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 3520: Loss=1.4109, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3530: Loss=1.4338, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 3540: Loss=1.4270, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3550: Loss=1.4179, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 3560: Loss=1.4112, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3570: Loss=1.4272, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 3580: Loss=1.4203, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3590: Loss=1.4038, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3600: Loss=1.4066, Acc=76.15%, Induction Acc=100.00%\n",
      "Epoch 3610: Loss=1.4153, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3620: Loss=1.4283, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3630: Loss=1.4155, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3640: Loss=1.4200, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3650: Loss=1.4283, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 3660: Loss=1.4167, Acc=75.79%, Induction Acc=99.74%\n",
      "Epoch 3670: Loss=1.4092, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3680: Loss=1.4254, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3690: Loss=1.4118, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3700: Loss=1.4131, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 3710: Loss=1.4218, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3720: Loss=1.4145, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 3730: Loss=1.4138, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 3740: Loss=1.4106, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 3750: Loss=1.4199, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 3760: Loss=1.4054, Acc=76.19%, Induction Acc=100.00%\n",
      "Epoch 3770: Loss=1.4163, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3780: Loss=1.4143, Acc=75.91%, Induction Acc=99.95%\n",
      "Epoch 3790: Loss=1.3986, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 3800: Loss=1.4010, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3810: Loss=1.4079, Acc=76.07%, Induction Acc=99.95%\n",
      "Epoch 3820: Loss=1.4025, Acc=75.99%, Induction Acc=99.84%\n",
      "Epoch 3830: Loss=1.4070, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3840: Loss=1.4115, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3850: Loss=1.4191, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3860: Loss=1.4097, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3870: Loss=1.4077, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 3880: Loss=1.4021, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3890: Loss=1.4033, Acc=75.91%, Induction Acc=99.95%\n",
      "Epoch 3900: Loss=1.3926, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3910: Loss=1.4115, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 3920: Loss=1.4042, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3930: Loss=1.3956, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3940: Loss=1.4038, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 3950: Loss=1.4079, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3960: Loss=1.4121, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 3970: Loss=1.3990, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 3980: Loss=1.4105, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 3990: Loss=1.4023, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4000: Loss=1.4002, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4010: Loss=1.4095, Acc=75.91%, Induction Acc=99.79%\n",
      "Epoch 4020: Loss=1.3911, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4030: Loss=1.4009, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4040: Loss=1.3987, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 4050: Loss=1.4017, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4060: Loss=1.3953, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4070: Loss=1.4020, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4080: Loss=1.4038, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4090: Loss=1.4028, Acc=76.15%, Induction Acc=100.00%\n",
      "Epoch 4100: Loss=1.4022, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 4110: Loss=1.3938, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4120: Loss=1.3874, Acc=76.15%, Induction Acc=100.00%\n",
      "Epoch 4130: Loss=1.3969, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4140: Loss=1.4023, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 4150: Loss=1.4003, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4160: Loss=1.4054, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4170: Loss=1.4063, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 4180: Loss=1.3967, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4190: Loss=1.3877, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4200: Loss=1.3845, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 4210: Loss=1.3932, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4220: Loss=1.3999, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 4230: Loss=1.4026, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 4240: Loss=1.4011, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4250: Loss=1.4026, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 4260: Loss=1.3984, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4270: Loss=1.3946, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4280: Loss=1.3964, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 4290: Loss=1.3996, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4300: Loss=1.4158, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4310: Loss=1.3847, Acc=76.19%, Induction Acc=100.00%\n",
      "Epoch 4320: Loss=1.3909, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4330: Loss=1.3844, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4340: Loss=1.3914, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4350: Loss=1.3930, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4360: Loss=1.3829, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 4370: Loss=1.4056, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4380: Loss=1.3977, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4390: Loss=1.3938, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4400: Loss=1.4026, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4410: Loss=1.3927, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 4420: Loss=1.4026, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 4430: Loss=1.3970, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4440: Loss=1.4019, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 4450: Loss=1.3893, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4460: Loss=1.3906, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4470: Loss=1.3982, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 4480: Loss=1.3952, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4490: Loss=1.3968, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4500: Loss=1.3863, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4510: Loss=1.3863, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 4520: Loss=1.3896, Acc=76.11%, Induction Acc=99.95%\n",
      "Epoch 4530: Loss=1.3930, Acc=75.87%, Induction Acc=99.84%\n",
      "Epoch 4540: Loss=1.3897, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4550: Loss=1.3932, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4560: Loss=1.3773, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4570: Loss=1.3940, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4580: Loss=1.3857, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4590: Loss=1.3865, Acc=76.07%, Induction Acc=99.95%\n",
      "Epoch 4600: Loss=1.3822, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4610: Loss=1.3960, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4620: Loss=1.3928, Acc=76.07%, Induction Acc=99.95%\n",
      "Epoch 4630: Loss=1.3943, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4640: Loss=1.3911, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 4650: Loss=1.3892, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 4660: Loss=1.3984, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 4670: Loss=1.3883, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 4680: Loss=1.3902, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4690: Loss=1.3886, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4700: Loss=1.4047, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4710: Loss=1.3828, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4720: Loss=1.3875, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4730: Loss=1.3789, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4740: Loss=1.3846, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4750: Loss=1.3828, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4760: Loss=1.3895, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4770: Loss=1.3766, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4780: Loss=1.3883, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4790: Loss=1.3852, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 4800: Loss=1.4008, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 4810: Loss=1.3843, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4820: Loss=1.3915, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 4830: Loss=1.3737, Acc=76.19%, Induction Acc=100.00%\n",
      "Epoch 4840: Loss=1.3898, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4850: Loss=1.3864, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4860: Loss=1.3866, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4870: Loss=1.3753, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4880: Loss=1.3855, Acc=76.07%, Induction Acc=99.95%\n",
      "Epoch 4890: Loss=1.3863, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4900: Loss=1.3839, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4910: Loss=1.3859, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 4920: Loss=1.3777, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4930: Loss=1.3941, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4940: Loss=1.3818, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4950: Loss=1.3890, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4960: Loss=1.3797, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4970: Loss=1.3789, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4980: Loss=1.3842, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4990: Loss=1.3874, Acc=76.11%, Induction Acc=100.00%\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,           # Relatively high for small models\n",
    "    betas=(0.9, 0.98), # Standard 'Transformer' betas\n",
    "    weight_decay=0.01  # Helps prevent the 'junk drawer' neuron problem\n",
    ")\n",
    "\n",
    "epochs = 5000\n",
    "losses = []\n",
    "accuracies = []\n",
    "induction_accuracies = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Generate data using a fixed prefix length\n",
    "    seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=SEQ_LEN)\n",
    "\n",
    "    # Move data to the specified device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    input_data = data[:, :-1]\n",
    "    target_data = data[:, 1:]\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model.forward(input_data)\n",
    "\n",
    "    # [b, s, vocab_size] -> [b*s, vocab_size]\n",
    "    input_to_ce = logits.view(-1, model.vocab_size)\n",
    "\n",
    "    # [b, vocab_size] -> [b*vocab_size], equivalent to flattening a 2D tensor into a 1D tensor\n",
    "    target_flat = target_data.reshape(-1)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(input_to_ce, target_flat)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    accuracy = (predictions == target_data).float().mean().item()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Induction accuracy (second half only where the pattern repeats)\n",
    "    induction_acc = (predictions[:, seq_len:] == target_data[:, seq_len:]).float().mean().item()\n",
    "    induction_accuracies.append(induction_acc)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Print every N epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={loss.item():.4f}, Acc={accuracy:.2%}, Induction Acc={induction_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44656cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = results_dir / \"2L_fixed_model.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
