{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bfcbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "from model import AttentionOnlyTransformer\n",
    "from generate_data import generate_rrt\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(67)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "SEQ_LEN = (20, 30)\n",
    "MAX_CONTEXT_LEN = 80\n",
    "OOD_SEQ_LENS = (10, 15, 35, 40)  # OOD sequence lengths for generalization eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e12a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: torch.Size([32, 80]), Prefix Length: 5\n",
      "First sequence sample (first 10 tokens):\n",
      "tensor([120, 167, 140,  36, 246, 120, 167, 140,  36, 246])\n"
     ]
    }
   ],
   "source": [
    "seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=5)\n",
    "\n",
    "print(f\"Dataset Shape: {data.shape}, Prefix Length: {seq_len}\")\n",
    "print(f\"First sequence sample (first 10 tokens):\\n{data[0, :10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fea9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 65,536\n",
      "\n",
      "Model architecture:\n",
      "AttentionOnlyTransformer(\n",
      "  (token_embedding): Embedding(256, 64)\n",
      "  (attention_blocks): ModuleList(\n",
      "    (0-1): 2 x AttentionBlock(\n",
      "      (W_qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "      (W_o): Linear(in_features=64, out_features=64, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (unembed): Linear(in_features=64, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = AttentionOnlyTransformer(\n",
    "    vocab_size=256,\n",
    "    d_model=64,\n",
    "    n_layers=NUM_LAYERS,\n",
    "    n_heads=4,\n",
    "    max_context_len=MAX_CONTEXT_LEN,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nModel architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021916e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=5.8080, Acc=0.67%, Induction=0.60%, OOD_10=0.27%, OOD_15=0.15%, OOD_35=0.57%, OOD_40=0.16%\n",
      "Epoch 100: Loss=5.6703, Acc=0.24%, Induction=0.29%, OOD_10=0.41%, OOD_15=0.59%, OOD_35=0.36%, OOD_40=0.32%\n",
      "Epoch 200: Loss=5.5801, Acc=0.75%, Induction=0.77%, OOD_10=0.91%, OOD_15=0.29%, OOD_35=0.43%, OOD_40=0.48%\n",
      "Epoch 300: Loss=5.5701, Acc=2.10%, Induction=2.78%, OOD_10=4.30%, OOD_15=3.08%, OOD_35=2.06%, OOD_40=1.28%\n",
      "Epoch 400: Loss=5.4718, Acc=2.85%, Induction=3.92%, OOD_10=7.16%, OOD_15=5.47%, OOD_35=3.12%, OOD_40=2.16%\n",
      "Epoch 500: Loss=5.4473, Acc=3.28%, Induction=4.87%, OOD_10=8.11%, OOD_15=6.59%, OOD_35=2.91%, OOD_40=2.64%\n",
      "Epoch 600: Loss=5.4178, Acc=2.93%, Induction=4.27%, OOD_10=9.19%, OOD_15=6.10%, OOD_35=3.48%, OOD_40=2.64%\n",
      "Epoch 700: Loss=5.3590, Acc=4.11%, Induction=5.50%, OOD_10=8.06%, OOD_15=6.69%, OOD_35=4.40%, OOD_40=3.37%\n",
      "Epoch 800: Loss=5.2196, Acc=4.67%, Induction=5.83%, OOD_10=10.24%, OOD_15=7.71%, OOD_35=3.05%, OOD_40=3.04%\n",
      "Epoch 900: Loss=5.2197, Acc=4.91%, Induction=6.41%, OOD_10=10.37%, OOD_15=6.01%, OOD_35=3.12%, OOD_40=2.32%\n",
      "Epoch 1000: Loss=5.2356, Acc=4.55%, Induction=6.67%, OOD_10=10.73%, OOD_15=6.69%, OOD_35=3.05%, OOD_40=2.40%\n",
      "Epoch 1100: Loss=5.1815, Acc=4.98%, Induction=6.47%, OOD_10=10.05%, OOD_15=6.40%, OOD_35=2.20%, OOD_40=2.00%\n",
      "Epoch 1200: Loss=5.2058, Acc=4.91%, Induction=7.15%, OOD_10=8.74%, OOD_15=7.52%, OOD_35=2.56%, OOD_40=2.88%\n",
      "Epoch 1300: Loss=5.1045, Acc=5.62%, Induction=7.53%, OOD_10=11.37%, OOD_15=7.28%, OOD_35=2.06%, OOD_40=2.00%\n",
      "Epoch 1400: Loss=5.0747, Acc=5.97%, Induction=7.81%, OOD_10=10.82%, OOD_15=6.25%, OOD_35=2.63%, OOD_40=1.84%\n",
      "Epoch 1500: Loss=5.1337, Acc=5.30%, Induction=8.09%, OOD_10=12.77%, OOD_15=8.50%, OOD_35=2.91%, OOD_40=3.37%\n",
      "Epoch 1600: Loss=4.9364, Acc=8.39%, Induction=11.59%, OOD_10=15.13%, OOD_15=10.40%, OOD_35=3.98%, OOD_40=2.64%\n",
      "Epoch 1700: Loss=4.9688, Acc=8.47%, Induction=12.62%, OOD_10=16.94%, OOD_15=12.11%, OOD_35=6.46%, OOD_40=4.33%\n",
      "Epoch 1800: Loss=4.7738, Acc=12.42%, Induction=16.70%, OOD_10=21.51%, OOD_15=16.75%, OOD_35=7.17%, OOD_40=7.69%\n",
      "Epoch 1900: Loss=4.5990, Acc=16.85%, Induction=25.24%, OOD_10=29.39%, OOD_15=23.54%, OOD_35=11.93%, OOD_40=9.29%\n",
      "Epoch 2000: Loss=4.3882, Acc=23.93%, Induction=36.83%, OOD_10=38.36%, OOD_15=36.28%, OOD_35=18.61%, OOD_40=16.27%\n",
      "Epoch 2100: Loss=3.5409, Acc=39.79%, Induction=53.93%, OOD_10=60.46%, OOD_15=52.59%, OOD_35=30.54%, OOD_40=26.28%\n",
      "Epoch 2200: Loss=3.1082, Acc=51.70%, Induction=71.49%, OOD_10=71.51%, OOD_15=69.48%, OOD_35=42.68%, OOD_40=36.46%\n",
      "Epoch 2300: Loss=2.6419, Acc=59.57%, Induction=80.93%, OOD_10=80.89%, OOD_15=76.03%, OOD_35=52.27%, OOD_40=44.63%\n",
      "Epoch 2400: Loss=3.1375, Acc=51.27%, Induction=79.23%, OOD_10=84.87%, OOD_15=82.08%, OOD_35=59.30%, OOD_40=47.36%\n",
      "Epoch 2500: Loss=2.6800, Acc=58.39%, Induction=85.24%, OOD_10=88.81%, OOD_15=83.01%, OOD_35=60.01%, OOD_40=51.20%\n",
      "Epoch 2600: Loss=2.2499, Acc=65.39%, Induction=87.29%, OOD_10=88.50%, OOD_15=86.43%, OOD_35=63.42%, OOD_40=53.04%\n",
      "Epoch 2700: Loss=2.3461, Acc=63.77%, Induction=89.62%, OOD_10=88.36%, OOD_15=84.96%, OOD_35=65.62%, OOD_40=55.29%\n",
      "Epoch 2800: Loss=2.1305, Acc=66.26%, Induction=89.87%, OOD_10=84.83%, OOD_15=86.33%, OOD_35=66.83%, OOD_40=56.81%\n",
      "Epoch 2900: Loss=2.5445, Acc=60.60%, Induction=91.77%, OOD_10=89.04%, OOD_15=87.60%, OOD_35=67.54%, OOD_40=56.01%\n",
      "Epoch 3000: Loss=2.5912, Acc=58.82%, Induction=90.99%, OOD_10=87.27%, OOD_15=85.64%, OOD_35=69.60%, OOD_40=57.93%\n",
      "Epoch 3100: Loss=2.1252, Acc=66.38%, Induction=91.61%, OOD_10=89.81%, OOD_15=87.40%, OOD_35=67.68%, OOD_40=57.53%\n",
      "Epoch 3200: Loss=2.2826, Acc=63.57%, Induction=92.65%, OOD_10=88.50%, OOD_15=88.04%, OOD_35=69.89%, OOD_40=59.78%\n",
      "Epoch 3300: Loss=2.3928, Acc=61.99%, Induction=93.81%, OOD_10=87.05%, OOD_15=88.92%, OOD_35=70.67%, OOD_40=59.13%\n",
      "Epoch 3400: Loss=2.7126, Acc=56.88%, Induction=89.56%, OOD_10=87.23%, OOD_15=88.09%, OOD_35=70.74%, OOD_40=60.58%\n",
      "Epoch 3500: Loss=2.1217, Acc=65.86%, Induction=94.32%, OOD_10=89.27%, OOD_15=89.31%, OOD_35=71.73%, OOD_40=60.98%\n",
      "Epoch 3600: Loss=1.9028, Acc=69.30%, Induction=94.23%, OOD_10=90.35%, OOD_15=89.84%, OOD_35=71.52%, OOD_40=59.94%\n",
      "Epoch 3700: Loss=2.1982, Acc=64.95%, Induction=94.91%, OOD_10=86.82%, OOD_15=87.11%, OOD_35=73.30%, OOD_40=59.46%\n",
      "Epoch 3800: Loss=2.4475, Acc=61.00%, Induction=94.24%, OOD_10=88.95%, OOD_15=89.94%, OOD_35=72.02%, OOD_40=59.86%\n",
      "Epoch 3900: Loss=1.9944, Acc=67.37%, Induction=94.64%, OOD_10=90.90%, OOD_15=91.60%, OOD_35=73.22%, OOD_40=62.90%\n",
      "Epoch 4000: Loss=1.9525, Acc=68.35%, Induction=94.52%, OOD_10=88.09%, OOD_15=88.77%, OOD_35=71.59%, OOD_40=61.54%\n",
      "Epoch 4100: Loss=2.2616, Acc=62.97%, Induction=93.57%, OOD_10=88.90%, OOD_15=88.92%, OOD_35=73.44%, OOD_40=61.46%\n",
      "Epoch 4200: Loss=1.9183, Acc=68.43%, Induction=94.57%, OOD_10=88.86%, OOD_15=89.94%, OOD_35=70.95%, OOD_40=60.58%\n",
      "Epoch 4300: Loss=2.0451, Acc=66.69%, Induction=95.40%, OOD_10=89.36%, OOD_15=89.99%, OOD_35=73.72%, OOD_40=60.98%\n",
      "Epoch 4400: Loss=2.3717, Acc=61.23%, Induction=94.61%, OOD_10=89.36%, OOD_15=91.75%, OOD_35=73.72%, OOD_40=62.34%\n",
      "Epoch 4500: Loss=1.8402, Acc=69.62%, Induction=94.61%, OOD_10=91.17%, OOD_15=89.40%, OOD_35=70.60%, OOD_40=61.86%\n",
      "Epoch 4600: Loss=2.1258, Acc=64.95%, Induction=94.68%, OOD_10=89.40%, OOD_15=92.72%, OOD_35=74.22%, OOD_40=61.54%\n",
      "Epoch 4700: Loss=2.5119, Acc=58.66%, Induction=92.31%, OOD_10=89.36%, OOD_15=90.58%, OOD_35=73.93%, OOD_40=62.34%\n",
      "Epoch 4800: Loss=2.3424, Acc=61.59%, Induction=95.28%, OOD_10=87.95%, OOD_15=89.01%, OOD_35=74.50%, OOD_40=63.06%\n",
      "Epoch 4900: Loss=2.2259, Acc=62.78%, Induction=95.01%, OOD_10=89.58%, OOD_15=90.23%, OOD_35=73.08%, OOD_40=60.74%\n",
      "Epoch 5000: Loss=2.1574, Acc=64.16%, Induction=95.46%, OOD_10=90.62%, OOD_15=88.57%, OOD_35=74.29%, OOD_40=63.46%\n",
      "Epoch 5100: Loss=2.1365, Acc=64.60%, Induction=96.05%, OOD_10=90.08%, OOD_15=89.55%, OOD_35=75.43%, OOD_40=61.54%\n",
      "Epoch 5200: Loss=2.4832, Acc=59.77%, Induction=94.06%, OOD_10=90.67%, OOD_15=90.14%, OOD_35=72.80%, OOD_40=63.30%\n",
      "Epoch 5300: Loss=2.4520, Acc=59.06%, Induction=93.00%, OOD_10=88.32%, OOD_15=89.21%, OOD_35=74.72%, OOD_40=62.66%\n",
      "Epoch 5400: Loss=2.2189, Acc=63.25%, Induction=95.91%, OOD_10=90.94%, OOD_15=89.31%, OOD_35=74.79%, OOD_40=61.54%\n",
      "Epoch 5500: Loss=2.0398, Acc=65.98%, Induction=96.30%, OOD_10=89.58%, OOD_15=91.16%, OOD_35=72.66%, OOD_40=61.70%\n",
      "Epoch 5600: Loss=2.3413, Acc=61.31%, Induction=94.61%, OOD_10=90.76%, OOD_15=89.99%, OOD_35=74.01%, OOD_40=62.74%\n",
      "Epoch 5700: Loss=2.0066, Acc=66.34%, Induction=96.76%, OOD_10=89.86%, OOD_15=89.84%, OOD_35=72.94%, OOD_40=59.78%\n",
      "Epoch 5800: Loss=2.0571, Acc=65.86%, Induction=95.95%, OOD_10=92.12%, OOD_15=92.09%, OOD_35=74.72%, OOD_40=62.58%\n",
      "Epoch 5900: Loss=1.7848, Acc=70.65%, Induction=97.37%, OOD_10=89.76%, OOD_15=90.38%, OOD_35=75.14%, OOD_40=62.74%\n",
      "Epoch 6000: Loss=1.7462, Acc=70.93%, Induction=94.97%, OOD_10=89.40%, OOD_15=87.60%, OOD_35=74.86%, OOD_40=62.42%\n",
      "Epoch 6100: Loss=1.7648, Acc=70.57%, Induction=95.80%, OOD_10=92.21%, OOD_15=90.04%, OOD_35=75.43%, OOD_40=64.98%\n",
      "Epoch 6200: Loss=2.0078, Acc=66.06%, Induction=96.41%, OOD_10=90.94%, OOD_15=91.80%, OOD_35=74.08%, OOD_40=62.58%\n",
      "Epoch 6300: Loss=1.9241, Acc=67.33%, Induction=96.31%, OOD_10=92.80%, OOD_15=93.75%, OOD_35=75.78%, OOD_40=64.10%\n",
      "Epoch 6400: Loss=1.8501, Acc=68.67%, Induction=96.15%, OOD_10=89.72%, OOD_15=89.60%, OOD_35=74.86%, OOD_40=62.58%\n",
      "Epoch 6500: Loss=2.0398, Acc=65.86%, Induction=95.83%, OOD_10=88.77%, OOD_15=87.94%, OOD_35=77.63%, OOD_40=62.82%\n",
      "Epoch 6600: Loss=2.0021, Acc=66.30%, Induction=96.53%, OOD_10=91.08%, OOD_15=91.50%, OOD_35=76.56%, OOD_40=65.30%\n",
      "Epoch 6700: Loss=2.2171, Acc=62.58%, Induction=96.75%, OOD_10=89.63%, OOD_15=89.55%, OOD_35=76.35%, OOD_40=61.94%\n",
      "Epoch 6800: Loss=2.0848, Acc=65.35%, Induction=97.11%, OOD_10=89.90%, OOD_15=91.06%, OOD_35=75.07%, OOD_40=65.38%\n",
      "Epoch 6900: Loss=1.7888, Acc=70.13%, Induction=96.71%, OOD_10=92.62%, OOD_15=90.04%, OOD_35=74.72%, OOD_40=61.22%\n",
      "Epoch 7000: Loss=2.0694, Acc=64.72%, Induction=96.17%, OOD_10=92.12%, OOD_15=89.26%, OOD_35=74.57%, OOD_40=62.58%\n",
      "Epoch 7100: Loss=1.7366, Acc=71.08%, Induction=96.28%, OOD_10=90.40%, OOD_15=91.02%, OOD_35=76.07%, OOD_40=64.66%\n",
      "Epoch 7200: Loss=1.8179, Acc=69.38%, Induction=95.56%, OOD_10=90.90%, OOD_15=90.43%, OOD_35=77.70%, OOD_40=64.98%\n",
      "Epoch 7300: Loss=2.2492, Acc=62.38%, Induction=96.38%, OOD_10=90.53%, OOD_15=91.60%, OOD_35=77.34%, OOD_40=64.26%\n",
      "Epoch 7400: Loss=1.9227, Acc=67.05%, Induction=95.85%, OOD_10=91.71%, OOD_15=90.19%, OOD_35=77.49%, OOD_40=64.50%\n",
      "Epoch 7500: Loss=2.0601, Acc=65.66%, Induction=97.23%, OOD_10=89.76%, OOD_15=91.85%, OOD_35=77.20%, OOD_40=65.62%\n",
      "Epoch 7600: Loss=1.8239, Acc=68.75%, Induction=96.43%, OOD_10=93.66%, OOD_15=92.97%, OOD_35=76.42%, OOD_40=64.66%\n",
      "Epoch 7700: Loss=2.2301, Acc=62.74%, Induction=96.94%, OOD_10=92.80%, OOD_15=92.68%, OOD_35=76.99%, OOD_40=65.46%\n",
      "Epoch 7800: Loss=1.8258, Acc=68.79%, Induction=96.65%, OOD_10=92.35%, OOD_15=91.26%, OOD_35=78.91%, OOD_40=63.94%\n",
      "Epoch 7900: Loss=2.0325, Acc=65.19%, Induction=96.88%, OOD_10=90.13%, OOD_15=87.70%, OOD_35=76.78%, OOD_40=68.19%\n",
      "Epoch 8000: Loss=1.9831, Acc=65.90%, Induction=96.30%, OOD_10=91.30%, OOD_15=93.21%, OOD_35=77.56%, OOD_40=64.98%\n",
      "Epoch 8100: Loss=1.8353, Acc=68.91%, Induction=95.07%, OOD_10=92.53%, OOD_15=92.82%, OOD_35=76.07%, OOD_40=62.58%\n",
      "Epoch 8200: Loss=2.4122, Acc=59.41%, Induction=93.62%, OOD_10=92.39%, OOD_15=92.97%, OOD_35=78.27%, OOD_40=65.62%\n",
      "Epoch 8300: Loss=1.7159, Acc=70.93%, Induction=96.01%, OOD_10=92.57%, OOD_15=91.31%, OOD_35=77.98%, OOD_40=66.03%\n",
      "Epoch 8400: Loss=2.0188, Acc=65.23%, Induction=97.11%, OOD_10=92.03%, OOD_15=90.58%, OOD_35=78.12%, OOD_40=65.06%\n",
      "Epoch 8500: Loss=1.6869, Acc=71.24%, Induction=95.13%, OOD_10=91.85%, OOD_15=93.31%, OOD_35=78.41%, OOD_40=66.99%\n",
      "Epoch 8600: Loss=1.9445, Acc=66.73%, Induction=97.40%, OOD_10=92.53%, OOD_15=91.21%, OOD_35=77.91%, OOD_40=66.11%\n",
      "Epoch 8700: Loss=2.0923, Acc=63.84%, Induction=96.75%, OOD_10=93.52%, OOD_15=94.43%, OOD_35=79.05%, OOD_40=67.55%\n",
      "Epoch 8800: Loss=2.1131, Acc=63.61%, Induction=96.33%, OOD_10=91.17%, OOD_15=91.70%, OOD_35=82.17%, OOD_40=68.11%\n",
      "Epoch 8900: Loss=2.3683, Acc=60.17%, Induction=94.94%, OOD_10=92.93%, OOD_15=92.68%, OOD_35=79.05%, OOD_40=66.43%\n",
      "Epoch 9000: Loss=1.9370, Acc=66.81%, Induction=97.34%, OOD_10=89.95%, OOD_15=89.11%, OOD_35=79.55%, OOD_40=66.19%\n",
      "Epoch 9100: Loss=1.7523, Acc=70.45%, Induction=94.28%, OOD_10=92.16%, OOD_15=92.48%, OOD_35=80.04%, OOD_40=66.75%\n",
      "Epoch 9200: Loss=2.0063, Acc=65.78%, Induction=97.94%, OOD_10=94.20%, OOD_15=94.14%, OOD_35=77.41%, OOD_40=67.87%\n",
      "Epoch 9300: Loss=1.7262, Acc=70.09%, Induction=96.71%, OOD_10=93.39%, OOD_15=92.38%, OOD_35=79.97%, OOD_40=66.11%\n",
      "Epoch 9400: Loss=2.2140, Acc=62.62%, Induction=96.75%, OOD_10=91.62%, OOD_15=93.70%, OOD_35=80.47%, OOD_40=67.63%\n",
      "Epoch 9500: Loss=2.1878, Acc=62.86%, Induction=97.00%, OOD_10=91.44%, OOD_15=90.72%, OOD_35=79.33%, OOD_40=69.15%\n",
      "Epoch 9600: Loss=1.7479, Acc=70.02%, Induction=98.16%, OOD_10=93.34%, OOD_15=92.63%, OOD_35=79.47%, OOD_40=66.19%\n",
      "Epoch 9700: Loss=2.3531, Acc=60.40%, Induction=95.25%, OOD_10=91.98%, OOD_15=90.77%, OOD_35=77.70%, OOD_40=65.14%\n",
      "Epoch 9800: Loss=1.7226, Acc=70.57%, Induction=97.48%, OOD_10=92.93%, OOD_15=93.02%, OOD_35=80.97%, OOD_40=69.63%\n",
      "Epoch 9900: Loss=1.7930, Acc=69.34%, Induction=97.32%, OOD_10=93.39%, OOD_15=91.85%, OOD_35=80.82%, OOD_40=66.51%\n",
      "Epoch 10000: Loss=2.2052, Acc=63.05%, Induction=97.55%, OOD_10=94.07%, OOD_15=93.26%, OOD_35=79.83%, OOD_40=68.83%\n",
      "Epoch 10100: Loss=1.6866, Acc=71.24%, Induction=95.02%, OOD_10=92.30%, OOD_15=90.97%, OOD_35=81.39%, OOD_40=68.51%\n",
      "Epoch 10200: Loss=2.1609, Acc=63.17%, Induction=97.24%, OOD_10=91.89%, OOD_15=91.36%, OOD_35=82.24%, OOD_40=68.75%\n",
      "Epoch 10300: Loss=1.8337, Acc=68.35%, Induction=97.78%, OOD_10=93.39%, OOD_15=91.99%, OOD_35=79.55%, OOD_40=69.95%\n",
      "Epoch 10400: Loss=1.6930, Acc=71.32%, Induction=95.13%, OOD_10=92.26%, OOD_15=91.50%, OOD_35=80.47%, OOD_40=67.23%\n",
      "Epoch 10500: Loss=1.6945, Acc=70.77%, Induction=96.17%, OOD_10=94.70%, OOD_15=92.87%, OOD_35=81.53%, OOD_40=68.51%\n",
      "Epoch 10600: Loss=1.7054, Acc=70.57%, Induction=97.15%, OOD_10=92.80%, OOD_15=92.48%, OOD_35=80.47%, OOD_40=68.59%\n",
      "Epoch 10700: Loss=1.9098, Acc=66.93%, Induction=97.51%, OOD_10=94.34%, OOD_15=93.12%, OOD_35=80.89%, OOD_40=69.55%\n",
      "Epoch 10800: Loss=2.1790, Acc=62.54%, Induction=96.63%, OOD_10=94.66%, OOD_15=92.53%, OOD_35=81.96%, OOD_40=69.79%\n",
      "Epoch 10900: Loss=2.2006, Acc=61.95%, Induction=95.71%, OOD_10=91.30%, OOD_15=92.24%, OOD_35=80.26%, OOD_40=68.03%\n",
      "Epoch 11000: Loss=1.7113, Acc=70.57%, Induction=95.42%, OOD_10=92.75%, OOD_15=93.60%, OOD_35=79.97%, OOD_40=68.51%\n",
      "Epoch 11100: Loss=2.2586, Acc=60.88%, Induction=96.06%, OOD_10=92.84%, OOD_15=91.36%, OOD_35=80.61%, OOD_40=70.51%\n",
      "Epoch 11200: Loss=1.6990, Acc=70.65%, Induction=97.48%, OOD_10=92.16%, OOD_15=92.33%, OOD_35=78.98%, OOD_40=69.15%\n",
      "Epoch 11300: Loss=1.7233, Acc=70.29%, Induction=93.70%, OOD_10=94.70%, OOD_15=91.89%, OOD_35=81.75%, OOD_40=68.75%\n",
      "Epoch 11400: Loss=2.2527, Acc=61.31%, Induction=96.56%, OOD_10=93.98%, OOD_15=94.14%, OOD_35=81.39%, OOD_40=71.07%\n",
      "Epoch 11500: Loss=1.6419, Acc=71.60%, Induction=97.09%, OOD_10=94.38%, OOD_15=92.87%, OOD_35=82.60%, OOD_40=70.67%\n",
      "Epoch 11600: Loss=2.0571, Acc=64.28%, Induction=97.42%, OOD_10=95.61%, OOD_15=96.19%, OOD_35=83.74%, OOD_40=69.95%\n",
      "Epoch 11700: Loss=1.9062, Acc=66.34%, Induction=96.64%, OOD_10=95.52%, OOD_15=94.73%, OOD_35=81.53%, OOD_40=69.31%\n",
      "Epoch 11800: Loss=1.9857, Acc=65.82%, Induction=97.64%, OOD_10=95.43%, OOD_15=92.77%, OOD_35=79.62%, OOD_40=67.55%\n",
      "Epoch 11900: Loss=2.2596, Acc=61.47%, Induction=96.81%, OOD_10=92.93%, OOD_15=93.51%, OOD_35=83.38%, OOD_40=70.67%\n",
      "Epoch 12000: Loss=1.8467, Acc=67.88%, Induction=97.33%, OOD_10=94.79%, OOD_15=93.16%, OOD_35=83.81%, OOD_40=72.44%\n",
      "Epoch 12100: Loss=1.6622, Acc=71.04%, Induction=96.39%, OOD_10=94.79%, OOD_15=94.53%, OOD_35=82.60%, OOD_40=69.39%\n",
      "Epoch 12200: Loss=1.7820, Acc=69.22%, Induction=97.27%, OOD_10=95.92%, OOD_15=93.60%, OOD_35=84.52%, OOD_40=70.27%\n",
      "Epoch 12300: Loss=1.8834, Acc=67.37%, Induction=98.09%, OOD_10=93.30%, OOD_15=93.12%, OOD_35=83.81%, OOD_40=71.23%\n",
      "Epoch 12400: Loss=2.1499, Acc=63.09%, Induction=97.37%, OOD_10=94.43%, OOD_15=93.36%, OOD_35=83.17%, OOD_40=69.63%\n",
      "Epoch 12500: Loss=1.6717, Acc=71.00%, Induction=98.03%, OOD_10=95.15%, OOD_15=94.68%, OOD_35=83.24%, OOD_40=72.44%\n",
      "Epoch 12600: Loss=1.9547, Acc=66.06%, Induction=98.29%, OOD_10=93.75%, OOD_15=92.14%, OOD_35=81.75%, OOD_40=69.71%\n",
      "Epoch 12700: Loss=2.0342, Acc=64.91%, Induction=98.14%, OOD_10=91.67%, OOD_15=93.85%, OOD_35=84.87%, OOD_40=72.60%\n",
      "Epoch 12800: Loss=1.6291, Acc=71.72%, Induction=95.66%, OOD_10=94.97%, OOD_15=94.48%, OOD_35=81.96%, OOD_40=69.87%\n",
      "Epoch 12900: Loss=1.8214, Acc=68.71%, Induction=98.24%, OOD_10=92.75%, OOD_15=94.53%, OOD_35=82.53%, OOD_40=71.47%\n",
      "Epoch 13000: Loss=2.1440, Acc=62.86%, Induction=96.88%, OOD_10=91.94%, OOD_15=93.31%, OOD_35=82.60%, OOD_40=71.23%\n",
      "Epoch 13100: Loss=2.0263, Acc=64.87%, Induction=97.78%, OOD_10=93.98%, OOD_15=93.12%, OOD_35=85.01%, OOD_40=70.59%\n",
      "Epoch 13200: Loss=1.8333, Acc=68.12%, Induction=97.44%, OOD_10=93.07%, OOD_15=92.82%, OOD_35=83.95%, OOD_40=71.71%\n",
      "Epoch 13300: Loss=1.6811, Acc=70.81%, Induction=97.64%, OOD_10=94.61%, OOD_15=95.70%, OOD_35=82.39%, OOD_40=71.39%\n",
      "Epoch 13400: Loss=1.6188, Acc=72.23%, Induction=96.56%, OOD_10=94.57%, OOD_15=93.02%, OOD_35=85.23%, OOD_40=70.67%\n",
      "Epoch 13500: Loss=1.8977, Acc=67.37%, Induction=98.03%, OOD_10=92.75%, OOD_15=92.24%, OOD_35=84.16%, OOD_40=72.92%\n",
      "Epoch 13600: Loss=2.0616, Acc=64.12%, Induction=97.00%, OOD_10=95.65%, OOD_15=93.90%, OOD_35=84.23%, OOD_40=72.12%\n",
      "Epoch 13700: Loss=1.6141, Acc=72.43%, Induction=96.72%, OOD_10=94.61%, OOD_15=93.70%, OOD_35=83.52%, OOD_40=72.84%\n",
      "Epoch 13800: Loss=1.5964, Acc=71.80%, Induction=96.08%, OOD_10=97.15%, OOD_15=94.73%, OOD_35=85.30%, OOD_40=73.16%\n",
      "Epoch 13900: Loss=2.2942, Acc=60.48%, Induction=95.00%, OOD_10=94.97%, OOD_15=94.48%, OOD_35=83.59%, OOD_40=74.28%\n",
      "Epoch 14000: Loss=1.7482, Acc=69.03%, Induction=96.43%, OOD_10=94.07%, OOD_15=93.21%, OOD_35=83.66%, OOD_40=72.84%\n",
      "Epoch 14100: Loss=2.2327, Acc=61.04%, Induction=96.06%, OOD_10=94.52%, OOD_15=91.89%, OOD_35=83.95%, OOD_40=72.28%\n",
      "Epoch 14200: Loss=1.8306, Acc=68.12%, Induction=97.27%, OOD_10=92.26%, OOD_15=92.72%, OOD_35=85.72%, OOD_40=72.20%\n",
      "Epoch 14300: Loss=1.8052, Acc=68.47%, Induction=97.90%, OOD_10=95.65%, OOD_15=94.53%, OOD_35=84.59%, OOD_40=72.84%\n",
      "Epoch 14400: Loss=1.9429, Acc=66.65%, Induction=98.29%, OOD_10=91.12%, OOD_15=87.84%, OOD_35=84.80%, OOD_40=72.52%\n",
      "Epoch 14500: Loss=1.6830, Acc=70.93%, Induction=97.53%, OOD_10=94.70%, OOD_15=93.90%, OOD_35=84.02%, OOD_40=72.60%\n",
      "Epoch 14600: Loss=1.9703, Acc=65.86%, Induction=97.41%, OOD_10=92.39%, OOD_15=91.36%, OOD_35=83.52%, OOD_40=74.84%\n",
      "Epoch 14700: Loss=2.2357, Acc=61.39%, Induction=96.50%, OOD_10=95.56%, OOD_15=93.31%, OOD_35=82.60%, OOD_40=71.31%\n",
      "Epoch 14800: Loss=1.6156, Acc=71.99%, Induction=96.24%, OOD_10=96.33%, OOD_15=93.36%, OOD_35=85.16%, OOD_40=72.60%\n",
      "Epoch 14900: Loss=1.8320, Acc=68.12%, Induction=97.39%, OOD_10=96.24%, OOD_15=93.07%, OOD_35=85.30%, OOD_40=73.48%\n",
      "Epoch 15000: Loss=1.8811, Acc=67.13%, Induction=97.80%, OOD_10=95.24%, OOD_15=94.09%, OOD_35=86.79%, OOD_40=75.40%\n",
      "Epoch 15100: Loss=2.1127, Acc=63.09%, Induction=97.06%, OOD_10=95.43%, OOD_15=93.21%, OOD_35=85.58%, OOD_40=73.88%\n",
      "Epoch 15200: Loss=2.2433, Acc=61.19%, Induction=96.19%, OOD_10=94.97%, OOD_15=91.70%, OOD_35=86.43%, OOD_40=74.60%\n",
      "Epoch 15300: Loss=1.6695, Acc=70.09%, Induction=96.71%, OOD_10=96.74%, OOD_15=94.97%, OOD_35=83.38%, OOD_40=76.28%\n",
      "Epoch 15400: Loss=2.2608, Acc=60.88%, Induction=95.94%, OOD_10=93.66%, OOD_15=93.65%, OOD_35=88.07%, OOD_40=76.04%\n",
      "Epoch 15500: Loss=1.6760, Acc=70.77%, Induction=95.80%, OOD_10=92.66%, OOD_15=89.40%, OOD_35=85.65%, OOD_40=72.92%\n",
      "Epoch 15600: Loss=2.2168, Acc=61.95%, Induction=97.37%, OOD_10=94.02%, OOD_15=92.68%, OOD_35=85.16%, OOD_40=76.44%\n",
      "Epoch 15700: Loss=1.8743, Acc=67.44%, Induction=97.92%, OOD_10=95.61%, OOD_15=93.60%, OOD_35=88.07%, OOD_40=73.40%\n",
      "Epoch 15800: Loss=2.1507, Acc=62.70%, Induction=96.57%, OOD_10=94.52%, OOD_15=91.85%, OOD_35=86.51%, OOD_40=76.20%\n",
      "Epoch 15900: Loss=1.6171, Acc=72.27%, Induction=97.58%, OOD_10=93.21%, OOD_15=92.43%, OOD_35=88.64%, OOD_40=76.36%\n",
      "Epoch 16000: Loss=2.0047, Acc=65.59%, Induction=98.38%, OOD_10=94.61%, OOD_15=94.43%, OOD_35=87.93%, OOD_40=76.84%\n",
      "Epoch 16100: Loss=2.2235, Acc=61.43%, Induction=96.88%, OOD_10=94.20%, OOD_15=89.89%, OOD_35=88.07%, OOD_40=80.93%\n",
      "Epoch 16200: Loss=1.6108, Acc=71.04%, Induction=94.92%, OOD_10=95.88%, OOD_15=95.70%, OOD_35=86.58%, OOD_40=76.44%\n",
      "Epoch 16300: Loss=1.5781, Acc=72.63%, Induction=96.93%, OOD_10=95.52%, OOD_15=93.02%, OOD_35=87.22%, OOD_40=79.65%\n",
      "Epoch 16400: Loss=2.0056, Acc=65.35%, Induction=98.14%, OOD_10=94.38%, OOD_15=89.99%, OOD_35=88.49%, OOD_40=79.25%\n",
      "Epoch 16500: Loss=2.2310, Acc=61.31%, Induction=96.25%, OOD_10=96.60%, OOD_15=93.80%, OOD_35=89.13%, OOD_40=77.88%\n",
      "Epoch 16600: Loss=1.5987, Acc=71.40%, Induction=96.50%, OOD_10=95.24%, OOD_15=93.55%, OOD_35=87.43%, OOD_40=78.37%\n",
      "Epoch 16700: Loss=1.6959, Acc=69.70%, Induction=95.67%, OOD_10=94.79%, OOD_15=93.02%, OOD_35=88.35%, OOD_40=79.49%\n",
      "Epoch 16800: Loss=2.0130, Acc=65.03%, Induction=97.54%, OOD_10=92.93%, OOD_15=90.09%, OOD_35=88.64%, OOD_40=80.61%\n",
      "Epoch 16900: Loss=2.0905, Acc=63.69%, Induction=97.55%, OOD_10=93.80%, OOD_15=91.06%, OOD_35=88.14%, OOD_40=77.96%\n",
      "Epoch 17000: Loss=2.0276, Acc=64.72%, Induction=97.42%, OOD_10=95.34%, OOD_15=90.28%, OOD_35=87.43%, OOD_40=79.57%\n",
      "Epoch 17100: Loss=1.9478, Acc=66.10%, Induction=97.64%, OOD_10=95.47%, OOD_15=92.97%, OOD_35=86.93%, OOD_40=79.97%\n",
      "Epoch 17200: Loss=2.0643, Acc=63.92%, Induction=97.61%, OOD_10=94.61%, OOD_15=91.75%, OOD_35=90.13%, OOD_40=81.73%\n",
      "Epoch 17300: Loss=1.7953, Acc=68.87%, Induction=98.12%, OOD_10=94.61%, OOD_15=90.33%, OOD_35=91.55%, OOD_40=82.45%\n",
      "Epoch 17400: Loss=2.0195, Acc=64.28%, Induction=96.81%, OOD_10=96.33%, OOD_15=89.31%, OOD_35=90.27%, OOD_40=80.45%\n",
      "Epoch 17500: Loss=1.6314, Acc=71.16%, Induction=97.04%, OOD_10=96.06%, OOD_15=92.24%, OOD_35=90.55%, OOD_40=81.81%\n",
      "Epoch 17600: Loss=1.8489, Acc=67.96%, Induction=98.38%, OOD_10=95.43%, OOD_15=93.90%, OOD_35=88.28%, OOD_40=84.13%\n",
      "Epoch 17700: Loss=1.7856, Acc=68.67%, Induction=97.56%, OOD_10=97.51%, OOD_15=96.48%, OOD_35=89.56%, OOD_40=83.97%\n",
      "Epoch 17800: Loss=2.2523, Acc=60.76%, Induction=95.06%, OOD_10=95.61%, OOD_15=93.85%, OOD_35=90.98%, OOD_40=82.05%\n",
      "Epoch 17900: Loss=2.0908, Acc=63.92%, Induction=97.86%, OOD_10=96.56%, OOD_15=94.14%, OOD_35=90.27%, OOD_40=83.17%\n",
      "Epoch 18000: Loss=2.1825, Acc=61.99%, Induction=97.06%, OOD_10=93.39%, OOD_15=87.35%, OOD_35=89.63%, OOD_40=82.61%\n",
      "Epoch 18100: Loss=2.2040, Acc=61.71%, Induction=96.81%, OOD_10=94.20%, OOD_15=90.92%, OOD_35=90.20%, OOD_40=82.69%\n",
      "Epoch 18200: Loss=2.0995, Acc=63.29%, Induction=97.00%, OOD_10=97.42%, OOD_15=94.29%, OOD_35=89.70%, OOD_40=83.09%\n",
      "Epoch 18300: Loss=1.5494, Acc=72.78%, Induction=96.66%, OOD_10=96.47%, OOD_15=95.46%, OOD_35=91.26%, OOD_40=81.97%\n",
      "Epoch 18400: Loss=1.7652, Acc=69.42%, Induction=98.12%, OOD_10=93.84%, OOD_15=89.94%, OOD_35=89.06%, OOD_40=83.25%\n",
      "Epoch 18500: Loss=2.2205, Acc=61.19%, Induction=96.19%, OOD_10=96.29%, OOD_15=93.99%, OOD_35=89.06%, OOD_40=84.13%\n",
      "Epoch 18600: Loss=1.7020, Acc=70.45%, Induction=97.60%, OOD_10=95.15%, OOD_15=90.87%, OOD_35=88.14%, OOD_40=84.29%\n",
      "Epoch 18700: Loss=2.0643, Acc=63.92%, Induction=98.04%, OOD_10=96.20%, OOD_15=94.09%, OOD_35=90.98%, OOD_40=82.85%\n",
      "Epoch 18800: Loss=1.9173, Acc=66.81%, Induction=98.17%, OOD_10=94.88%, OOD_15=92.29%, OOD_35=89.49%, OOD_40=81.57%\n",
      "Epoch 18900: Loss=2.0993, Acc=63.61%, Induction=97.55%, OOD_10=96.51%, OOD_15=91.50%, OOD_35=88.92%, OOD_40=83.65%\n",
      "Epoch 19000: Loss=1.6951, Acc=70.53%, Induction=97.88%, OOD_10=95.70%, OOD_15=89.89%, OOD_35=89.91%, OOD_40=83.65%\n",
      "Epoch 19100: Loss=1.6251, Acc=71.72%, Induction=98.14%, OOD_10=96.51%, OOD_15=90.53%, OOD_35=88.21%, OOD_40=82.05%\n",
      "Epoch 19200: Loss=1.7754, Acc=69.11%, Induction=98.12%, OOD_10=95.88%, OOD_15=93.55%, OOD_35=88.21%, OOD_40=83.65%\n",
      "Epoch 19300: Loss=1.7111, Acc=70.02%, Induction=97.21%, OOD_10=93.48%, OOD_15=90.09%, OOD_35=89.28%, OOD_40=84.70%\n",
      "Epoch 19400: Loss=1.6059, Acc=71.99%, Induction=97.04%, OOD_10=95.20%, OOD_15=90.97%, OOD_35=88.35%, OOD_40=82.29%\n",
      "Epoch 19500: Loss=1.7577, Acc=69.50%, Induction=98.01%, OOD_10=91.67%, OOD_15=89.06%, OOD_35=88.42%, OOD_40=83.17%\n",
      "Epoch 19600: Loss=1.8569, Acc=67.80%, Induction=97.92%, OOD_10=95.24%, OOD_15=88.18%, OOD_35=89.06%, OOD_40=82.53%\n",
      "Epoch 19700: Loss=1.5778, Acc=72.31%, Induction=96.93%, OOD_10=95.38%, OOD_15=88.18%, OOD_35=86.58%, OOD_40=79.09%\n",
      "Epoch 19800: Loss=2.0494, Acc=64.40%, Induction=96.69%, OOD_10=94.02%, OOD_15=90.97%, OOD_35=88.14%, OOD_40=83.25%\n",
      "Epoch 19900: Loss=1.9761, Acc=65.39%, Induction=97.96%, OOD_10=95.38%, OOD_15=88.48%, OOD_35=89.56%, OOD_40=84.29%\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,           # Relatively high for small models\n",
    "    betas=(0.9, 0.98), # Standard 'Transformer' betas\n",
    "    weight_decay=0.01  # Helps prevent the 'junk drawer' neuron problem\n",
    ")\n",
    "\n",
    "epochs = 20000\n",
    "losses = []\n",
    "accuracies = []\n",
    "induction_accuracies = []\n",
    "\n",
    "# Training history for visualization (saved every 100 epochs, includes OOD evals)\n",
    "training_history = {\n",
    "    \"epochs\": [],\n",
    "    \"losses\": [],\n",
    "    \"accuracies\": [],\n",
    "    \"induction_accs\": [],\n",
    "    \"ood_induction_accs\": {seq_len: [] for seq_len in OOD_SEQ_LENS},  # keyed by OOD seq_len\n",
    "    \"attention_snapshots\": {},  # epoch -> attention patterns (only when seq_len == 20)\n",
    "}\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Generate data using a varied prefix length\n",
    "    seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=SEQ_LEN)\n",
    "\n",
    "    # Move data to the specified device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    input_data = data[:, :-1]\n",
    "    target_data = data[:, 1:]\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model.forward(input_data)\n",
    "\n",
    "    # [b, s, vocab_size] -> [b*s, vocab_size]\n",
    "    input_to_ce = logits.view(-1, model.vocab_size)\n",
    "\n",
    "    # [b, vocab_size] -> [b*vocab_size], equivalent to flattening a 2D tensor into a 1D tensor\n",
    "    target_flat = target_data.reshape(-1)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(input_to_ce, target_flat)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    accuracy = (predictions == target_data).float().mean().item()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Induction accuracy (second half only where the pattern repeats)\n",
    "    induction_acc = (predictions[:, seq_len:] == target_data[:, seq_len:]).float().mean().item()\n",
    "    induction_accuracies.append(induction_acc)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Save metrics every 100 epochs (includes OOD seq_len=15 eval)\n",
    "    if epoch % 100 == 0:\n",
    "        training_history[\"epochs\"].append(epoch)\n",
    "        training_history[\"losses\"].append(loss.item())\n",
    "        training_history[\"accuracies\"].append(accuracy)\n",
    "        training_history[\"induction_accs\"].append(induction_acc)\n",
    "\n",
    "        # Evaluate out-of-distribution induction accuracy at multiple seq_lens\n",
    "        ood_accs = {}\n",
    "        with torch.no_grad():\n",
    "            for ood_len in OOD_SEQ_LENS:\n",
    "                _, ood_data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=ood_len)\n",
    "                ood_data = ood_data.to(device)\n",
    "                ood_logits = model(ood_data[:, :-1])\n",
    "                ood_predictions = ood_logits.argmax(dim=-1)\n",
    "                ood_targets = ood_data[:, 1:]\n",
    "                ood_acc = (ood_predictions[:, ood_len:] == ood_targets[:, ood_len:]).float().mean().item()\n",
    "                ood_accs[ood_len] = ood_acc\n",
    "                training_history[\"ood_induction_accs\"][ood_len].append(ood_acc)\n",
    "\n",
    "        # Save attention snapshots by running a dedicated forward pass with seq_len=20\n",
    "        with torch.no_grad():\n",
    "            _, snap_data = generate_rrt(num_samples=1, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=20)\n",
    "            snap_data = snap_data.to(device)\n",
    "            _ = model(snap_data[:, :-1])  # run forward to populate attn_weights\n",
    "            attention_patterns = []\n",
    "            for block in model.attention_blocks:\n",
    "                # attn_weights shape: [batch, n_heads, seq, seq] -> take first batch, detach\n",
    "                attention_patterns.append(block.attn_weights[0].detach().cpu())\n",
    "            # Stack to [n_layers, n_heads, seq, seq]\n",
    "            training_history[\"attention_snapshots\"][epoch] = torch.stack(attention_patterns)\n",
    "        \n",
    "        ood_str = \", \".join([f\"OOD_{k}={v:.2%}\" for k, v in ood_accs.items()])\n",
    "        print(f\"Epoch {epoch}: Loss={loss.item():.4f}, Acc={accuracy:.2%}, Induction={induction_acc:.2%}, {ood_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44656cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters and training history\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = results_dir / f\"2L_varied_model_{epochs}.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b8ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20000: Loss=1.6728, Acc=70.85%, Induction=98.38%, OOD10=95.88%, OOD15=91.85%, OOD35=88.28%, OOD40=82.61%\n",
      "Epoch 20100: Loss=1.9788, Acc=65.31%, Induction=97.72%, OOD10=94.20%, OOD15=91.31%, OOD35=88.85%, OOD40=81.17%\n",
      "Epoch 20200: Loss=1.8109, Acc=68.75%, Induction=99.07%, OOD10=93.25%, OOD15=89.01%, OOD35=90.48%, OOD40=82.61%\n",
      "Epoch 20300: Loss=2.0822, Acc=63.92%, Induction=97.24%, OOD10=95.15%, OOD15=91.89%, OOD35=88.57%, OOD40=81.81%\n",
      "Epoch 20400: Loss=1.9072, Acc=66.93%, Induction=97.82%, OOD10=94.97%, OOD15=89.36%, OOD35=88.49%, OOD40=81.97%\n",
      "Epoch 20500: Loss=1.8916, Acc=67.25%, Induction=98.70%, OOD10=95.97%, OOD15=91.94%, OOD35=89.06%, OOD40=82.93%\n",
      "Epoch 20600: Loss=1.5650, Acc=72.23%, Induction=95.39%, OOD10=96.01%, OOD15=91.16%, OOD35=88.42%, OOD40=82.85%\n",
      "Epoch 20700: Loss=2.0975, Acc=63.65%, Induction=97.00%, OOD10=94.20%, OOD15=87.99%, OOD35=88.28%, OOD40=82.13%\n",
      "Epoch 20800: Loss=2.1645, Acc=62.18%, Induction=96.94%, OOD10=93.89%, OOD15=90.43%, OOD35=87.78%, OOD40=81.97%\n",
      "Epoch 20900: Loss=1.6116, Acc=72.11%, Induction=98.30%, OOD10=93.30%, OOD15=92.29%, OOD35=86.72%, OOD40=81.81%\n",
      "Epoch 21000: Loss=1.6977, Acc=70.61%, Induction=98.16%, OOD10=94.29%, OOD15=90.33%, OOD35=86.51%, OOD40=82.53%\n",
      "Epoch 21100: Loss=1.9934, Acc=65.86%, Induction=98.44%, OOD10=94.29%, OOD15=89.01%, OOD35=88.28%, OOD40=81.17%\n",
      "Epoch 21200: Loss=1.5721, Acc=72.55%, Induction=97.20%, OOD10=93.98%, OOD15=90.38%, OOD35=85.58%, OOD40=81.17%\n",
      "Epoch 21300: Loss=1.9639, Acc=66.10%, Induction=98.74%, OOD10=96.51%, OOD15=92.43%, OOD35=89.42%, OOD40=81.57%\n",
      "Epoch 21400: Loss=1.5692, Acc=72.35%, Induction=95.60%, OOD10=93.52%, OOD15=89.65%, OOD35=85.94%, OOD40=80.13%\n",
      "Epoch 21500: Loss=1.7394, Acc=70.09%, Induction=99.03%, OOD10=93.16%, OOD15=87.16%, OOD35=84.59%, OOD40=77.48%\n",
      "Epoch 21600: Loss=1.8313, Acc=68.00%, Induction=98.38%, OOD10=96.47%, OOD15=92.04%, OOD35=86.86%, OOD40=77.72%\n",
      "Epoch 21700: Loss=2.1959, Acc=61.95%, Induction=96.25%, OOD10=95.79%, OOD15=91.26%, OOD35=86.36%, OOD40=77.48%\n",
      "Epoch 21800: Loss=1.7451, Acc=69.62%, Induction=98.58%, OOD10=96.88%, OOD15=92.53%, OOD35=85.65%, OOD40=77.40%\n",
      "Epoch 21900: Loss=1.9874, Acc=65.51%, Induction=97.90%, OOD10=94.34%, OOD15=87.21%, OOD35=84.73%, OOD40=75.40%\n",
      "Epoch 22000: Loss=1.6079, Acc=71.95%, Induction=98.52%, OOD10=93.12%, OOD15=91.70%, OOD35=87.43%, OOD40=81.33%\n",
      "Epoch 22100: Loss=2.0431, Acc=64.52%, Induction=98.16%, OOD10=96.33%, OOD15=92.68%, OOD35=88.49%, OOD40=81.33%\n",
      "Epoch 22200: Loss=2.1944, Acc=61.79%, Induction=96.50%, OOD10=93.98%, OOD15=89.21%, OOD35=85.80%, OOD40=76.44%\n",
      "Epoch 22300: Loss=1.8871, Acc=67.25%, Induction=98.47%, OOD10=96.88%, OOD15=92.53%, OOD35=86.51%, OOD40=79.09%\n",
      "Epoch 22400: Loss=1.5414, Acc=72.71%, Induction=97.84%, OOD10=95.56%, OOD15=90.53%, OOD35=83.66%, OOD40=74.84%\n",
      "Epoch 22500: Loss=1.7411, Acc=69.94%, Induction=99.03%, OOD10=93.61%, OOD15=90.23%, OOD35=85.44%, OOD40=77.00%\n",
      "Epoch 22600: Loss=1.8305, Acc=68.39%, Induction=98.44%, OOD10=95.34%, OOD15=89.45%, OOD35=86.01%, OOD40=77.16%\n",
      "Epoch 22700: Loss=1.9142, Acc=66.93%, Induction=98.23%, OOD10=95.88%, OOD15=91.41%, OOD35=84.30%, OOD40=79.73%\n",
      "Epoch 22800: Loss=1.8315, Acc=68.28%, Induction=98.21%, OOD10=93.61%, OOD15=91.94%, OOD35=83.52%, OOD40=76.36%\n",
      "Epoch 22900: Loss=1.5218, Acc=73.26%, Induction=97.09%, OOD10=95.65%, OOD15=89.60%, OOD35=84.66%, OOD40=77.24%\n",
      "Epoch 23000: Loss=1.9859, Acc=65.15%, Induction=97.48%, OOD10=96.20%, OOD15=90.38%, OOD35=80.61%, OOD40=72.68%\n",
      "Epoch 23100: Loss=2.0620, Acc=64.60%, Induction=98.22%, OOD10=93.84%, OOD15=88.28%, OOD35=82.10%, OOD40=75.80%\n",
      "Epoch 23200: Loss=1.6877, Acc=70.89%, Induction=98.27%, OOD10=95.24%, OOD15=90.53%, OOD35=84.23%, OOD40=79.73%\n",
      "Epoch 23300: Loss=1.8394, Acc=68.24%, Induction=97.86%, OOD10=96.33%, OOD15=90.87%, OOD35=77.77%, OOD40=70.51%\n",
      "Epoch 23400: Loss=1.9058, Acc=67.17%, Induction=98.00%, OOD10=96.33%, OOD15=89.65%, OOD35=83.88%, OOD40=72.12%\n",
      "Epoch 23500: Loss=1.9520, Acc=65.86%, Induction=98.20%, OOD10=95.34%, OOD15=90.38%, OOD35=88.07%, OOD40=75.64%\n",
      "Epoch 23600: Loss=1.6232, Acc=71.52%, Induction=97.53%, OOD10=94.38%, OOD15=85.60%, OOD35=82.60%, OOD40=71.31%\n",
      "Epoch 23700: Loss=1.7403, Acc=69.38%, Induction=98.12%, OOD10=93.70%, OOD15=87.60%, OOD35=82.88%, OOD40=77.16%\n",
      "Epoch 23800: Loss=2.1733, Acc=62.30%, Induction=97.00%, OOD10=97.42%, OOD15=92.29%, OOD35=82.46%, OOD40=73.96%\n",
      "Epoch 23900: Loss=2.0504, Acc=64.20%, Induction=97.86%, OOD10=94.57%, OOD15=89.65%, OOD35=80.40%, OOD40=76.68%\n",
      "Epoch 24000: Loss=1.5614, Acc=72.07%, Induction=95.29%, OOD10=95.74%, OOD15=90.38%, OOD35=82.53%, OOD40=74.76%\n",
      "Epoch 24100: Loss=1.9921, Acc=65.70%, Induction=98.14%, OOD10=96.69%, OOD15=90.23%, OOD35=78.84%, OOD40=71.88%\n",
      "Epoch 24200: Loss=1.7507, Acc=69.34%, Induction=98.12%, OOD10=95.92%, OOD15=90.04%, OOD35=83.95%, OOD40=71.63%\n",
      "Epoch 24300: Loss=1.8054, Acc=68.75%, Induction=98.55%, OOD10=89.09%, OOD15=82.18%, OOD35=76.99%, OOD40=64.42%\n",
      "Epoch 24400: Loss=1.5914, Acc=72.39%, Induction=98.68%, OOD10=93.75%, OOD15=86.72%, OOD35=74.29%, OOD40=68.67%\n",
      "Epoch 24500: Loss=1.8788, Acc=67.44%, Induction=98.35%, OOD10=94.02%, OOD15=85.21%, OOD35=79.05%, OOD40=69.47%\n",
      "Epoch 24600: Loss=2.0562, Acc=64.44%, Induction=97.86%, OOD10=91.98%, OOD15=87.84%, OOD35=77.20%, OOD40=68.83%\n",
      "Epoch 24700: Loss=1.8661, Acc=67.48%, Induction=98.76%, OOD10=96.24%, OOD15=91.89%, OOD35=79.33%, OOD40=72.12%\n",
      "Epoch 24800: Loss=1.5472, Acc=73.10%, Induction=96.93%, OOD10=95.29%, OOD15=90.19%, OOD35=77.70%, OOD40=71.31%\n",
      "Epoch 24900: Loss=1.5978, Acc=72.11%, Induction=98.46%, OOD10=94.70%, OOD15=87.94%, OOD35=80.61%, OOD40=72.04%\n",
      "Epoch 25000: Loss=1.6644, Acc=70.89%, Induction=98.44%, OOD10=96.56%, OOD15=90.53%, OOD35=80.18%, OOD40=72.44%\n",
      "Epoch 25100: Loss=2.1861, Acc=62.30%, Induction=97.31%, OOD10=96.74%, OOD15=91.46%, OOD35=75.92%, OOD40=70.51%\n",
      "Epoch 25200: Loss=2.0679, Acc=63.81%, Induction=97.12%, OOD10=93.84%, OOD15=86.96%, OOD35=78.27%, OOD40=72.20%\n",
      "Epoch 25300: Loss=2.1574, Acc=62.34%, Induction=97.37%, OOD10=95.47%, OOD15=87.84%, OOD35=74.50%, OOD40=63.06%\n",
      "Epoch 25400: Loss=1.7957, Acc=68.35%, Induction=98.73%, OOD10=95.92%, OOD15=91.60%, OOD35=74.64%, OOD40=68.59%\n",
      "Epoch 25500: Loss=1.6721, Acc=70.97%, Induction=98.38%, OOD10=94.88%, OOD15=84.81%, OOD35=71.59%, OOD40=62.18%\n",
      "Epoch 25600: Loss=1.9744, Acc=65.55%, Induction=97.78%, OOD10=96.06%, OOD15=88.72%, OOD35=72.94%, OOD40=65.62%\n",
      "Epoch 25700: Loss=1.8136, Acc=68.31%, Induction=98.26%, OOD10=96.88%, OOD15=91.85%, OOD35=75.43%, OOD40=71.07%\n",
      "Epoch 25800: Loss=2.1756, Acc=61.75%, Induction=96.00%, OOD10=98.01%, OOD15=92.48%, OOD35=75.07%, OOD40=67.95%\n",
      "Epoch 25900: Loss=1.6645, Acc=71.28%, Induction=98.77%, OOD10=94.84%, OOD15=90.43%, OOD35=71.16%, OOD40=62.66%\n",
      "Epoch 26000: Loss=1.9653, Acc=66.26%, Induction=98.74%, OOD10=93.93%, OOD15=87.06%, OOD35=77.63%, OOD40=70.27%\n",
      "Epoch 26100: Loss=1.6596, Acc=71.16%, Induction=98.66%, OOD10=93.07%, OOD15=84.62%, OOD35=75.21%, OOD40=68.35%\n",
      "Epoch 26200: Loss=1.6670, Acc=71.20%, Induction=98.49%, OOD10=92.93%, OOD15=84.91%, OOD35=64.84%, OOD40=55.61%\n",
      "Epoch 26300: Loss=1.8860, Acc=67.13%, Induction=98.58%, OOD10=95.02%, OOD15=91.65%, OOD35=74.36%, OOD40=66.83%\n",
      "Epoch 26400: Loss=1.5842, Acc=72.19%, Induction=98.30%, OOD10=96.15%, OOD15=86.28%, OOD35=64.70%, OOD40=57.29%\n",
      "Epoch 26500: Loss=1.8898, Acc=67.29%, Induction=98.47%, OOD10=95.15%, OOD15=90.92%, OOD35=74.86%, OOD40=67.23%\n",
      "Epoch 26600: Loss=1.5234, Acc=73.54%, Induction=97.09%, OOD10=96.01%, OOD15=89.55%, OOD35=69.46%, OOD40=61.38%\n",
      "Epoch 26700: Loss=1.8880, Acc=67.09%, Induction=98.05%, OOD10=95.06%, OOD15=90.04%, OOD35=76.85%, OOD40=68.59%\n",
      "Epoch 26800: Loss=1.5847, Acc=72.15%, Induction=98.46%, OOD10=93.80%, OOD15=87.79%, OOD35=70.67%, OOD40=64.82%\n",
      "Epoch 26900: Loss=2.0303, Acc=64.95%, Induction=98.77%, OOD10=94.88%, OOD15=88.09%, OOD35=70.10%, OOD40=61.54%\n",
      "Epoch 27000: Loss=2.0346, Acc=64.52%, Induction=97.86%, OOD10=93.84%, OOD15=88.72%, OOD35=72.02%, OOD40=63.06%\n",
      "Epoch 27100: Loss=1.8833, Acc=67.48%, Induction=98.88%, OOD10=95.97%, OOD15=88.87%, OOD35=76.78%, OOD40=64.98%\n",
      "Epoch 27200: Loss=1.5900, Acc=72.35%, Induction=98.36%, OOD10=92.98%, OOD15=84.23%, OOD35=66.05%, OOD40=57.53%\n",
      "Epoch 27300: Loss=1.7961, Acc=68.83%, Induction=98.96%, OOD10=95.92%, OOD15=86.08%, OOD35=70.24%, OOD40=54.57%\n",
      "Epoch 27400: Loss=1.5906, Acc=72.39%, Induction=98.52%, OOD10=95.43%, OOD15=87.50%, OOD35=72.59%, OOD40=58.73%\n",
      "Epoch 27500: Loss=1.8073, Acc=68.59%, Induction=98.55%, OOD10=96.20%, OOD15=87.60%, OOD35=63.49%, OOD40=56.97%\n",
      "Epoch 27600: Loss=1.5473, Acc=73.02%, Induction=97.79%, OOD10=93.75%, OOD15=84.03%, OOD35=63.57%, OOD40=54.25%\n",
      "Epoch 27700: Loss=1.6607, Acc=71.44%, Induction=99.00%, OOD10=94.97%, OOD15=87.45%, OOD35=66.12%, OOD40=57.69%\n",
      "Epoch 27800: Loss=1.5045, Acc=73.66%, Induction=97.03%, OOD10=93.25%, OOD15=86.96%, OOD35=67.47%, OOD40=54.09%\n",
      "Epoch 27900: Loss=2.1581, Acc=62.38%, Induction=96.94%, OOD10=94.70%, OOD15=91.26%, OOD35=69.89%, OOD40=56.41%\n",
      "Epoch 28000: Loss=1.7893, Acc=68.71%, Induction=98.73%, OOD10=95.06%, OOD15=87.21%, OOD35=67.97%, OOD40=57.53%\n",
      "Epoch 28100: Loss=2.0632, Acc=64.12%, Induction=97.86%, OOD10=96.60%, OOD15=92.63%, OOD35=67.83%, OOD40=60.42%\n",
      "Epoch 28200: Loss=1.9211, Acc=66.89%, Induction=99.52%, OOD10=93.84%, OOD15=86.77%, OOD35=67.26%, OOD40=54.97%\n",
      "Epoch 28300: Loss=1.7852, Acc=69.19%, Induction=99.36%, OOD10=92.30%, OOD15=85.40%, OOD35=60.65%, OOD40=51.44%\n",
      "Epoch 28400: Loss=1.8874, Acc=67.13%, Induction=98.17%, OOD10=93.80%, OOD15=84.72%, OOD35=54.90%, OOD40=48.64%\n",
      "Epoch 28500: Loss=1.5447, Acc=72.55%, Induction=97.36%, OOD10=93.48%, OOD15=85.40%, OOD35=60.87%, OOD40=50.24%\n",
      "Epoch 28600: Loss=1.5665, Acc=72.94%, Induction=99.40%, OOD10=95.15%, OOD15=89.06%, OOD35=69.53%, OOD40=55.05%\n",
      "Epoch 28700: Loss=1.5889, Acc=72.19%, Induction=98.41%, OOD10=94.20%, OOD15=86.82%, OOD35=59.80%, OOD40=47.52%\n",
      "Epoch 28800: Loss=2.1381, Acc=63.09%, Induction=98.00%, OOD10=95.06%, OOD15=88.43%, OOD35=61.29%, OOD40=51.44%\n",
      "Epoch 28900: Loss=1.9491, Acc=65.94%, Induction=98.56%, OOD10=95.02%, OOD15=85.45%, OOD35=59.52%, OOD40=50.64%\n",
      "Epoch 29000: Loss=1.5218, Acc=73.50%, Induction=98.33%, OOD10=93.75%, OOD15=86.72%, OOD35=60.16%, OOD40=49.60%\n",
      "Epoch 29100: Loss=1.8923, Acc=66.69%, Induction=97.64%, OOD10=94.34%, OOD15=85.74%, OOD35=65.20%, OOD40=53.45%\n",
      "Epoch 29200: Loss=1.7873, Acc=68.95%, Induction=99.07%, OOD10=94.07%, OOD15=86.52%, OOD35=62.86%, OOD40=49.44%\n",
      "Epoch 29300: Loss=1.8428, Acc=67.68%, Induction=98.88%, OOD10=93.98%, OOD15=89.31%, OOD35=65.70%, OOD40=55.61%\n",
      "Epoch 29400: Loss=2.1455, Acc=62.66%, Induction=97.50%, OOD10=92.93%, OOD15=86.04%, OOD35=60.94%, OOD40=48.64%\n",
      "Epoch 29500: Loss=1.7331, Acc=69.82%, Induction=98.30%, OOD10=93.70%, OOD15=85.99%, OOD35=66.12%, OOD40=49.52%\n",
      "Epoch 29600: Loss=2.1165, Acc=63.09%, Induction=97.56%, OOD10=92.62%, OOD15=85.55%, OOD35=62.22%, OOD40=50.56%\n",
      "Epoch 29700: Loss=1.5156, Acc=73.34%, Induction=98.22%, OOD10=95.06%, OOD15=86.87%, OOD35=59.87%, OOD40=46.88%\n",
      "Epoch 29800: Loss=1.5019, Acc=73.69%, Induction=98.55%, OOD10=92.21%, OOD15=79.74%, OOD35=62.93%, OOD40=48.80%\n",
      "Epoch 29900: Loss=1.7313, Acc=69.38%, Induction=98.01%, OOD10=97.60%, OOD15=88.48%, OOD35=64.91%, OOD40=49.52%\n",
      "Epoch 30000: Loss=1.6472, Acc=71.48%, Induction=98.88%, OOD10=93.34%, OOD15=82.13%, OOD35=61.79%, OOD40=46.71%\n",
      "Epoch 30100: Loss=1.6332, Acc=71.52%, Induction=99.33%, OOD10=94.75%, OOD15=86.33%, OOD35=54.83%, OOD40=42.39%\n",
      "Epoch 30200: Loss=2.1425, Acc=62.22%, Induction=96.88%, OOD10=91.71%, OOD15=86.04%, OOD35=61.29%, OOD40=47.28%\n",
      "Epoch 30300: Loss=1.9503, Acc=66.10%, Induction=98.62%, OOD10=93.57%, OOD15=83.30%, OOD35=56.61%, OOD40=44.07%\n",
      "Epoch 30400: Loss=1.5563, Acc=72.90%, Induction=99.18%, OOD10=94.79%, OOD15=84.33%, OOD35=51.70%, OOD40=40.22%\n",
      "Epoch 30500: Loss=1.6561, Acc=71.12%, Induction=98.77%, OOD10=94.02%, OOD15=88.38%, OOD35=61.43%, OOD40=49.60%\n",
      "Epoch 30600: Loss=2.1771, Acc=62.30%, Induction=96.63%, OOD10=96.42%, OOD15=83.89%, OOD35=53.12%, OOD40=42.63%\n",
      "Epoch 30700: Loss=2.1367, Acc=63.21%, Induction=98.31%, OOD10=94.38%, OOD15=84.28%, OOD35=60.58%, OOD40=45.75%\n",
      "Epoch 30800: Loss=2.1122, Acc=63.21%, Induction=97.94%, OOD10=95.79%, OOD15=87.50%, OOD35=60.23%, OOD40=44.87%\n",
      "Epoch 30900: Loss=1.6998, Acc=70.29%, Induction=99.15%, OOD10=93.70%, OOD15=86.18%, OOD35=64.99%, OOD40=48.56%\n",
      "Epoch 31000: Loss=1.5088, Acc=73.73%, Induction=97.14%, OOD10=94.02%, OOD15=85.55%, OOD35=54.47%, OOD40=42.23%\n",
      "Epoch 31100: Loss=1.7925, Acc=69.15%, Induction=99.02%, OOD10=94.97%, OOD15=83.30%, OOD35=60.37%, OOD40=41.59%\n",
      "Epoch 31200: Loss=1.8590, Acc=67.84%, Induction=99.00%, OOD10=94.84%, OOD15=85.94%, OOD35=59.59%, OOD40=42.63%\n",
      "Epoch 31300: Loss=1.4987, Acc=74.01%, Induction=98.87%, OOD10=90.35%, OOD15=83.64%, OOD35=49.86%, OOD40=37.82%\n",
      "Epoch 31400: Loss=1.7754, Acc=69.11%, Induction=99.25%, OOD10=92.75%, OOD15=83.69%, OOD35=46.73%, OOD40=36.94%\n",
      "Epoch 31500: Loss=1.7159, Acc=70.21%, Induction=98.98%, OOD10=93.21%, OOD15=82.71%, OOD35=53.20%, OOD40=37.26%\n",
      "Epoch 31600: Loss=1.5748, Acc=72.55%, Induction=98.79%, OOD10=91.12%, OOD15=80.32%, OOD35=47.23%, OOD40=35.82%\n",
      "Epoch 31700: Loss=1.5605, Acc=72.86%, Induction=99.23%, OOD10=91.98%, OOD15=82.42%, OOD35=49.36%, OOD40=39.98%\n",
      "Epoch 31800: Loss=1.4814, Acc=74.29%, Induction=97.99%, OOD10=95.24%, OOD15=84.33%, OOD35=58.74%, OOD40=41.83%\n",
      "Epoch 31900: Loss=1.5655, Acc=72.98%, Induction=99.07%, OOD10=92.16%, OOD15=83.06%, OOD35=48.72%, OOD40=35.66%\n",
      "Epoch 32000: Loss=1.5090, Acc=73.73%, Induction=98.33%, OOD10=91.71%, OOD15=81.49%, OOD35=51.35%, OOD40=38.06%\n",
      "Epoch 32100: Loss=1.6334, Acc=71.52%, Induction=99.16%, OOD10=94.88%, OOD15=86.04%, OOD35=57.03%, OOD40=40.22%\n",
      "Epoch 32200: Loss=1.8500, Acc=67.76%, Induction=99.23%, OOD10=92.98%, OOD15=82.32%, OOD35=55.33%, OOD40=38.38%\n",
      "Epoch 32300: Loss=2.1207, Acc=63.33%, Induction=98.19%, OOD10=94.97%, OOD15=88.33%, OOD35=55.33%, OOD40=40.87%\n",
      "Epoch 32400: Loss=1.4937, Acc=73.81%, Induction=98.71%, OOD10=93.57%, OOD15=81.79%, OOD35=52.41%, OOD40=37.50%\n",
      "Epoch 32500: Loss=1.7232, Acc=69.86%, Induction=98.41%, OOD10=93.93%, OOD15=81.25%, OOD35=51.78%, OOD40=38.30%\n",
      "Epoch 32600: Loss=1.6400, Acc=71.80%, Induction=99.44%, OOD10=97.10%, OOD15=86.77%, OOD35=60.37%, OOD40=41.43%\n",
      "Epoch 32700: Loss=1.5628, Acc=72.82%, Induction=99.18%, OOD10=93.84%, OOD15=85.16%, OOD35=49.22%, OOD40=37.74%\n",
      "Epoch 32800: Loss=1.5007, Acc=73.81%, Induction=98.76%, OOD10=91.26%, OOD15=82.37%, OOD35=51.35%, OOD40=37.42%\n",
      "Epoch 32900: Loss=2.1587, Acc=62.78%, Induction=97.37%, OOD10=93.80%, OOD15=85.30%, OOD35=50.07%, OOD40=34.94%\n",
      "Epoch 33000: Loss=1.5677, Acc=72.35%, Induction=98.52%, OOD10=95.43%, OOD15=84.23%, OOD35=47.37%, OOD40=33.81%\n",
      "Epoch 33100: Loss=1.6348, Acc=71.48%, Induction=99.00%, OOD10=92.16%, OOD15=79.49%, OOD35=44.96%, OOD40=33.01%\n",
      "Epoch 33200: Loss=1.6311, Acc=71.60%, Induction=99.27%, OOD10=92.89%, OOD15=85.21%, OOD35=50.21%, OOD40=37.90%\n",
      "Epoch 33300: Loss=1.5780, Acc=72.55%, Induction=98.57%, OOD10=91.26%, OOD15=80.81%, OOD35=47.94%, OOD40=33.73%\n",
      "Epoch 33400: Loss=1.5048, Acc=73.58%, Induction=97.14%, OOD10=89.27%, OOD15=82.18%, OOD35=45.60%, OOD40=33.17%\n",
      "Epoch 33500: Loss=1.6325, Acc=71.56%, Induction=99.16%, OOD10=91.80%, OOD15=79.93%, OOD35=46.02%, OOD40=32.53%\n",
      "Epoch 33600: Loss=1.5032, Acc=73.62%, Induction=98.55%, OOD10=92.89%, OOD15=81.20%, OOD35=47.02%, OOD40=34.94%\n",
      "Epoch 33700: Loss=1.4582, Acc=74.60%, Induction=98.36%, OOD10=91.39%, OOD15=82.67%, OOD35=49.01%, OOD40=37.18%\n",
      "Epoch 33800: Loss=1.4947, Acc=73.89%, Induction=97.72%, OOD10=90.22%, OOD15=83.20%, OOD35=54.62%, OOD40=36.62%\n",
      "Epoch 33900: Loss=2.0956, Acc=63.53%, Induction=98.37%, OOD10=91.80%, OOD15=83.40%, OOD35=53.41%, OOD40=35.98%\n",
      "Epoch 34000: Loss=1.4888, Acc=73.93%, Induction=98.71%, OOD10=94.07%, OOD15=78.71%, OOD35=42.05%, OOD40=27.96%\n",
      "Epoch 34100: Loss=1.8450, Acc=67.68%, Induction=98.70%, OOD10=94.20%, OOD15=81.25%, OOD35=50.71%, OOD40=33.57%\n",
      "Epoch 34200: Loss=1.6336, Acc=71.72%, Induction=99.33%, OOD10=89.09%, OOD15=79.54%, OOD35=47.73%, OOD40=31.73%\n",
      "Epoch 34300: Loss=1.4453, Acc=74.45%, Induction=97.99%, OOD10=92.48%, OOD15=84.86%, OOD35=45.17%, OOD40=33.25%\n",
      "Epoch 34400: Loss=1.4412, Acc=75.32%, Induction=98.89%, OOD10=91.35%, OOD15=80.37%, OOD35=39.13%, OOD40=32.85%\n",
      "Epoch 34500: Loss=2.0276, Acc=64.68%, Induction=98.35%, OOD10=90.08%, OOD15=79.79%, OOD35=42.54%, OOD40=34.05%\n",
      "Epoch 34600: Loss=2.1310, Acc=62.94%, Induction=98.00%, OOD10=92.53%, OOD15=80.13%, OOD35=43.68%, OOD40=31.57%\n",
      "Epoch 34700: Loss=2.0106, Acc=65.03%, Induction=98.41%, OOD10=89.81%, OOD15=76.61%, OOD35=46.38%, OOD40=28.77%\n",
      "Epoch 34800: Loss=1.6216, Acc=71.76%, Induction=99.55%, OOD10=92.53%, OOD15=81.59%, OOD35=48.51%, OOD40=35.74%\n",
      "Epoch 34900: Loss=1.9195, Acc=66.50%, Induction=98.86%, OOD10=92.39%, OOD15=86.38%, OOD35=50.71%, OOD40=35.02%\n",
      "Epoch 35000: Loss=2.0144, Acc=64.91%, Induction=98.59%, OOD10=94.38%, OOD15=84.03%, OOD35=44.96%, OOD40=30.93%\n",
      "Epoch 35100: Loss=1.7844, Acc=68.63%, Induction=98.38%, OOD10=94.11%, OOD15=84.28%, OOD35=49.08%, OOD40=33.49%\n",
      "Epoch 35200: Loss=1.4940, Acc=73.73%, Induction=98.71%, OOD10=91.71%, OOD15=78.66%, OOD35=46.09%, OOD40=30.77%\n",
      "Epoch 35300: Loss=1.6347, Acc=71.68%, Induction=99.22%, OOD10=93.30%, OOD15=81.64%, OOD35=41.34%, OOD40=32.37%\n",
      "Epoch 35400: Loss=1.5596, Acc=72.63%, Induction=98.85%, OOD10=93.07%, OOD15=80.76%, OOD35=37.29%, OOD40=28.93%\n",
      "Epoch 35500: Loss=1.9363, Acc=66.50%, Induction=99.10%, OOD10=90.72%, OOD15=79.98%, OOD35=45.74%, OOD40=31.01%\n",
      "Epoch 35600: Loss=1.4922, Acc=73.69%, Induction=98.65%, OOD10=92.57%, OOD15=80.08%, OOD35=35.87%, OOD40=29.89%\n",
      "Epoch 35700: Loss=2.0086, Acc=65.07%, Induction=98.90%, OOD10=92.62%, OOD15=83.20%, OOD35=48.58%, OOD40=30.45%\n",
      "Epoch 35800: Loss=1.4540, Acc=74.64%, Induction=98.20%, OOD10=90.67%, OOD15=79.69%, OOD35=42.12%, OOD40=31.01%\n",
      "Epoch 35900: Loss=1.7699, Acc=69.22%, Induction=99.19%, OOD10=90.26%, OOD15=76.81%, OOD35=49.01%, OOD40=28.37%\n",
      "Epoch 36000: Loss=1.8411, Acc=67.96%, Induction=99.23%, OOD10=89.49%, OOD15=81.30%, OOD35=43.61%, OOD40=30.05%\n",
      "Epoch 36100: Loss=1.7051, Acc=70.37%, Induction=99.26%, OOD10=91.98%, OOD15=77.34%, OOD35=43.96%, OOD40=29.33%\n",
      "Epoch 36200: Loss=1.6232, Acc=71.72%, Induction=99.33%, OOD10=93.30%, OOD15=79.20%, OOD35=45.53%, OOD40=31.33%\n",
      "Epoch 36300: Loss=1.7002, Acc=70.49%, Induction=99.26%, OOD10=89.81%, OOD15=76.07%, OOD35=43.47%, OOD40=26.68%\n",
      "Epoch 36400: Loss=1.9190, Acc=66.61%, Induction=98.92%, OOD10=91.44%, OOD15=81.49%, OOD35=42.05%, OOD40=29.81%\n",
      "Epoch 36500: Loss=1.6262, Acc=71.60%, Induction=99.11%, OOD10=92.39%, OOD15=81.98%, OOD35=38.49%, OOD40=25.96%\n",
      "Epoch 36600: Loss=1.4946, Acc=74.33%, Induction=99.35%, OOD10=90.40%, OOD15=79.35%, OOD35=39.70%, OOD40=28.21%\n",
      "Epoch 36700: Loss=1.9358, Acc=66.34%, Induction=98.56%, OOD10=94.07%, OOD15=82.18%, OOD35=42.90%, OOD40=27.56%\n",
      "Epoch 36800: Loss=1.8484, Acc=67.76%, Induction=98.94%, OOD10=92.71%, OOD15=81.59%, OOD35=44.03%, OOD40=28.45%\n",
      "Epoch 36900: Loss=1.4808, Acc=74.09%, Induction=99.14%, OOD10=92.16%, OOD15=77.44%, OOD35=33.31%, OOD40=25.88%\n",
      "Epoch 37000: Loss=1.7066, Acc=70.29%, Induction=98.98%, OOD10=89.45%, OOD15=78.27%, OOD35=39.06%, OOD40=26.36%\n",
      "Epoch 37100: Loss=1.8420, Acc=67.92%, Induction=99.23%, OOD10=92.53%, OOD15=78.81%, OOD35=42.54%, OOD40=26.76%\n",
      "Epoch 37200: Loss=1.7810, Acc=68.99%, Induction=98.84%, OOD10=92.03%, OOD15=80.37%, OOD35=40.62%, OOD40=22.60%\n",
      "Epoch 37300: Loss=1.4786, Acc=74.25%, Induction=99.14%, OOD10=89.27%, OOD15=73.29%, OOD35=28.98%, OOD40=23.72%\n",
      "Epoch 37400: Loss=1.6184, Acc=71.60%, Induction=99.22%, OOD10=89.45%, OOD15=77.59%, OOD35=30.11%, OOD40=25.24%\n",
      "Epoch 37500: Loss=1.6348, Acc=71.44%, Induction=98.94%, OOD10=92.21%, OOD15=77.25%, OOD35=32.95%, OOD40=24.68%\n",
      "Epoch 37600: Loss=2.0930, Acc=63.69%, Induction=98.87%, OOD10=90.17%, OOD15=79.20%, OOD35=33.38%, OOD40=23.72%\n",
      "Epoch 37700: Loss=1.7759, Acc=68.79%, Induction=98.61%, OOD10=93.30%, OOD15=78.76%, OOD35=27.63%, OOD40=21.23%\n",
      "Epoch 37800: Loss=1.5473, Acc=73.06%, Induction=99.51%, OOD10=92.84%, OOD15=80.22%, OOD35=38.85%, OOD40=24.28%\n",
      "Epoch 37900: Loss=1.4376, Acc=75.24%, Induction=99.05%, OOD10=94.47%, OOD15=79.74%, OOD35=29.97%, OOD40=25.00%\n",
      "Epoch 38000: Loss=1.4775, Acc=74.29%, Induction=97.93%, OOD10=90.99%, OOD15=78.76%, OOD35=39.56%, OOD40=25.48%\n",
      "Epoch 38100: Loss=1.6162, Acc=72.11%, Induction=99.61%, OOD10=92.57%, OOD15=79.25%, OOD35=34.94%, OOD40=24.60%\n",
      "Epoch 38200: Loss=1.4531, Acc=74.72%, Induction=98.41%, OOD10=91.49%, OOD15=77.00%, OOD35=37.00%, OOD40=23.00%\n",
      "Epoch 38300: Loss=1.5651, Acc=72.63%, Induction=98.74%, OOD10=91.71%, OOD15=79.39%, OOD35=28.91%, OOD40=20.19%\n",
      "Epoch 38400: Loss=1.7692, Acc=68.95%, Induction=98.90%, OOD10=91.53%, OOD15=77.78%, OOD35=26.56%, OOD40=21.79%\n",
      "Epoch 38500: Loss=1.6287, Acc=71.72%, Induction=99.27%, OOD10=89.67%, OOD15=77.88%, OOD35=27.84%, OOD40=19.07%\n",
      "Epoch 38600: Loss=2.0061, Acc=65.11%, Induction=99.02%, OOD10=90.62%, OOD15=81.84%, OOD35=36.51%, OOD40=23.64%\n",
      "Epoch 38700: Loss=1.7656, Acc=69.42%, Induction=99.54%, OOD10=93.80%, OOD15=78.76%, OOD35=30.47%, OOD40=21.07%\n",
      "Epoch 38800: Loss=2.1209, Acc=63.29%, Induction=98.12%, OOD10=94.11%, OOD15=82.18%, OOD35=32.32%, OOD40=22.28%\n",
      "Epoch 38900: Loss=1.4599, Acc=74.37%, Induction=98.04%, OOD10=94.70%, OOD15=82.52%, OOD35=36.29%, OOD40=22.36%\n",
      "Epoch 39000: Loss=1.7532, Acc=69.38%, Induction=99.54%, OOD10=90.99%, OOD15=77.64%, OOD35=29.76%, OOD40=20.51%\n",
      "Epoch 39100: Loss=1.7731, Acc=69.19%, Induction=99.25%, OOD10=90.94%, OOD15=78.76%, OOD35=40.48%, OOD40=18.83%\n",
      "Epoch 39200: Loss=1.8565, Acc=67.76%, Induction=98.94%, OOD10=93.39%, OOD15=78.47%, OOD35=36.01%, OOD40=23.72%\n",
      "Epoch 39300: Loss=2.0024, Acc=65.19%, Induction=98.96%, OOD10=90.81%, OOD15=78.32%, OOD35=24.29%, OOD40=16.83%\n",
      "Epoch 39400: Loss=1.8561, Acc=67.80%, Induction=99.23%, OOD10=87.77%, OOD15=77.54%, OOD35=29.47%, OOD40=19.39%\n",
      "Epoch 39500: Loss=1.6305, Acc=71.20%, Induction=98.55%, OOD10=91.30%, OOD15=77.54%, OOD35=31.32%, OOD40=19.79%\n",
      "Epoch 39600: Loss=1.7057, Acc=70.37%, Induction=99.20%, OOD10=88.86%, OOD15=82.52%, OOD35=31.96%, OOD40=16.11%\n",
      "Epoch 39700: Loss=2.0215, Acc=65.03%, Induction=98.84%, OOD10=90.08%, OOD15=78.96%, OOD35=26.63%, OOD40=13.46%\n",
      "Epoch 39800: Loss=2.0872, Acc=63.81%, Induction=99.06%, OOD10=93.61%, OOD15=80.76%, OOD35=34.87%, OOD40=17.55%\n",
      "Epoch 39900: Loss=1.5413, Acc=73.38%, Induction=99.73%, OOD10=91.17%, OOD15=76.17%, OOD35=26.21%, OOD40=17.07%\n",
      "Epoch 40000: Loss=1.9896, Acc=65.23%, Induction=99.08%, OOD10=94.61%, OOD15=79.39%, OOD35=28.34%, OOD40=17.15%\n",
      "Epoch 40100: Loss=1.6343, Acc=71.72%, Induction=99.00%, OOD10=88.36%, OOD15=73.44%, OOD35=28.48%, OOD40=17.87%\n",
      "Epoch 40200: Loss=1.6204, Acc=71.68%, Induction=99.22%, OOD10=90.94%, OOD15=76.90%, OOD35=25.64%, OOD40=15.30%\n",
      "Epoch 40300: Loss=1.7733, Acc=69.19%, Induction=99.19%, OOD10=95.06%, OOD15=80.52%, OOD35=30.75%, OOD40=16.59%\n",
      "Epoch 40400: Loss=1.9925, Acc=65.19%, Induction=98.90%, OOD10=91.71%, OOD15=79.88%, OOD35=32.88%, OOD40=14.50%\n",
      "Epoch 40500: Loss=1.6884, Acc=70.53%, Induction=99.37%, OOD10=91.94%, OOD15=77.15%, OOD35=23.30%, OOD40=13.62%\n",
      "Epoch 40600: Loss=1.6971, Acc=70.45%, Induction=99.26%, OOD10=91.67%, OOD15=79.10%, OOD35=25.78%, OOD40=14.82%\n",
      "Epoch 40700: Loss=1.8496, Acc=67.88%, Induction=99.06%, OOD10=90.26%, OOD15=78.22%, OOD35=34.94%, OOD40=14.42%\n",
      "Epoch 40800: Loss=1.8359, Acc=67.92%, Induction=99.17%, OOD10=90.35%, OOD15=75.78%, OOD35=29.76%, OOD40=11.54%\n",
      "Epoch 40900: Loss=1.9262, Acc=66.50%, Induction=99.16%, OOD10=92.35%, OOD15=77.64%, OOD35=24.72%, OOD40=12.34%\n",
      "Epoch 41000: Loss=1.4975, Acc=74.09%, Induction=99.14%, OOD10=92.98%, OOD15=77.25%, OOD35=33.38%, OOD40=12.74%\n",
      "Epoch 41100: Loss=1.5352, Acc=72.90%, Induction=99.18%, OOD10=92.53%, OOD15=76.81%, OOD35=28.34%, OOD40=17.87%\n",
      "Epoch 41200: Loss=1.6204, Acc=71.80%, Induction=99.55%, OOD10=88.13%, OOD15=71.19%, OOD35=21.88%, OOD40=15.14%\n",
      "Epoch 41300: Loss=1.5708, Acc=72.59%, Induction=98.85%, OOD10=90.08%, OOD15=78.86%, OOD35=27.41%, OOD40=9.78%\n",
      "Epoch 41400: Loss=1.9165, Acc=66.65%, Induction=99.04%, OOD10=92.44%, OOD15=77.34%, OOD35=27.70%, OOD40=10.66%\n",
      "Epoch 41500: Loss=1.8455, Acc=67.76%, Induction=98.82%, OOD10=93.39%, OOD15=77.73%, OOD35=26.78%, OOD40=10.58%\n",
      "Epoch 41600: Loss=1.9242, Acc=66.65%, Induction=99.28%, OOD10=90.17%, OOD15=78.32%, OOD35=26.14%, OOD40=10.26%\n",
      "Epoch 41700: Loss=1.7017, Acc=70.37%, Induction=99.20%, OOD10=90.99%, OOD15=78.61%, OOD35=30.97%, OOD40=13.46%\n",
      "Epoch 41800: Loss=2.0844, Acc=63.41%, Induction=98.37%, OOD10=92.07%, OOD15=77.78%, OOD35=27.34%, OOD40=10.42%\n",
      "Epoch 41900: Loss=1.7680, Acc=68.91%, Induction=98.90%, OOD10=91.62%, OOD15=80.08%, OOD35=32.74%, OOD40=13.78%\n",
      "Epoch 42000: Loss=2.0988, Acc=63.45%, Induction=98.12%, OOD10=93.12%, OOD15=76.56%, OOD35=23.44%, OOD40=10.42%\n",
      "Epoch 42100: Loss=1.4327, Acc=75.08%, Induction=98.89%, OOD10=91.30%, OOD15=81.05%, OOD35=28.34%, OOD40=12.58%\n",
      "Epoch 42200: Loss=1.6907, Acc=70.77%, Induction=99.77%, OOD10=89.22%, OOD15=75.88%, OOD35=22.16%, OOD40=8.09%\n",
      "Epoch 42300: Loss=2.0681, Acc=63.96%, Induction=99.12%, OOD10=88.90%, OOD15=77.15%, OOD35=24.57%, OOD40=10.02%\n",
      "Epoch 42400: Loss=1.8418, Acc=67.68%, Induction=99.00%, OOD10=93.03%, OOD15=78.03%, OOD35=25.00%, OOD40=5.93%\n",
      "Epoch 42500: Loss=1.6989, Acc=70.33%, Induction=99.32%, OOD10=92.57%, OOD15=78.66%, OOD35=23.79%, OOD40=9.70%\n",
      "Epoch 42600: Loss=1.5510, Acc=73.10%, Induction=99.51%, OOD10=87.77%, OOD15=72.22%, OOD35=24.08%, OOD40=6.57%\n",
      "Epoch 42700: Loss=2.0741, Acc=64.00%, Induction=99.00%, OOD10=88.72%, OOD15=72.07%, OOD35=24.22%, OOD40=9.78%\n",
      "Epoch 42800: Loss=2.0671, Acc=63.88%, Induction=99.25%, OOD10=90.40%, OOD15=78.81%, OOD35=34.09%, OOD40=12.26%\n",
      "Epoch 42900: Loss=1.4801, Acc=74.13%, Induction=99.08%, OOD10=92.03%, OOD15=78.32%, OOD35=24.29%, OOD40=6.01%\n",
      "Epoch 43000: Loss=1.6258, Acc=71.76%, Induction=99.27%, OOD10=91.89%, OOD15=80.62%, OOD35=26.63%, OOD40=8.73%\n",
      "Epoch 43100: Loss=1.6861, Acc=70.65%, Induction=99.49%, OOD10=92.62%, OOD15=81.40%, OOD35=26.99%, OOD40=6.09%\n",
      "Epoch 43200: Loss=1.4758, Acc=74.49%, Induction=99.68%, OOD10=90.62%, OOD15=79.44%, OOD35=19.89%, OOD40=6.01%\n",
      "Epoch 43300: Loss=1.8402, Acc=67.92%, Induction=99.35%, OOD10=90.85%, OOD15=77.34%, OOD35=25.43%, OOD40=6.81%\n",
      "Epoch 43400: Loss=1.4798, Acc=74.49%, Induction=99.73%, OOD10=89.76%, OOD15=75.63%, OOD35=25.07%, OOD40=4.09%\n",
      "Epoch 43500: Loss=2.0855, Acc=63.65%, Induction=98.87%, OOD10=89.27%, OOD15=77.49%, OOD35=20.38%, OOD40=6.25%\n",
      "Epoch 43600: Loss=1.8355, Acc=68.20%, Induction=99.65%, OOD10=93.52%, OOD15=77.73%, OOD35=22.73%, OOD40=6.97%\n",
      "Epoch 43700: Loss=1.8303, Acc=68.16%, Induction=99.59%, OOD10=91.94%, OOD15=79.44%, OOD35=25.43%, OOD40=5.61%\n",
      "Epoch 43800: Loss=1.5432, Acc=73.06%, Induction=99.40%, OOD10=85.96%, OOD15=75.88%, OOD35=22.37%, OOD40=4.09%\n",
      "Epoch 43900: Loss=1.8595, Acc=67.64%, Induction=98.82%, OOD10=90.17%, OOD15=79.64%, OOD35=25.36%, OOD40=5.29%\n",
      "Epoch 44000: Loss=1.9109, Acc=66.53%, Induction=98.92%, OOD10=88.86%, OOD15=75.34%, OOD35=24.72%, OOD40=5.61%\n",
      "Epoch 44100: Loss=1.9194, Acc=66.57%, Induction=98.98%, OOD10=93.39%, OOD15=80.42%, OOD35=22.02%, OOD40=5.05%\n",
      "Epoch 44200: Loss=1.8526, Acc=67.84%, Induction=98.94%, OOD10=89.86%, OOD15=73.93%, OOD35=25.64%, OOD40=7.85%\n",
      "Epoch 44300: Loss=1.9124, Acc=66.85%, Induction=99.64%, OOD10=94.29%, OOD15=75.73%, OOD35=19.89%, OOD40=5.05%\n",
      "Epoch 44400: Loss=1.7644, Acc=69.22%, Induction=99.42%, OOD10=91.35%, OOD15=78.52%, OOD35=34.66%, OOD40=5.37%\n",
      "Epoch 44500: Loss=1.4766, Acc=74.60%, Induction=99.52%, OOD10=92.12%, OOD15=77.29%, OOD35=26.14%, OOD40=6.01%\n",
      "Epoch 44600: Loss=1.5526, Acc=72.94%, Induction=99.01%, OOD10=94.25%, OOD15=81.59%, OOD35=26.92%, OOD40=3.77%\n",
      "Epoch 44700: Loss=1.9099, Acc=66.69%, Induction=99.04%, OOD10=92.62%, OOD15=76.22%, OOD35=18.75%, OOD40=3.85%\n",
      "Epoch 44800: Loss=1.7724, Acc=69.34%, Induction=99.54%, OOD10=89.90%, OOD15=76.76%, OOD35=23.08%, OOD40=4.33%\n",
      "Epoch 44900: Loss=1.4716, Acc=74.21%, Induction=99.30%, OOD10=91.89%, OOD15=77.44%, OOD35=21.95%, OOD40=4.65%\n",
      "Epoch 45000: Loss=1.4430, Acc=74.84%, Induction=98.68%, OOD10=92.16%, OOD15=81.35%, OOD35=26.63%, OOD40=4.33%\n",
      "Epoch 45100: Loss=1.4426, Acc=75.00%, Induction=98.68%, OOD10=90.72%, OOD15=80.22%, OOD35=33.03%, OOD40=5.69%\n",
      "Epoch 45200: Loss=1.9860, Acc=65.51%, Induction=99.20%, OOD10=92.98%, OOD15=80.42%, OOD35=28.55%, OOD40=4.25%\n",
      "Epoch 45300: Loss=1.6346, Acc=71.48%, Induction=99.05%, OOD10=91.12%, OOD15=79.64%, OOD35=25.21%, OOD40=4.09%\n",
      "Epoch 45400: Loss=1.5414, Acc=73.30%, Induction=99.73%, OOD10=93.84%, OOD15=77.98%, OOD35=23.37%, OOD40=2.72%\n",
      "Epoch 45500: Loss=1.7731, Acc=69.03%, Induction=98.96%, OOD10=87.27%, OOD15=73.10%, OOD35=23.65%, OOD40=2.96%\n",
      "Epoch 45600: Loss=1.8276, Acc=68.08%, Induction=99.47%, OOD10=89.13%, OOD15=79.64%, OOD35=24.72%, OOD40=5.53%\n",
      "Epoch 45700: Loss=1.9258, Acc=66.93%, Induction=99.76%, OOD10=89.36%, OOD15=77.88%, OOD35=26.35%, OOD40=3.53%\n",
      "Epoch 45800: Loss=1.8396, Acc=67.92%, Induction=99.17%, OOD10=88.22%, OOD15=75.15%, OOD35=23.37%, OOD40=2.72%\n",
      "Epoch 45900: Loss=1.4243, Acc=75.08%, Induction=98.94%, OOD10=92.16%, OOD15=79.88%, OOD35=23.01%, OOD40=4.01%\n",
      "Epoch 46000: Loss=2.1025, Acc=63.49%, Induction=98.37%, OOD10=92.98%, OOD15=77.44%, OOD35=19.11%, OOD40=3.45%\n",
      "Epoch 46100: Loss=1.8464, Acc=67.80%, Induction=99.35%, OOD10=91.71%, OOD15=76.17%, OOD35=19.18%, OOD40=2.48%\n",
      "Epoch 46200: Loss=1.4435, Acc=75.00%, Induction=98.89%, OOD10=88.41%, OOD15=76.66%, OOD35=22.59%, OOD40=3.61%\n",
      "Epoch 46300: Loss=1.4688, Acc=74.76%, Induction=98.57%, OOD10=86.10%, OOD15=74.80%, OOD35=19.25%, OOD40=2.00%\n",
      "Epoch 46400: Loss=1.8235, Acc=68.24%, Induction=99.59%, OOD10=91.67%, OOD15=81.35%, OOD35=22.66%, OOD40=2.96%\n",
      "Epoch 46500: Loss=1.6157, Acc=71.88%, Induction=99.61%, OOD10=88.41%, OOD15=77.64%, OOD35=21.52%, OOD40=3.12%\n",
      "Epoch 46600: Loss=1.7571, Acc=69.34%, Induction=99.48%, OOD10=90.94%, OOD15=77.93%, OOD35=22.94%, OOD40=2.80%\n",
      "Epoch 46700: Loss=1.4768, Acc=73.97%, Induction=99.03%, OOD10=94.20%, OOD15=77.25%, OOD35=24.36%, OOD40=3.77%\n",
      "Epoch 46800: Loss=1.7824, Acc=68.99%, Induction=99.02%, OOD10=89.54%, OOD15=73.93%, OOD35=22.44%, OOD40=2.40%\n",
      "Epoch 46900: Loss=1.8379, Acc=67.80%, Induction=99.06%, OOD10=88.95%, OOD15=77.69%, OOD35=28.62%, OOD40=3.61%\n",
      "Epoch 47000: Loss=1.7568, Acc=69.50%, Induction=99.71%, OOD10=91.71%, OOD15=81.05%, OOD35=22.51%, OOD40=3.45%\n",
      "Epoch 47100: Loss=1.7722, Acc=69.07%, Induction=99.19%, OOD10=90.99%, OOD15=75.93%, OOD35=19.46%, OOD40=2.48%\n",
      "Epoch 47200: Loss=1.8446, Acc=68.04%, Induction=99.29%, OOD10=89.67%, OOD15=75.59%, OOD35=18.61%, OOD40=3.12%\n",
      "Epoch 47300: Loss=1.6250, Acc=71.72%, Induction=99.33%, OOD10=91.58%, OOD15=76.22%, OOD35=16.90%, OOD40=1.84%\n",
      "Epoch 47400: Loss=1.7712, Acc=68.79%, Induction=98.55%, OOD10=89.95%, OOD15=75.00%, OOD35=25.78%, OOD40=3.37%\n",
      "Epoch 47500: Loss=2.1354, Acc=63.21%, Induction=98.00%, OOD10=90.53%, OOD15=74.37%, OOD35=14.70%, OOD40=1.68%\n",
      "Epoch 47600: Loss=1.9784, Acc=65.59%, Induction=99.57%, OOD10=92.93%, OOD15=76.95%, OOD35=23.01%, OOD40=3.12%\n",
      "Epoch 47700: Loss=1.9082, Acc=66.81%, Induction=99.34%, OOD10=89.58%, OOD15=75.39%, OOD35=22.94%, OOD40=2.32%\n",
      "Epoch 47800: Loss=1.9722, Acc=65.47%, Induction=99.45%, OOD10=90.31%, OOD15=76.42%, OOD35=23.22%, OOD40=2.24%\n",
      "Epoch 47900: Loss=1.4773, Acc=74.45%, Induction=99.52%, OOD10=88.90%, OOD15=73.58%, OOD35=21.95%, OOD40=1.84%\n",
      "Epoch 48000: Loss=1.9076, Acc=66.57%, Induction=98.98%, OOD10=92.48%, OOD15=76.61%, OOD35=24.57%, OOD40=4.01%\n",
      "Epoch 48100: Loss=1.4751, Acc=74.29%, Induction=99.30%, OOD10=88.72%, OOD15=78.66%, OOD35=23.30%, OOD40=2.08%\n",
      "Epoch 48200: Loss=1.6850, Acc=70.89%, Induction=99.77%, OOD10=91.08%, OOD15=76.51%, OOD35=21.66%, OOD40=1.28%\n",
      "Epoch 48300: Loss=1.9117, Acc=66.42%, Induction=99.16%, OOD10=92.21%, OOD15=79.93%, OOD35=26.49%, OOD40=2.16%\n",
      "Epoch 48400: Loss=1.4292, Acc=75.44%, Induction=99.47%, OOD10=89.86%, OOD15=73.63%, OOD35=21.24%, OOD40=1.84%\n",
      "Epoch 48500: Loss=1.9155, Acc=66.65%, Induction=99.22%, OOD10=89.81%, OOD15=77.39%, OOD35=25.28%, OOD40=2.96%\n",
      "Epoch 48600: Loss=1.7198, Acc=70.02%, Induction=98.75%, OOD10=92.84%, OOD15=79.15%, OOD35=22.51%, OOD40=2.72%\n",
      "Epoch 48700: Loss=1.5534, Acc=73.22%, Induction=99.73%, OOD10=87.05%, OOD15=79.98%, OOD35=21.88%, OOD40=2.72%\n",
      "Epoch 48800: Loss=2.0735, Acc=64.16%, Induction=99.31%, OOD10=88.41%, OOD15=74.51%, OOD35=19.39%, OOD40=1.60%\n",
      "Epoch 48900: Loss=1.9972, Acc=65.27%, Induction=99.08%, OOD10=91.03%, OOD15=77.83%, OOD35=31.25%, OOD40=4.57%\n",
      "Epoch 49000: Loss=1.6946, Acc=70.61%, Induction=99.37%, OOD10=91.26%, OOD15=79.00%, OOD35=18.54%, OOD40=1.04%\n",
      "Epoch 49100: Loss=2.0595, Acc=64.28%, Induction=99.50%, OOD10=93.25%, OOD15=77.54%, OOD35=23.79%, OOD40=2.00%\n",
      "Epoch 49200: Loss=1.8547, Acc=67.92%, Induction=99.17%, OOD10=87.68%, OOD15=74.76%, OOD35=17.33%, OOD40=2.00%\n",
      "Epoch 49300: Loss=1.8322, Acc=68.04%, Induction=99.29%, OOD10=92.62%, OOD15=73.68%, OOD35=22.66%, OOD40=2.96%\n",
      "Epoch 49400: Loss=1.4808, Acc=74.41%, Induction=99.57%, OOD10=91.98%, OOD15=77.98%, OOD35=22.30%, OOD40=2.24%\n",
      "Epoch 49500: Loss=1.6936, Acc=70.33%, Induction=99.26%, OOD10=95.34%, OOD15=82.86%, OOD35=27.56%, OOD40=2.32%\n",
      "Epoch 49600: Loss=1.6886, Acc=70.29%, Induction=98.98%, OOD10=89.67%, OOD15=76.71%, OOD35=17.33%, OOD40=2.00%\n",
      "Epoch 49700: Loss=1.6842, Acc=70.85%, Induction=99.66%, OOD10=87.45%, OOD15=74.02%, OOD35=16.97%, OOD40=1.76%\n",
      "Epoch 49800: Loss=2.0732, Acc=63.88%, Induction=99.12%, OOD10=92.12%, OOD15=76.42%, OOD35=21.45%, OOD40=1.76%\n",
      "Epoch 49900: Loss=1.4085, Acc=75.87%, Induction=99.63%, OOD10=89.95%, OOD15=71.48%, OOD35=16.05%, OOD40=1.28%\n",
      "Epoch 50000: Loss=1.7793, Acc=68.95%, Induction=98.90%, OOD10=93.93%, OOD15=79.05%, OOD35=25.71%, OOD40=2.88%\n",
      "Epoch 50100: Loss=1.8454, Acc=67.96%, Induction=99.35%, OOD10=88.95%, OOD15=72.22%, OOD35=20.53%, OOD40=2.64%\n",
      "Epoch 50200: Loss=1.6221, Acc=71.80%, Induction=99.44%, OOD10=91.03%, OOD15=76.22%, OOD35=19.32%, OOD40=1.92%\n",
      "Epoch 50300: Loss=1.6829, Acc=70.65%, Induction=99.66%, OOD10=90.35%, OOD15=78.12%, OOD35=17.19%, OOD40=2.08%\n",
      "Epoch 50400: Loss=1.5515, Acc=72.94%, Induction=99.23%, OOD10=93.61%, OOD15=81.15%, OOD35=23.15%, OOD40=2.72%\n",
      "Epoch 50500: Loss=1.4420, Acc=75.00%, Induction=98.73%, OOD10=91.76%, OOD15=77.29%, OOD35=20.17%, OOD40=1.44%\n",
      "Epoch 50600: Loss=1.4795, Acc=74.33%, Induction=99.41%, OOD10=93.25%, OOD15=79.54%, OOD35=16.90%, OOD40=1.76%\n",
      "Epoch 50700: Loss=1.5469, Acc=73.26%, Induction=99.51%, OOD10=89.81%, OOD15=77.49%, OOD35=21.09%, OOD40=2.40%\n",
      "Epoch 50800: Loss=1.6882, Acc=70.41%, Induction=99.37%, OOD10=88.81%, OOD15=75.98%, OOD35=22.23%, OOD40=1.36%\n",
      "Epoch 50900: Loss=1.5452, Acc=73.06%, Induction=99.56%, OOD10=91.21%, OOD15=76.61%, OOD35=20.88%, OOD40=2.72%\n",
      "Epoch 51000: Loss=1.5323, Acc=73.42%, Induction=99.62%, OOD10=88.22%, OOD15=73.93%, OOD35=24.57%, OOD40=2.72%\n",
      "Epoch 51100: Loss=2.0060, Acc=65.07%, Induction=98.90%, OOD10=92.30%, OOD15=75.44%, OOD35=23.44%, OOD40=2.40%\n",
      "Epoch 51200: Loss=1.6888, Acc=70.49%, Induction=99.15%, OOD10=94.43%, OOD15=78.42%, OOD35=18.96%, OOD40=1.36%\n",
      "Epoch 51300: Loss=1.4675, Acc=74.45%, Induction=99.52%, OOD10=92.84%, OOD15=77.20%, OOD35=22.09%, OOD40=2.48%\n",
      "Epoch 51400: Loss=1.5597, Acc=73.06%, Induction=99.40%, OOD10=88.63%, OOD15=74.02%, OOD35=22.02%, OOD40=1.44%\n",
      "Epoch 51500: Loss=1.5406, Acc=73.18%, Induction=99.56%, OOD10=90.17%, OOD15=75.34%, OOD35=16.55%, OOD40=1.52%\n",
      "Epoch 51600: Loss=1.8446, Acc=67.84%, Induction=98.88%, OOD10=92.66%, OOD15=73.93%, OOD35=14.56%, OOD40=1.04%\n",
      "Epoch 51700: Loss=1.8566, Acc=67.56%, Induction=98.82%, OOD10=89.13%, OOD15=76.51%, OOD35=21.80%, OOD40=1.84%\n",
      "Epoch 51800: Loss=1.5375, Acc=73.18%, Induction=99.67%, OOD10=92.53%, OOD15=79.44%, OOD35=20.10%, OOD40=2.00%\n",
      "Epoch 51900: Loss=1.6313, Acc=71.52%, Induction=98.88%, OOD10=88.81%, OOD15=73.78%, OOD35=17.76%, OOD40=2.00%\n",
      "Epoch 52000: Loss=1.7650, Acc=69.26%, Induction=99.54%, OOD10=88.59%, OOD15=78.42%, OOD35=22.44%, OOD40=2.48%\n",
      "Epoch 52100: Loss=2.1141, Acc=63.05%, Induction=97.81%, OOD10=92.93%, OOD15=77.15%, OOD35=17.83%, OOD40=0.80%\n",
      "Epoch 52200: Loss=2.0928, Acc=63.81%, Induction=98.75%, OOD10=91.17%, OOD15=74.71%, OOD35=18.18%, OOD40=1.28%\n",
      "Epoch 52300: Loss=1.4306, Acc=75.04%, Induction=98.73%, OOD10=93.34%, OOD15=76.61%, OOD35=17.26%, OOD40=2.00%\n",
      "Epoch 52400: Loss=1.6933, Acc=70.33%, Induction=99.26%, OOD10=91.39%, OOD15=75.05%, OOD35=19.89%, OOD40=1.84%\n",
      "Epoch 52500: Loss=1.6872, Acc=70.65%, Induction=99.43%, OOD10=91.49%, OOD15=79.98%, OOD35=22.44%, OOD40=1.92%\n",
      "Epoch 52600: Loss=1.4855, Acc=73.97%, Induction=98.92%, OOD10=92.30%, OOD15=76.37%, OOD35=26.28%, OOD40=1.68%\n",
      "Epoch 52700: Loss=2.0796, Acc=63.65%, Induction=98.62%, OOD10=90.62%, OOD15=75.73%, OOD35=15.48%, OOD40=1.12%\n",
      "Epoch 52800: Loss=1.5493, Acc=73.10%, Induction=99.29%, OOD10=90.58%, OOD15=77.88%, OOD35=26.49%, OOD40=2.16%\n",
      "Epoch 52900: Loss=1.6226, Acc=71.76%, Induction=99.44%, OOD10=88.95%, OOD15=76.46%, OOD35=24.22%, OOD40=1.20%\n",
      "Epoch 53000: Loss=2.1189, Acc=63.57%, Induction=98.75%, OOD10=89.31%, OOD15=76.17%, OOD35=20.53%, OOD40=2.00%\n",
      "Epoch 53100: Loss=1.9032, Acc=66.73%, Induction=99.52%, OOD10=88.86%, OOD15=73.54%, OOD35=15.20%, OOD40=1.52%\n",
      "Epoch 53200: Loss=1.6939, Acc=70.57%, Induction=99.43%, OOD10=93.52%, OOD15=79.30%, OOD35=18.96%, OOD40=1.92%\n",
      "Epoch 53300: Loss=1.9820, Acc=65.51%, Induction=99.26%, OOD10=88.13%, OOD15=75.05%, OOD35=11.22%, OOD40=1.04%\n",
      "Epoch 53400: Loss=1.7759, Acc=69.15%, Induction=99.31%, OOD10=93.30%, OOD15=76.51%, OOD35=16.76%, OOD40=2.24%\n",
      "Epoch 53500: Loss=1.7696, Acc=69.22%, Induction=99.31%, OOD10=89.27%, OOD15=73.63%, OOD35=17.97%, OOD40=1.76%\n",
      "Epoch 53600: Loss=1.9753, Acc=65.47%, Induction=99.39%, OOD10=91.12%, OOD15=76.90%, OOD35=26.35%, OOD40=1.28%\n",
      "Epoch 53700: Loss=1.4673, Acc=74.33%, Induction=99.52%, OOD10=91.98%, OOD15=77.98%, OOD35=17.26%, OOD40=1.68%\n",
      "Epoch 53800: Loss=1.8406, Acc=68.20%, Induction=99.47%, OOD10=92.21%, OOD15=78.37%, OOD35=20.17%, OOD40=1.36%\n",
      "Epoch 53900: Loss=1.9350, Acc=66.42%, Induction=98.80%, OOD10=92.71%, OOD15=78.76%, OOD35=18.11%, OOD40=3.12%\n",
      "Epoch 54000: Loss=1.9127, Acc=66.69%, Induction=99.22%, OOD10=89.86%, OOD15=77.44%, OOD35=18.68%, OOD40=1.52%\n",
      "Epoch 54100: Loss=2.0848, Acc=64.04%, Induction=99.00%, OOD10=91.26%, OOD15=77.00%, OOD35=18.89%, OOD40=1.28%\n",
      "Epoch 54200: Loss=1.4281, Acc=75.24%, Induction=98.99%, OOD10=87.95%, OOD15=74.41%, OOD35=20.53%, OOD40=1.52%\n",
      "Epoch 54300: Loss=1.9840, Acc=65.70%, Induction=99.63%, OOD10=89.90%, OOD15=76.76%, OOD35=19.25%, OOD40=1.20%\n",
      "Epoch 54400: Loss=1.5380, Acc=73.14%, Induction=99.56%, OOD10=88.63%, OOD15=71.97%, OOD35=17.26%, OOD40=1.12%\n",
      "Epoch 54500: Loss=1.5473, Acc=73.10%, Induction=99.45%, OOD10=92.84%, OOD15=79.20%, OOD35=16.62%, OOD40=1.76%\n",
      "Epoch 54600: Loss=1.8921, Acc=66.97%, Induction=99.76%, OOD10=92.07%, OOD15=77.93%, OOD35=21.59%, OOD40=0.80%\n",
      "Epoch 54700: Loss=1.8373, Acc=68.08%, Induction=99.59%, OOD10=90.67%, OOD15=82.08%, OOD35=29.62%, OOD40=2.80%\n",
      "Epoch 54800: Loss=1.7619, Acc=69.30%, Induction=99.42%, OOD10=91.26%, OOD15=77.25%, OOD35=22.23%, OOD40=2.24%\n",
      "Epoch 54900: Loss=1.4292, Acc=75.20%, Induction=98.83%, OOD10=91.76%, OOD15=77.93%, OOD35=21.73%, OOD40=2.40%\n",
      "Epoch 55000: Loss=1.5459, Acc=73.14%, Induction=99.40%, OOD10=88.00%, OOD15=75.49%, OOD35=21.73%, OOD40=1.68%\n",
      "Epoch 55100: Loss=1.4803, Acc=74.17%, Induction=99.25%, OOD10=87.36%, OOD15=73.58%, OOD35=18.32%, OOD40=1.52%\n",
      "Epoch 55200: Loss=1.6897, Acc=70.77%, Induction=99.72%, OOD10=90.08%, OOD15=76.76%, OOD35=21.88%, OOD40=1.60%\n",
      "Epoch 55300: Loss=1.6864, Acc=70.81%, Induction=99.77%, OOD10=87.95%, OOD15=74.32%, OOD35=23.58%, OOD40=1.20%\n",
      "Epoch 55400: Loss=1.6099, Acc=71.99%, Induction=99.50%, OOD10=90.81%, OOD15=78.81%, OOD35=22.44%, OOD40=3.12%\n",
      "Epoch 55500: Loss=1.6163, Acc=71.56%, Induction=99.05%, OOD10=92.12%, OOD15=78.81%, OOD35=20.17%, OOD40=1.68%\n",
      "Epoch 55600: Loss=1.8301, Acc=68.20%, Induction=99.76%, OOD10=89.76%, OOD15=76.03%, OOD35=23.51%, OOD40=1.84%\n",
      "Epoch 55700: Loss=1.4446, Acc=75.12%, Induction=98.83%, OOD10=93.25%, OOD15=79.20%, OOD35=23.30%, OOD40=1.84%\n",
      "Epoch 55800: Loss=1.4609, Acc=74.60%, Induction=99.84%, OOD10=91.58%, OOD15=74.37%, OOD35=15.98%, OOD40=1.84%\n",
      "Epoch 55900: Loss=1.6810, Acc=70.77%, Induction=99.66%, OOD10=88.90%, OOD15=75.59%, OOD35=23.08%, OOD40=2.00%\n",
      "Epoch 56000: Loss=1.4768, Acc=74.60%, Induction=99.41%, OOD10=87.86%, OOD15=74.46%, OOD35=24.22%, OOD40=1.28%\n",
      "Epoch 56100: Loss=1.7668, Acc=69.34%, Induction=99.42%, OOD10=90.67%, OOD15=75.98%, OOD35=16.12%, OOD40=1.44%\n",
      "Epoch 56200: Loss=1.6205, Acc=71.72%, Induction=99.44%, OOD10=91.80%, OOD15=75.78%, OOD35=15.55%, OOD40=2.40%\n",
      "Epoch 56300: Loss=1.8425, Acc=67.96%, Induction=99.17%, OOD10=93.03%, OOD15=80.62%, OOD35=21.66%, OOD40=2.64%\n",
      "Epoch 56400: Loss=1.5429, Acc=73.14%, Induction=99.56%, OOD10=90.31%, OOD15=75.39%, OOD35=24.64%, OOD40=1.44%\n",
      "Epoch 56500: Loss=1.4998, Acc=74.53%, Induction=98.04%, OOD10=86.14%, OOD15=72.36%, OOD35=18.04%, OOD40=1.76%\n",
      "Epoch 56600: Loss=1.5448, Acc=73.22%, Induction=99.62%, OOD10=89.76%, OOD15=80.71%, OOD35=23.65%, OOD40=2.88%\n",
      "Epoch 56700: Loss=1.9053, Acc=66.85%, Induction=99.52%, OOD10=90.44%, OOD15=73.68%, OOD35=17.26%, OOD40=2.00%\n",
      "Epoch 56800: Loss=1.5453, Acc=73.22%, Induction=99.51%, OOD10=92.16%, OOD15=76.56%, OOD35=23.65%, OOD40=2.48%\n",
      "Epoch 56900: Loss=1.4763, Acc=74.05%, Induction=99.03%, OOD10=90.72%, OOD15=77.88%, OOD35=20.45%, OOD40=3.04%\n",
      "Epoch 57000: Loss=1.4962, Acc=73.81%, Induction=98.71%, OOD10=88.41%, OOD15=71.68%, OOD35=22.73%, OOD40=2.40%\n",
      "Epoch 57100: Loss=1.8362, Acc=67.88%, Induction=99.00%, OOD10=89.76%, OOD15=76.42%, OOD35=23.01%, OOD40=1.76%\n",
      "Epoch 57200: Loss=1.8281, Acc=68.16%, Induction=99.53%, OOD10=88.68%, OOD15=76.86%, OOD35=19.67%, OOD40=1.60%\n",
      "Epoch 57300: Loss=2.0885, Acc=63.88%, Induction=98.69%, OOD10=90.08%, OOD15=75.00%, OOD35=23.08%, OOD40=2.00%\n",
      "Epoch 57400: Loss=1.6290, Acc=71.91%, Induction=99.39%, OOD10=90.04%, OOD15=76.56%, OOD35=22.44%, OOD40=1.44%\n",
      "Epoch 57500: Loss=1.4236, Acc=75.20%, Induction=98.99%, OOD10=89.76%, OOD15=75.44%, OOD35=18.32%, OOD40=1.36%\n",
      "Epoch 57600: Loss=1.4795, Acc=74.41%, Induction=99.35%, OOD10=89.40%, OOD15=75.10%, OOD35=20.17%, OOD40=1.60%\n",
      "Epoch 57700: Loss=1.8502, Acc=67.76%, Induction=99.00%, OOD10=93.30%, OOD15=79.83%, OOD35=15.62%, OOD40=1.36%\n",
      "Epoch 57800: Loss=1.6081, Acc=71.88%, Induction=99.50%, OOD10=92.48%, OOD15=77.69%, OOD35=20.38%, OOD40=1.12%\n",
      "Epoch 57900: Loss=1.9113, Acc=66.61%, Induction=99.16%, OOD10=90.04%, OOD15=77.59%, OOD35=23.37%, OOD40=2.96%\n",
      "Epoch 58000: Loss=1.8282, Acc=68.04%, Induction=99.41%, OOD10=90.31%, OOD15=75.98%, OOD35=22.30%, OOD40=1.52%\n",
      "Epoch 58100: Loss=2.0951, Acc=63.88%, Induction=98.94%, OOD10=89.67%, OOD15=75.83%, OOD35=17.33%, OOD40=1.60%\n",
      "Epoch 58200: Loss=1.7561, Acc=69.30%, Induction=99.42%, OOD10=92.35%, OOD15=78.12%, OOD35=20.10%, OOD40=1.84%\n",
      "Epoch 58300: Loss=1.9023, Acc=66.89%, Induction=99.70%, OOD10=92.03%, OOD15=79.20%, OOD35=19.82%, OOD40=1.44%\n",
      "Epoch 58400: Loss=1.8217, Acc=68.04%, Induction=99.41%, OOD10=91.67%, OOD15=73.24%, OOD35=12.93%, OOD40=1.20%\n",
      "Epoch 58500: Loss=1.4425, Acc=74.76%, Induction=98.57%, OOD10=93.48%, OOD15=80.18%, OOD35=20.81%, OOD40=1.12%\n",
      "Epoch 58600: Loss=1.4678, Acc=74.53%, Induction=99.68%, OOD10=88.68%, OOD15=75.63%, OOD35=23.44%, OOD40=2.00%\n",
      "Epoch 58700: Loss=1.9800, Acc=65.51%, Induction=99.39%, OOD10=92.57%, OOD15=79.44%, OOD35=19.74%, OOD40=1.36%\n",
      "Epoch 58800: Loss=1.9292, Acc=66.50%, Induction=99.16%, OOD10=90.31%, OOD15=77.93%, OOD35=14.28%, OOD40=1.36%\n",
      "Epoch 58900: Loss=1.9698, Acc=65.70%, Induction=99.88%, OOD10=89.58%, OOD15=77.49%, OOD35=16.48%, OOD40=1.68%\n",
      "Epoch 59000: Loss=1.4306, Acc=75.24%, Induction=98.83%, OOD10=89.76%, OOD15=78.52%, OOD35=23.30%, OOD40=2.24%\n",
      "Epoch 59100: Loss=1.4870, Acc=74.21%, Induction=99.08%, OOD10=90.22%, OOD15=76.12%, OOD35=21.52%, OOD40=2.00%\n",
      "Epoch 59200: Loss=1.5537, Acc=72.90%, Induction=99.23%, OOD10=88.00%, OOD15=74.80%, OOD35=22.73%, OOD40=3.61%\n",
      "Epoch 59300: Loss=1.4359, Acc=74.88%, Induction=98.62%, OOD10=93.75%, OOD15=80.32%, OOD35=23.58%, OOD40=2.64%\n",
      "Epoch 59400: Loss=1.8427, Acc=67.96%, Induction=99.47%, OOD10=89.09%, OOD15=80.42%, OOD35=26.99%, OOD40=1.52%\n",
      "Epoch 59500: Loss=1.7504, Acc=69.38%, Induction=99.48%, OOD10=91.53%, OOD15=79.44%, OOD35=24.72%, OOD40=1.28%\n",
      "Epoch 59600: Loss=1.6912, Acc=70.53%, Induction=99.26%, OOD10=90.67%, OOD15=77.98%, OOD35=21.24%, OOD40=2.88%\n",
      "Epoch 59700: Loss=1.4738, Acc=74.29%, Induction=99.46%, OOD10=91.35%, OOD15=77.34%, OOD35=28.98%, OOD40=2.80%\n",
      "Epoch 59800: Loss=1.7050, Acc=70.37%, Induction=99.09%, OOD10=93.66%, OOD15=77.44%, OOD35=26.92%, OOD40=2.64%\n",
      "Epoch 59900: Loss=1.4132, Acc=75.51%, Induction=99.26%, OOD10=90.31%, OOD15=77.83%, OOD35=18.25%, OOD40=2.88%\n"
     ]
    }
   ],
   "source": [
    "# Training (continued from epoch 30000)\n",
    "epoch_offset = epochs\n",
    "epochs = 40000\n",
    "for epoch in range(epoch_offset, epoch_offset + epochs):\n",
    "\n",
    "    # Generate data using a varied prefix length\n",
    "    seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=SEQ_LEN)\n",
    "\n",
    "    # Move data to the specified device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    input_data = data[:, :-1]\n",
    "    target_data = data[:, 1:]\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model.forward(input_data)\n",
    "\n",
    "    # [b, s, vocab_size] -> [b*s, vocab_size]\n",
    "    input_to_ce = logits.view(-1, model.vocab_size)\n",
    "\n",
    "    # [b, vocab_size] -> [b*vocab_size], equivalent to flattening a 2D tensor into a 1D tensor\n",
    "    target_flat = target_data.reshape(-1)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(input_to_ce, target_flat)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    accuracy = (predictions == target_data).float().mean().item()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Induction accuracy (second half only where the pattern repeats)\n",
    "    induction_acc = (predictions[:, seq_len:] == target_data[:, seq_len:]).float().mean().item()\n",
    "    induction_accuracies.append(induction_acc)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Save metrics every 100 epochs (includes OOD seq_len=15 eval)\n",
    "    if epoch % 100 == 0:\n",
    "        training_history[\"epochs\"].append(epoch)\n",
    "        training_history[\"losses\"].append(loss.item())\n",
    "        training_history[\"accuracies\"].append(accuracy)\n",
    "        training_history[\"induction_accs\"].append(induction_acc)\n",
    "\n",
    "        # Evaluate out-of-distribution induction accuracy at multiple seq_lens\n",
    "        ood_accs = {}\n",
    "        with torch.no_grad():\n",
    "            for ood_len in OOD_SEQ_LENS:\n",
    "                _, ood_data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=ood_len)\n",
    "                ood_data = ood_data.to(device)\n",
    "                ood_logits = model(ood_data[:, :-1])\n",
    "                ood_predictions = ood_logits.argmax(dim=-1)\n",
    "                ood_targets = ood_data[:, 1:]\n",
    "                ood_acc = (ood_predictions[:, ood_len:] == ood_targets[:, ood_len:]).float().mean().item()\n",
    "                ood_accs[ood_len] = ood_acc\n",
    "                training_history[\"ood_induction_accs\"][ood_len].append(ood_acc)\n",
    "\n",
    "        # Save attention snapshots by running a dedicated forward pass with seq_len=20\n",
    "        with torch.no_grad():\n",
    "            _, snap_data = generate_rrt(num_samples=1, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=20)\n",
    "            snap_data = snap_data.to(device)\n",
    "            _ = model(snap_data[:, :-1])  # run forward to populate attn_weights\n",
    "            attention_patterns = []\n",
    "            for block in model.attention_blocks:\n",
    "                # attn_weights shape: [batch, n_heads, seq, seq] -> take first batch, detach\n",
    "                attention_patterns.append(block.attn_weights[0].detach().cpu())\n",
    "            # Stack to [n_layers, n_heads, seq, seq]\n",
    "            training_history[\"attention_snapshots\"][epoch] = torch.stack(attention_patterns)\n",
    "        \n",
    "        ood_str = \", \".join([f\"OOD{k}={v:.2%}\" for k, v in ood_accs.items()])\n",
    "        print(f\"Epoch {epoch}: Loss={loss.item():.4f}, Acc={accuracy:.2%}, Induction={induction_acc:.2%}, {ood_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters and training history\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = results_dir / f\"2L_varied_model_{epoch_offset + epochs}.pt\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "history_path = results_dir / \"2L_varied_training_history.pt\"\n",
    "torch.save(training_history, history_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
