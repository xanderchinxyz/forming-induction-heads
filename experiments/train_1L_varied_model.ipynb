{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bfcbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "from model import AttentionOnlyTransformer\n",
    "from generate_data import generate_rrt\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(67)\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "SEQ_LEN = (15, 35)\n",
    "MAX_CONTEXT_LEN = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e12a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: torch.Size([32, 80]), Prefix Length: 5\n",
      "First sequence sample (first 10 tokens):\n",
      "tensor([120, 167, 140,  36, 246, 120, 167, 140,  36, 246])\n"
     ]
    }
   ],
   "source": [
    "seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=5)\n",
    "\n",
    "print(f\"Dataset Shape: {data.shape}, Prefix Length: {seq_len}\")\n",
    "print(f\"First sequence sample (first 10 tokens):\\n{data[0, :10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82fea9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 49,152\n",
      "\n",
      "Model architecture:\n",
      "AttentionOnlyTransformer(\n",
      "  (token_embedding): Embedding(256, 64)\n",
      "  (attention_blocks): ModuleList(\n",
      "    (0): AttentionBlock(\n",
      "      (W_qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "      (W_o): Linear(in_features=64, out_features=64, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (unembed): Linear(in_features=64, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = AttentionOnlyTransformer(\n",
    "    vocab_size=256,\n",
    "    d_model=64,\n",
    "    n_layers=NUM_LAYERS,\n",
    "    n_heads=4,\n",
    "    max_context_len=MAX_CONTEXT_LEN,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nModel architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021916e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=5.8013, Acc=0.40%, Induction Acc=0.41%\n",
      "Epoch 100: Loss=5.6815, Acc=0.16%, Induction Acc=0.20%\n",
      "Epoch 200: Loss=5.5915, Acc=0.24%, Induction Acc=0.26%\n",
      "Epoch 300: Loss=5.5660, Acc=0.87%, Induction Acc=1.06%\n",
      "Epoch 400: Loss=5.5711, Acc=2.25%, Induction Acc=3.59%\n",
      "Epoch 500: Loss=5.5011, Acc=2.10%, Induction Acc=2.78%\n",
      "Epoch 600: Loss=5.4214, Acc=3.72%, Induction Acc=4.41%\n",
      "Epoch 700: Loss=5.4383, Acc=3.24%, Induction Acc=4.36%\n",
      "Epoch 800: Loss=5.4044, Acc=3.44%, Induction Acc=4.60%\n",
      "Epoch 900: Loss=5.4400, Acc=3.20%, Induction Acc=4.62%\n",
      "Epoch 1000: Loss=5.3835, Acc=3.72%, Induction Acc=4.89%\n",
      "Epoch 1100: Loss=5.2066, Acc=5.74%, Induction Acc=6.80%\n",
      "Epoch 1200: Loss=5.3661, Acc=3.96%, Induction Acc=5.64%\n",
      "Epoch 1300: Loss=5.2560, Acc=4.83%, Induction Acc=5.95%\n",
      "Epoch 1400: Loss=5.2703, Acc=4.35%, Induction Acc=5.31%\n",
      "Epoch 1500: Loss=5.3002, Acc=4.55%, Induction Acc=6.80%\n",
      "Epoch 1600: Loss=5.2740, Acc=4.91%, Induction Acc=6.73%\n",
      "Epoch 1700: Loss=5.2610, Acc=4.47%, Induction Acc=7.02%\n",
      "Epoch 1800: Loss=5.2489, Acc=4.98%, Induction Acc=7.72%\n",
      "Epoch 1900: Loss=5.3879, Acc=3.48%, Induction Acc=5.35%\n",
      "Epoch 2000: Loss=5.2162, Acc=4.11%, Induction Acc=6.19%\n",
      "Epoch 2100: Loss=5.0754, Acc=5.34%, Induction Acc=6.35%\n",
      "Epoch 2200: Loss=5.1936, Acc=5.50%, Induction Acc=8.31%\n",
      "Epoch 2300: Loss=5.3012, Acc=3.48%, Induction Acc=5.64%\n",
      "Epoch 2400: Loss=5.1970, Acc=4.47%, Induction Acc=6.19%\n",
      "Epoch 2500: Loss=5.1446, Acc=4.91%, Induction Acc=6.94%\n",
      "Epoch 2600: Loss=5.1612, Acc=5.78%, Induction Acc=8.56%\n",
      "Epoch 2700: Loss=5.0945, Acc=5.70%, Induction Acc=8.20%\n",
      "Epoch 2800: Loss=4.9469, Acc=6.01%, Induction Acc=7.29%\n",
      "Epoch 2900: Loss=5.1351, Acc=4.79%, Induction Acc=6.78%\n",
      "Epoch 3000: Loss=5.0035, Acc=5.38%, Induction Acc=6.60%\n",
      "Epoch 3100: Loss=5.2270, Acc=4.43%, Induction Acc=6.93%\n",
      "Epoch 3200: Loss=5.0647, Acc=5.97%, Induction Acc=8.95%\n",
      "Epoch 3300: Loss=5.0699, Acc=4.71%, Induction Acc=5.93%\n",
      "Epoch 3400: Loss=4.8004, Acc=7.00%, Induction Acc=8.58%\n",
      "Epoch 3500: Loss=5.0569, Acc=4.83%, Induction Acc=5.78%\n",
      "Epoch 3600: Loss=5.0278, Acc=6.21%, Induction Acc=9.63%\n",
      "Epoch 3700: Loss=5.0219, Acc=5.14%, Induction Acc=6.46%\n",
      "Epoch 3800: Loss=4.8083, Acc=6.80%, Induction Acc=8.30%\n",
      "Epoch 3900: Loss=5.0259, Acc=6.65%, Induction Acc=10.44%\n",
      "Epoch 4000: Loss=4.9866, Acc=6.45%, Induction Acc=10.14%\n",
      "Epoch 4100: Loss=4.9706, Acc=5.02%, Induction Acc=6.05%\n",
      "Epoch 4200: Loss=5.0026, Acc=3.88%, Induction Acc=4.46%\n",
      "Epoch 4300: Loss=4.9975, Acc=5.58%, Induction Acc=8.08%\n",
      "Epoch 4400: Loss=4.9916, Acc=6.69%, Induction Acc=9.62%\n",
      "Epoch 4500: Loss=5.3245, Acc=2.53%, Induction Acc=4.10%\n",
      "Epoch 4600: Loss=4.9467, Acc=6.88%, Induction Acc=10.62%\n",
      "Epoch 4700: Loss=4.8614, Acc=5.50%, Induction Acc=6.75%\n",
      "Epoch 4800: Loss=4.9944, Acc=5.66%, Induction Acc=8.61%\n",
      "Epoch 4900: Loss=4.9423, Acc=5.62%, Induction Acc=8.41%\n",
      "Epoch 5000: Loss=4.9851, Acc=5.85%, Induction Acc=9.05%\n",
      "Epoch 5100: Loss=5.0070, Acc=4.67%, Induction Acc=6.31%\n",
      "Epoch 5200: Loss=4.9717, Acc=4.35%, Induction Acc=5.52%\n",
      "Epoch 5300: Loss=4.9429, Acc=5.18%, Induction Acc=6.68%\n",
      "Epoch 5400: Loss=4.8580, Acc=4.35%, Induction Acc=5.34%\n",
      "Epoch 5500: Loss=4.8253, Acc=5.58%, Induction Acc=6.75%\n",
      "Epoch 5600: Loss=4.9156, Acc=4.39%, Induction Acc=5.28%\n",
      "Epoch 5700: Loss=5.0088, Acc=4.19%, Induction Acc=4.74%\n",
      "Epoch 5800: Loss=5.0065, Acc=4.19%, Induction Acc=5.79%\n",
      "Epoch 5900: Loss=4.9015, Acc=6.21%, Induction Acc=9.19%\n",
      "Epoch 6000: Loss=4.9550, Acc=6.65%, Induction Acc=11.04%\n",
      "Epoch 6100: Loss=4.9393, Acc=5.18%, Induction Acc=6.93%\n",
      "Epoch 6200: Loss=4.9578, Acc=4.67%, Induction Acc=5.83%\n",
      "Epoch 6300: Loss=4.9355, Acc=3.96%, Induction Acc=5.12%\n",
      "Epoch 6400: Loss=4.9521, Acc=4.71%, Induction Acc=5.33%\n",
      "Epoch 6500: Loss=4.7289, Acc=5.78%, Induction Acc=7.11%\n",
      "Epoch 6600: Loss=4.9726, Acc=4.75%, Induction Acc=6.66%\n",
      "Epoch 6700: Loss=4.5964, Acc=7.16%, Induction Acc=8.45%\n",
      "Epoch 6800: Loss=4.5798, Acc=8.66%, Induction Acc=10.76%\n",
      "Epoch 6900: Loss=4.5567, Acc=7.79%, Induction Acc=9.18%\n",
      "Epoch 7000: Loss=4.8592, Acc=5.34%, Induction Acc=7.47%\n",
      "Epoch 7100: Loss=4.9277, Acc=4.55%, Induction Acc=5.43%\n",
      "Epoch 7200: Loss=4.9742, Acc=4.47%, Induction Acc=5.31%\n",
      "Epoch 7300: Loss=4.9571, Acc=3.84%, Induction Acc=5.17%\n",
      "Epoch 7400: Loss=4.9752, Acc=4.27%, Induction Acc=5.90%\n",
      "Epoch 7500: Loss=5.0389, Acc=3.20%, Induction Acc=3.95%\n",
      "Epoch 7600: Loss=4.8661, Acc=5.81%, Induction Acc=8.70%\n",
      "Epoch 7700: Loss=5.0565, Acc=3.09%, Induction Acc=3.86%\n",
      "Epoch 7800: Loss=4.9516, Acc=4.23%, Induction Acc=5.51%\n",
      "Epoch 7900: Loss=4.8218, Acc=5.46%, Induction Acc=6.51%\n",
      "Epoch 8000: Loss=4.9516, Acc=5.50%, Induction Acc=8.76%\n",
      "Epoch 8100: Loss=4.9399, Acc=4.00%, Induction Acc=5.28%\n",
      "Epoch 8200: Loss=4.5463, Acc=7.08%, Induction Acc=8.54%\n",
      "Epoch 8300: Loss=4.8093, Acc=5.54%, Induction Acc=7.44%\n",
      "Epoch 8400: Loss=5.2473, Acc=2.57%, Induction Acc=4.17%\n",
      "Epoch 8500: Loss=5.0314, Acc=3.88%, Induction Acc=5.34%\n",
      "Epoch 8600: Loss=4.8568, Acc=5.62%, Induction Acc=8.17%\n",
      "Epoch 8700: Loss=4.5109, Acc=7.83%, Induction Acc=9.62%\n",
      "Epoch 8800: Loss=4.9024, Acc=4.15%, Induction Acc=5.58%\n",
      "Epoch 8900: Loss=5.2747, Acc=2.61%, Induction Acc=4.24%\n",
      "Epoch 9000: Loss=4.7230, Acc=8.62%, Induction Acc=13.39%\n",
      "Epoch 9100: Loss=5.0586, Acc=4.47%, Induction Acc=7.05%\n",
      "Epoch 9200: Loss=4.8379, Acc=6.72%, Induction Acc=10.37%\n",
      "Epoch 9300: Loss=4.8783, Acc=5.66%, Induction Acc=9.44%\n",
      "Epoch 9400: Loss=4.7258, Acc=6.69%, Induction Acc=9.04%\n",
      "Epoch 9500: Loss=4.7457, Acc=7.83%, Induction Acc=12.63%\n",
      "Epoch 9600: Loss=4.4641, Acc=7.59%, Induction Acc=9.13%\n",
      "Epoch 9700: Loss=5.2102, Acc=2.33%, Induction Acc=3.89%\n",
      "Epoch 9800: Loss=4.9059, Acc=4.87%, Induction Acc=5.78%\n",
      "Epoch 9900: Loss=4.8099, Acc=5.85%, Induction Acc=8.86%\n",
      "Epoch 10000: Loss=5.2651, Acc=2.45%, Induction Acc=3.61%\n",
      "Epoch 10100: Loss=4.7992, Acc=5.54%, Induction Acc=7.59%\n",
      "Epoch 10200: Loss=4.9373, Acc=5.18%, Induction Acc=8.22%\n",
      "Epoch 10300: Loss=4.9540, Acc=4.07%, Induction Acc=5.47%\n",
      "Epoch 10400: Loss=4.2824, Acc=11.04%, Induction Acc=13.24%\n",
      "Epoch 10500: Loss=4.8574, Acc=6.21%, Induction Acc=7.71%\n",
      "Epoch 10600: Loss=4.8425, Acc=4.91%, Induction Acc=6.97%\n",
      "Epoch 10700: Loss=4.5642, Acc=7.67%, Induction Acc=9.02%\n",
      "Epoch 10800: Loss=4.5936, Acc=11.04%, Induction Acc=17.95%\n",
      "Epoch 10900: Loss=4.6494, Acc=9.34%, Induction Acc=14.80%\n",
      "Epoch 11000: Loss=4.6913, Acc=8.74%, Induction Acc=13.71%\n",
      "Epoch 11100: Loss=5.1050, Acc=3.84%, Induction Acc=6.05%\n",
      "Epoch 11200: Loss=4.8065, Acc=5.85%, Induction Acc=7.38%\n",
      "Epoch 11300: Loss=4.4650, Acc=6.41%, Induction Acc=7.67%\n",
      "Epoch 11400: Loss=4.8163, Acc=4.87%, Induction Acc=6.53%\n",
      "Epoch 11500: Loss=4.8776, Acc=3.88%, Induction Acc=5.50%\n",
      "Epoch 11600: Loss=4.9537, Acc=4.07%, Induction Acc=5.51%\n",
      "Epoch 11700: Loss=4.7415, Acc=5.54%, Induction Acc=7.35%\n",
      "Epoch 11800: Loss=4.9479, Acc=3.40%, Induction Acc=4.63%\n",
      "Epoch 11900: Loss=5.3787, Acc=2.69%, Induction Acc=4.37%\n",
      "Epoch 12000: Loss=5.3875, Acc=1.58%, Induction Acc=2.50%\n",
      "Epoch 12100: Loss=4.6462, Acc=9.53%, Induction Acc=15.17%\n",
      "Epoch 12200: Loss=4.9009, Acc=4.98%, Induction Acc=8.15%\n",
      "Epoch 12300: Loss=4.8553, Acc=4.75%, Induction Acc=7.05%\n",
      "Epoch 12400: Loss=4.8818, Acc=5.02%, Induction Acc=7.15%\n",
      "Epoch 12500: Loss=4.6263, Acc=6.96%, Induction Acc=8.95%\n",
      "Epoch 12600: Loss=5.4488, Acc=1.46%, Induction Acc=2.22%\n",
      "Epoch 12700: Loss=4.2552, Acc=12.18%, Induction Acc=14.78%\n",
      "Epoch 12800: Loss=4.7942, Acc=5.02%, Induction Acc=7.00%\n",
      "Epoch 12900: Loss=4.7526, Acc=6.09%, Induction Acc=8.28%\n",
      "Epoch 13000: Loss=4.2805, Acc=10.76%, Induction Acc=13.19%\n",
      "Epoch 13100: Loss=4.8741, Acc=3.52%, Induction Acc=4.58%\n",
      "Epoch 13200: Loss=4.9303, Acc=3.32%, Induction Acc=4.51%\n",
      "Epoch 13300: Loss=4.8892, Acc=4.15%, Induction Acc=5.45%\n",
      "Epoch 13400: Loss=4.7671, Acc=4.51%, Induction Acc=6.02%\n",
      "Epoch 13500: Loss=4.6637, Acc=5.66%, Induction Acc=7.38%\n",
      "Epoch 13600: Loss=4.7016, Acc=4.98%, Induction Acc=6.47%\n",
      "Epoch 13700: Loss=4.5695, Acc=10.48%, Induction Acc=17.53%\n",
      "Epoch 13800: Loss=4.3501, Acc=10.21%, Induction Acc=11.95%\n",
      "Epoch 13900: Loss=4.7939, Acc=4.94%, Induction Acc=6.47%\n",
      "Epoch 14000: Loss=4.8682, Acc=4.19%, Induction Acc=5.46%\n",
      "Epoch 14100: Loss=5.1705, Acc=2.53%, Induction Acc=4.17%\n",
      "Epoch 14200: Loss=4.9179, Acc=3.96%, Induction Acc=5.54%\n",
      "Epoch 14300: Loss=4.9492, Acc=3.32%, Induction Acc=4.50%\n",
      "Epoch 14400: Loss=4.6227, Acc=8.74%, Induction Acc=13.39%\n",
      "Epoch 14500: Loss=4.5651, Acc=10.72%, Induction Acc=17.82%\n",
      "Epoch 14600: Loss=4.6723, Acc=6.84%, Induction Acc=8.99%\n",
      "Epoch 14700: Loss=4.8564, Acc=4.00%, Induction Acc=5.17%\n",
      "Epoch 14800: Loss=4.7679, Acc=5.26%, Induction Acc=6.80%\n",
      "Epoch 14900: Loss=4.8740, Acc=3.56%, Induction Acc=4.33%\n",
      "Epoch 15000: Loss=5.1662, Acc=2.41%, Induction Acc=3.75%\n",
      "Epoch 15100: Loss=4.7731, Acc=5.42%, Induction Acc=8.03%\n",
      "Epoch 15200: Loss=4.6591, Acc=6.13%, Induction Acc=7.07%\n",
      "Epoch 15300: Loss=4.8827, Acc=4.35%, Induction Acc=6.56%\n",
      "Epoch 15400: Loss=5.2741, Acc=2.77%, Induction Acc=4.24%\n",
      "Epoch 15500: Loss=4.9632, Acc=2.85%, Induction Acc=3.92%\n",
      "Epoch 15600: Loss=4.5480, Acc=9.93%, Induction Acc=15.43%\n",
      "Epoch 15700: Loss=4.8183, Acc=4.83%, Induction Acc=6.36%\n",
      "Epoch 15800: Loss=4.6674, Acc=8.31%, Induction Acc=13.76%\n",
      "Epoch 15900: Loss=4.6064, Acc=10.01%, Induction Acc=16.42%\n",
      "Epoch 16000: Loss=4.9586, Acc=3.60%, Induction Acc=5.76%\n",
      "Epoch 16100: Loss=4.5042, Acc=10.96%, Induction Acc=17.64%\n",
      "Epoch 16200: Loss=4.7388, Acc=6.49%, Induction Acc=7.43%\n",
      "Epoch 16300: Loss=4.7295, Acc=4.87%, Induction Acc=6.09%\n",
      "Epoch 16400: Loss=4.9473, Acc=2.81%, Induction Acc=3.75%\n",
      "Epoch 16500: Loss=4.4807, Acc=8.62%, Induction Acc=10.03%\n",
      "Epoch 16600: Loss=4.6981, Acc=6.21%, Induction Acc=7.89%\n",
      "Epoch 16700: Loss=4.8178, Acc=4.55%, Induction Acc=5.92%\n",
      "Epoch 16800: Loss=5.2155, Acc=1.98%, Induction Acc=3.12%\n",
      "Epoch 16900: Loss=4.6719, Acc=6.69%, Induction Acc=10.06%\n",
      "Epoch 17000: Loss=4.4900, Acc=10.32%, Induction Acc=16.01%\n",
      "Epoch 17100: Loss=4.6510, Acc=7.44%, Induction Acc=11.00%\n",
      "Epoch 17200: Loss=4.8896, Acc=3.56%, Induction Acc=4.89%\n",
      "Epoch 17300: Loss=4.8266, Acc=4.31%, Induction Acc=6.25%\n",
      "Epoch 17400: Loss=4.9191, Acc=3.48%, Induction Acc=4.78%\n",
      "Epoch 17500: Loss=4.8757, Acc=3.96%, Induction Acc=5.06%\n",
      "Epoch 17600: Loss=4.6552, Acc=6.13%, Induction Acc=7.95%\n",
      "Epoch 17700: Loss=4.7558, Acc=5.54%, Induction Acc=6.40%\n",
      "Epoch 17800: Loss=5.0856, Acc=2.41%, Induction Acc=3.89%\n",
      "Epoch 17900: Loss=4.8963, Acc=4.00%, Induction Acc=5.66%\n",
      "Epoch 18000: Loss=4.9403, Acc=3.60%, Induction Acc=4.72%\n",
      "Epoch 18100: Loss=5.1573, Acc=2.37%, Induction Acc=3.75%\n",
      "Epoch 18200: Loss=4.3264, Acc=7.63%, Induction Acc=9.03%\n",
      "Epoch 18300: Loss=5.2366, Acc=2.41%, Induction Acc=3.89%\n",
      "Epoch 18400: Loss=4.3193, Acc=8.94%, Induction Acc=10.45%\n",
      "Epoch 18500: Loss=4.1452, Acc=11.63%, Induction Acc=13.84%\n",
      "Epoch 18600: Loss=4.8551, Acc=4.75%, Induction Acc=5.83%\n",
      "Epoch 18700: Loss=4.3555, Acc=7.52%, Induction Acc=9.08%\n",
      "Epoch 18800: Loss=4.7693, Acc=5.10%, Induction Acc=7.45%\n",
      "Epoch 18900: Loss=4.7027, Acc=5.18%, Induction Acc=6.86%\n",
      "Epoch 19000: Loss=4.9586, Acc=3.16%, Induction Acc=4.11%\n",
      "Epoch 19100: Loss=5.0537, Acc=3.92%, Induction Acc=6.11%\n",
      "Epoch 19200: Loss=4.3316, Acc=10.96%, Induction Acc=12.70%\n",
      "Epoch 19300: Loss=4.7456, Acc=3.92%, Induction Acc=4.93%\n",
      "Epoch 19400: Loss=4.4805, Acc=10.80%, Induction Acc=17.12%\n",
      "Epoch 19500: Loss=4.9793, Acc=2.93%, Induction Acc=4.02%\n",
      "Epoch 19600: Loss=4.7956, Acc=5.70%, Induction Acc=8.21%\n",
      "Epoch 19700: Loss=4.7181, Acc=4.75%, Induction Acc=6.25%\n",
      "Epoch 19800: Loss=4.8786, Acc=3.56%, Induction Acc=4.72%\n",
      "Epoch 19900: Loss=4.8345, Acc=4.75%, Induction Acc=6.60%\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,           # Relatively high for small models\n",
    "    betas=(0.9, 0.98), # Standard 'Transformer' betas\n",
    "    weight_decay=0.01  # Helps prevent the 'junk drawer' neuron problem\n",
    ")\n",
    "\n",
    "epochs = 20000\n",
    "losses = []\n",
    "accuracies = []\n",
    "induction_accuracies = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Generate data using a fixed prefix length\n",
    "    seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=SEQ_LEN)\n",
    "\n",
    "    # Move data to the specified device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    input_data = data[:, :-1]\n",
    "    target_data = data[:, 1:]\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model.forward(input_data)\n",
    "\n",
    "    # [b, s, vocab_size] -> [b*s, vocab_size]\n",
    "    input_to_ce = logits.view(-1, model.vocab_size)\n",
    "\n",
    "    # [b, vocab_size] -> [b*vocab_size], equivalent to flattening a 2D tensor into a 1D tensor\n",
    "    target_flat = target_data.reshape(-1)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(input_to_ce, target_flat)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    accuracy = (predictions == target_data).float().mean().item()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Induction accuracy (second half only where the pattern repeats)\n",
    "    induction_acc = (predictions[:, seq_len:] == target_data[:, seq_len:]).float().mean().item()\n",
    "    induction_accuracies.append(induction_acc)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Print every N epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={loss.item():.4f}, Acc={accuracy:.2%}, Induction Acc={induction_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44656cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = results_dir / \"1L_varied_model.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
