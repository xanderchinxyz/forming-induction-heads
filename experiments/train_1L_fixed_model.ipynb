{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfcbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from model import AttentionOnlyTransformer\n",
    "from generate_data import generate_rrt\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(67)\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "SEQ_LEN = 20\n",
    "MAX_CONTEXT_LEN = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e12a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: torch.Size([32, 80]), Prefix Length: 5\n",
      "First sequence sample (first 10 tokens):\n",
      "tensor([120, 167, 140,  36, 246, 120, 167, 140,  36, 246])\n"
     ]
    }
   ],
   "source": [
    "seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=5)\n",
    "\n",
    "print(f\"Dataset Shape: {data.shape}, Prefix Length: {seq_len}\")\n",
    "print(f\"First sequence sample (first 10 tokens):\\n{data[0, :10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fea9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 49,152\n",
      "\n",
      "Model architecture:\n",
      "AttentionOnlyTransformer(\n",
      "  (token_embedding): Embedding(256, 64)\n",
      "  (attention_blocks): ModuleList(\n",
      "    (0): AttentionBlock(\n",
      "      (W_qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "      (W_o): Linear(in_features=64, out_features=64, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (unembed): Linear(in_features=64, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = AttentionOnlyTransformer(\n",
    "    vocab_size=256,\n",
    "    d_model=64,\n",
    "    n_layers=NUM_LAYERS,\n",
    "    n_heads=4,\n",
    "    max_context_len=MAX_CONTEXT_LEN,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nModel architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021916e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=5.8025, Acc=0.28%, Induction Acc=0.21%\n",
      "Epoch 10: Loss=5.7272, Acc=0.51%, Induction Acc=0.42%\n",
      "Epoch 20: Loss=5.7244, Acc=0.36%, Induction Acc=0.37%\n",
      "Epoch 30: Loss=5.7267, Acc=0.32%, Induction Acc=0.32%\n",
      "Epoch 40: Loss=5.7141, Acc=0.36%, Induction Acc=0.32%\n",
      "Epoch 50: Loss=5.6760, Acc=0.16%, Induction Acc=0.11%\n",
      "Epoch 60: Loss=5.6841, Acc=0.40%, Induction Acc=0.48%\n",
      "Epoch 70: Loss=5.6981, Acc=0.36%, Induction Acc=0.37%\n",
      "Epoch 80: Loss=5.6974, Acc=0.28%, Induction Acc=0.21%\n",
      "Epoch 90: Loss=5.6770, Acc=0.36%, Induction Acc=0.42%\n",
      "Epoch 100: Loss=5.6830, Acc=0.47%, Induction Acc=0.58%\n",
      "Epoch 110: Loss=5.6762, Acc=0.47%, Induction Acc=0.37%\n",
      "Epoch 120: Loss=5.6440, Acc=0.20%, Induction Acc=0.16%\n",
      "Epoch 130: Loss=5.6755, Acc=0.28%, Induction Acc=0.37%\n",
      "Epoch 140: Loss=5.6516, Acc=0.28%, Induction Acc=0.32%\n",
      "Epoch 150: Loss=5.6224, Acc=0.36%, Induction Acc=0.32%\n",
      "Epoch 160: Loss=5.6384, Acc=0.24%, Induction Acc=0.16%\n",
      "Epoch 170: Loss=5.6412, Acc=0.24%, Induction Acc=0.16%\n",
      "Epoch 180: Loss=5.6205, Acc=0.28%, Induction Acc=0.37%\n",
      "Epoch 190: Loss=5.6210, Acc=0.83%, Induction Acc=0.90%\n",
      "Epoch 200: Loss=5.6134, Acc=0.24%, Induction Acc=0.26%\n",
      "Epoch 210: Loss=5.5985, Acc=0.83%, Induction Acc=0.95%\n",
      "Epoch 220: Loss=5.6060, Acc=0.67%, Induction Acc=0.79%\n",
      "Epoch 230: Loss=5.5665, Acc=1.11%, Induction Acc=1.38%\n",
      "Epoch 240: Loss=5.5713, Acc=1.07%, Induction Acc=1.22%\n",
      "Epoch 250: Loss=5.5444, Acc=1.27%, Induction Acc=1.54%\n",
      "Epoch 260: Loss=5.5741, Acc=0.87%, Induction Acc=1.06%\n",
      "Epoch 270: Loss=5.5421, Acc=1.54%, Induction Acc=2.01%\n",
      "Epoch 280: Loss=5.5853, Acc=1.58%, Induction Acc=1.96%\n",
      "Epoch 290: Loss=5.5361, Acc=2.25%, Induction Acc=2.91%\n",
      "Epoch 300: Loss=5.5066, Acc=2.25%, Induction Acc=2.97%\n",
      "Epoch 310: Loss=5.5154, Acc=2.57%, Induction Acc=3.12%\n",
      "Epoch 320: Loss=5.5104, Acc=2.06%, Induction Acc=2.65%\n",
      "Epoch 330: Loss=5.5389, Acc=2.37%, Induction Acc=3.12%\n",
      "Epoch 340: Loss=5.5006, Acc=3.20%, Induction Acc=4.03%\n",
      "Epoch 350: Loss=5.5199, Acc=2.77%, Induction Acc=3.50%\n",
      "Epoch 360: Loss=5.4913, Acc=3.20%, Induction Acc=3.92%\n",
      "Epoch 370: Loss=5.4938, Acc=3.48%, Induction Acc=4.56%\n",
      "Epoch 380: Loss=5.4847, Acc=3.56%, Induction Acc=4.61%\n",
      "Epoch 390: Loss=5.4884, Acc=3.60%, Induction Acc=4.66%\n",
      "Epoch 400: Loss=5.4785, Acc=3.72%, Induction Acc=4.71%\n",
      "Epoch 410: Loss=5.4669, Acc=3.68%, Induction Acc=4.87%\n",
      "Epoch 420: Loss=5.4701, Acc=4.27%, Induction Acc=5.46%\n",
      "Epoch 430: Loss=5.4138, Acc=4.79%, Induction Acc=6.20%\n",
      "Epoch 440: Loss=5.3974, Acc=4.55%, Induction Acc=5.77%\n",
      "Epoch 450: Loss=5.4269, Acc=3.84%, Induction Acc=4.98%\n",
      "Epoch 460: Loss=5.4302, Acc=4.83%, Induction Acc=6.20%\n",
      "Epoch 470: Loss=5.4207, Acc=4.35%, Induction Acc=5.46%\n",
      "Epoch 480: Loss=5.4307, Acc=5.02%, Induction Acc=6.46%\n",
      "Epoch 490: Loss=5.3814, Acc=4.79%, Induction Acc=6.20%\n",
      "Epoch 500: Loss=5.3764, Acc=5.78%, Induction Acc=7.52%\n",
      "Epoch 510: Loss=5.3602, Acc=5.22%, Induction Acc=6.78%\n",
      "Epoch 520: Loss=5.3740, Acc=4.83%, Induction Acc=6.20%\n",
      "Epoch 530: Loss=5.3668, Acc=5.34%, Induction Acc=6.94%\n",
      "Epoch 540: Loss=5.3844, Acc=4.91%, Induction Acc=6.41%\n",
      "Epoch 550: Loss=5.3177, Acc=5.62%, Induction Acc=7.04%\n",
      "Epoch 560: Loss=5.3675, Acc=5.38%, Induction Acc=6.99%\n",
      "Epoch 570: Loss=5.3163, Acc=6.53%, Induction Acc=8.53%\n",
      "Epoch 580: Loss=5.3064, Acc=6.17%, Induction Acc=7.84%\n",
      "Epoch 590: Loss=5.2799, Acc=6.69%, Induction Acc=8.42%\n",
      "Epoch 600: Loss=5.3309, Acc=6.17%, Induction Acc=8.16%\n",
      "Epoch 610: Loss=5.3250, Acc=6.17%, Induction Acc=7.94%\n",
      "Epoch 620: Loss=5.2977, Acc=6.53%, Induction Acc=8.47%\n",
      "Epoch 630: Loss=5.2857, Acc=6.92%, Induction Acc=9.06%\n",
      "Epoch 640: Loss=5.3096, Acc=5.85%, Induction Acc=7.57%\n",
      "Epoch 650: Loss=5.2622, Acc=6.45%, Induction Acc=8.53%\n",
      "Epoch 660: Loss=5.2138, Acc=7.71%, Induction Acc=10.06%\n",
      "Epoch 670: Loss=5.2401, Acc=7.16%, Induction Acc=9.32%\n",
      "Epoch 680: Loss=5.2286, Acc=7.40%, Induction Acc=9.59%\n",
      "Epoch 690: Loss=5.2294, Acc=7.71%, Induction Acc=9.85%\n",
      "Epoch 700: Loss=5.2452, Acc=6.37%, Induction Acc=8.32%\n",
      "Epoch 710: Loss=5.2510, Acc=6.88%, Induction Acc=8.85%\n",
      "Epoch 720: Loss=5.2380, Acc=7.28%, Induction Acc=9.27%\n",
      "Epoch 730: Loss=5.1881, Acc=8.31%, Induction Acc=10.96%\n",
      "Epoch 740: Loss=5.1766, Acc=6.69%, Induction Acc=8.58%\n",
      "Epoch 750: Loss=5.1327, Acc=9.06%, Induction Acc=11.97%\n",
      "Epoch 760: Loss=5.1435, Acc=7.95%, Induction Acc=10.49%\n",
      "Epoch 770: Loss=5.1164, Acc=9.45%, Induction Acc=11.86%\n",
      "Epoch 780: Loss=5.0675, Acc=8.94%, Induction Acc=11.60%\n",
      "Epoch 790: Loss=5.1537, Acc=7.95%, Induction Acc=10.38%\n",
      "Epoch 800: Loss=5.1360, Acc=9.22%, Induction Acc=11.92%\n",
      "Epoch 810: Loss=5.0829, Acc=10.28%, Induction Acc=13.61%\n",
      "Epoch 820: Loss=5.0635, Acc=10.13%, Induction Acc=13.08%\n",
      "Epoch 830: Loss=5.0471, Acc=9.41%, Induction Acc=12.45%\n",
      "Epoch 840: Loss=5.0504, Acc=9.97%, Induction Acc=12.92%\n",
      "Epoch 850: Loss=5.0852, Acc=9.22%, Induction Acc=11.97%\n",
      "Epoch 860: Loss=5.0351, Acc=9.65%, Induction Acc=12.45%\n",
      "Epoch 870: Loss=4.9931, Acc=11.35%, Induction Acc=14.99%\n",
      "Epoch 880: Loss=4.9715, Acc=11.59%, Induction Acc=15.10%\n",
      "Epoch 890: Loss=4.9784, Acc=10.92%, Induction Acc=14.30%\n",
      "Epoch 900: Loss=4.9585, Acc=10.76%, Induction Acc=14.04%\n",
      "Epoch 910: Loss=4.9639, Acc=11.39%, Induction Acc=14.94%\n",
      "Epoch 920: Loss=4.9902, Acc=11.12%, Induction Acc=14.67%\n",
      "Epoch 930: Loss=4.9349, Acc=10.52%, Induction Acc=13.61%\n",
      "Epoch 940: Loss=4.9214, Acc=10.60%, Induction Acc=13.72%\n",
      "Epoch 950: Loss=4.8925, Acc=12.10%, Induction Acc=15.84%\n",
      "Epoch 960: Loss=4.9133, Acc=12.38%, Induction Acc=16.26%\n",
      "Epoch 970: Loss=4.8829, Acc=11.99%, Induction Acc=15.68%\n",
      "Epoch 980: Loss=4.8306, Acc=12.70%, Induction Acc=16.74%\n",
      "Epoch 990: Loss=4.7968, Acc=13.81%, Induction Acc=18.01%\n",
      "Epoch 1000: Loss=4.8443, Acc=13.29%, Induction Acc=17.43%\n",
      "Epoch 1010: Loss=4.7654, Acc=14.72%, Induction Acc=19.54%\n",
      "Epoch 1020: Loss=4.7554, Acc=14.32%, Induction Acc=18.75%\n",
      "Epoch 1030: Loss=4.7469, Acc=14.72%, Induction Acc=19.60%\n",
      "Epoch 1040: Loss=4.7345, Acc=15.23%, Induction Acc=19.81%\n",
      "Epoch 1050: Loss=4.6782, Acc=15.07%, Induction Acc=19.92%\n",
      "Epoch 1060: Loss=4.6664, Acc=16.69%, Induction Acc=21.82%\n",
      "Epoch 1070: Loss=4.7612, Acc=15.51%, Induction Acc=20.34%\n",
      "Epoch 1080: Loss=4.6431, Acc=18.08%, Induction Acc=23.78%\n",
      "Epoch 1090: Loss=4.6674, Acc=16.46%, Induction Acc=21.50%\n",
      "Epoch 1100: Loss=4.6396, Acc=15.39%, Induction Acc=20.23%\n",
      "Epoch 1110: Loss=4.6219, Acc=15.15%, Induction Acc=19.86%\n",
      "Epoch 1120: Loss=4.6240, Acc=18.12%, Induction Acc=23.73%\n",
      "Epoch 1130: Loss=4.5291, Acc=16.57%, Induction Acc=21.72%\n",
      "Epoch 1140: Loss=4.5518, Acc=17.44%, Induction Acc=22.88%\n",
      "Epoch 1150: Loss=4.5665, Acc=18.47%, Induction Acc=24.21%\n",
      "Epoch 1160: Loss=4.4998, Acc=18.99%, Induction Acc=24.95%\n",
      "Epoch 1170: Loss=4.4478, Acc=18.91%, Induction Acc=24.95%\n",
      "Epoch 1180: Loss=4.5085, Acc=18.51%, Induction Acc=24.31%\n",
      "Epoch 1190: Loss=4.4123, Acc=20.49%, Induction Acc=27.12%\n",
      "Epoch 1200: Loss=4.3662, Acc=19.90%, Induction Acc=26.32%\n",
      "Epoch 1210: Loss=4.3693, Acc=19.62%, Induction Acc=26.01%\n",
      "Epoch 1220: Loss=4.3381, Acc=20.65%, Induction Acc=27.28%\n",
      "Epoch 1230: Loss=4.3432, Acc=21.80%, Induction Acc=28.87%\n",
      "Epoch 1240: Loss=4.3314, Acc=20.37%, Induction Acc=26.69%\n",
      "Epoch 1250: Loss=4.3513, Acc=21.72%, Induction Acc=28.60%\n",
      "Epoch 1260: Loss=4.2070, Acc=21.72%, Induction Acc=28.76%\n",
      "Epoch 1270: Loss=4.2039, Acc=22.47%, Induction Acc=29.71%\n",
      "Epoch 1280: Loss=4.2192, Acc=23.30%, Induction Acc=30.61%\n",
      "Epoch 1290: Loss=4.1682, Acc=23.42%, Induction Acc=31.04%\n",
      "Epoch 1300: Loss=4.1592, Acc=23.93%, Induction Acc=31.83%\n",
      "Epoch 1310: Loss=4.2166, Acc=23.10%, Induction Acc=30.77%\n",
      "Epoch 1320: Loss=4.1609, Acc=24.37%, Induction Acc=32.15%\n",
      "Epoch 1330: Loss=4.0874, Acc=25.08%, Induction Acc=33.21%\n",
      "Epoch 1340: Loss=4.0272, Acc=25.87%, Induction Acc=34.22%\n",
      "Epoch 1350: Loss=4.0103, Acc=26.42%, Induction Acc=35.01%\n",
      "Epoch 1360: Loss=4.0422, Acc=26.94%, Induction Acc=35.65%\n",
      "Epoch 1370: Loss=3.9865, Acc=27.14%, Induction Acc=35.65%\n",
      "Epoch 1380: Loss=4.0068, Acc=27.61%, Induction Acc=36.76%\n",
      "Epoch 1390: Loss=3.9365, Acc=27.65%, Induction Acc=36.60%\n",
      "Epoch 1400: Loss=3.9152, Acc=27.37%, Induction Acc=35.91%\n",
      "Epoch 1410: Loss=3.8680, Acc=30.38%, Induction Acc=40.20%\n",
      "Epoch 1420: Loss=3.9152, Acc=27.73%, Induction Acc=36.76%\n",
      "Epoch 1430: Loss=3.8705, Acc=28.20%, Induction Acc=37.24%\n",
      "Epoch 1440: Loss=3.8815, Acc=29.55%, Induction Acc=39.14%\n",
      "Epoch 1450: Loss=3.7926, Acc=30.18%, Induction Acc=39.83%\n",
      "Epoch 1460: Loss=3.8348, Acc=28.76%, Induction Acc=38.24%\n",
      "Epoch 1470: Loss=3.7350, Acc=31.29%, Induction Acc=41.31%\n",
      "Epoch 1480: Loss=3.7433, Acc=31.25%, Induction Acc=41.53%\n",
      "Epoch 1490: Loss=3.6666, Acc=31.29%, Induction Acc=41.47%\n",
      "Epoch 1500: Loss=3.7110, Acc=30.54%, Induction Acc=40.52%\n",
      "Epoch 1510: Loss=3.6172, Acc=33.47%, Induction Acc=44.60%\n",
      "Epoch 1520: Loss=3.6290, Acc=32.63%, Induction Acc=43.17%\n",
      "Epoch 1530: Loss=3.4633, Acc=34.77%, Induction Acc=45.76%\n",
      "Epoch 1540: Loss=3.5291, Acc=34.26%, Induction Acc=45.50%\n",
      "Epoch 1550: Loss=3.5348, Acc=33.86%, Induction Acc=44.92%\n",
      "Epoch 1560: Loss=3.4387, Acc=35.56%, Induction Acc=47.25%\n",
      "Epoch 1570: Loss=3.4617, Acc=36.59%, Induction Acc=48.20%\n",
      "Epoch 1580: Loss=3.4196, Acc=37.30%, Induction Acc=49.58%\n",
      "Epoch 1590: Loss=3.4743, Acc=34.65%, Induction Acc=46.13%\n",
      "Epoch 1600: Loss=3.3908, Acc=35.96%, Induction Acc=47.46%\n",
      "Epoch 1610: Loss=3.3589, Acc=37.10%, Induction Acc=49.15%\n",
      "Epoch 1620: Loss=3.3560, Acc=36.47%, Induction Acc=48.52%\n",
      "Epoch 1630: Loss=3.3388, Acc=38.81%, Induction Acc=51.48%\n",
      "Epoch 1640: Loss=3.3043, Acc=37.97%, Induction Acc=50.53%\n",
      "Epoch 1650: Loss=3.2844, Acc=38.17%, Induction Acc=50.53%\n",
      "Epoch 1660: Loss=3.3209, Acc=38.41%, Induction Acc=51.01%\n",
      "Epoch 1670: Loss=3.1894, Acc=39.32%, Induction Acc=52.22%\n",
      "Epoch 1680: Loss=3.1738, Acc=40.43%, Induction Acc=53.60%\n",
      "Epoch 1690: Loss=3.1739, Acc=40.66%, Induction Acc=53.81%\n",
      "Epoch 1700: Loss=3.0840, Acc=41.61%, Induction Acc=55.46%\n",
      "Epoch 1710: Loss=3.1204, Acc=42.01%, Induction Acc=55.67%\n",
      "Epoch 1720: Loss=3.0851, Acc=41.97%, Induction Acc=55.56%\n",
      "Epoch 1730: Loss=3.0503, Acc=44.26%, Induction Acc=58.63%\n",
      "Epoch 1740: Loss=3.0450, Acc=43.20%, Induction Acc=57.36%\n",
      "Epoch 1750: Loss=3.0338, Acc=42.80%, Induction Acc=56.62%\n",
      "Epoch 1760: Loss=2.9972, Acc=45.37%, Induction Acc=59.96%\n",
      "Epoch 1770: Loss=3.0509, Acc=43.91%, Induction Acc=57.94%\n",
      "Epoch 1780: Loss=2.9783, Acc=44.94%, Induction Acc=59.53%\n",
      "Epoch 1790: Loss=2.8927, Acc=47.15%, Induction Acc=62.66%\n",
      "Epoch 1800: Loss=2.9142, Acc=46.40%, Induction Acc=61.39%\n",
      "Epoch 1810: Loss=2.8149, Acc=48.06%, Induction Acc=63.35%\n",
      "Epoch 1820: Loss=2.8152, Acc=47.43%, Induction Acc=62.92%\n",
      "Epoch 1830: Loss=2.7574, Acc=49.88%, Induction Acc=66.15%\n",
      "Epoch 1840: Loss=2.8478, Acc=48.18%, Induction Acc=63.88%\n",
      "Epoch 1850: Loss=2.6863, Acc=51.58%, Induction Acc=68.22%\n",
      "Epoch 1860: Loss=2.7520, Acc=49.01%, Induction Acc=64.83%\n",
      "Epoch 1870: Loss=2.7350, Acc=50.67%, Induction Acc=67.00%\n",
      "Epoch 1880: Loss=2.7201, Acc=50.36%, Induction Acc=66.79%\n",
      "Epoch 1890: Loss=2.7032, Acc=50.44%, Induction Acc=67.00%\n",
      "Epoch 1900: Loss=2.6914, Acc=50.99%, Induction Acc=67.48%\n",
      "Epoch 1910: Loss=2.6314, Acc=54.03%, Induction Acc=71.77%\n",
      "Epoch 1920: Loss=2.6803, Acc=52.41%, Induction Acc=69.60%\n",
      "Epoch 1930: Loss=2.5918, Acc=54.31%, Induction Acc=72.03%\n",
      "Epoch 1940: Loss=2.5617, Acc=55.10%, Induction Acc=72.83%\n",
      "Epoch 1950: Loss=2.5768, Acc=54.71%, Induction Acc=72.78%\n",
      "Epoch 1960: Loss=2.5598, Acc=54.63%, Induction Acc=72.30%\n",
      "Epoch 1970: Loss=2.5732, Acc=54.35%, Induction Acc=72.03%\n",
      "Epoch 1980: Loss=2.5567, Acc=55.02%, Induction Acc=72.93%\n",
      "Epoch 1990: Loss=2.5435, Acc=55.06%, Induction Acc=73.04%\n",
      "Epoch 2000: Loss=2.4394, Acc=56.41%, Induction Acc=74.36%\n",
      "Epoch 2010: Loss=2.4279, Acc=58.07%, Induction Acc=76.91%\n",
      "Epoch 2020: Loss=2.4504, Acc=57.52%, Induction Acc=76.22%\n",
      "Epoch 2030: Loss=2.5146, Acc=56.17%, Induction Acc=74.42%\n",
      "Epoch 2040: Loss=2.3999, Acc=58.58%, Induction Acc=77.49%\n",
      "Epoch 2050: Loss=2.3605, Acc=59.26%, Induction Acc=78.39%\n",
      "Epoch 2060: Loss=2.3440, Acc=59.85%, Induction Acc=78.92%\n",
      "Epoch 2070: Loss=2.3237, Acc=59.93%, Induction Acc=79.61%\n",
      "Epoch 2080: Loss=2.3046, Acc=61.12%, Induction Acc=80.83%\n",
      "Epoch 2090: Loss=2.2893, Acc=62.42%, Induction Acc=82.73%\n",
      "Epoch 2100: Loss=2.3163, Acc=61.75%, Induction Acc=81.83%\n",
      "Epoch 2110: Loss=2.2899, Acc=61.59%, Induction Acc=81.57%\n",
      "Epoch 2120: Loss=2.2567, Acc=63.13%, Induction Acc=83.47%\n",
      "Epoch 2130: Loss=2.1845, Acc=64.12%, Induction Acc=84.90%\n",
      "Epoch 2140: Loss=2.2088, Acc=62.66%, Induction Acc=83.10%\n",
      "Epoch 2150: Loss=2.1544, Acc=66.26%, Induction Acc=87.82%\n",
      "Epoch 2160: Loss=2.2034, Acc=64.24%, Induction Acc=84.90%\n",
      "Epoch 2170: Loss=2.1684, Acc=64.79%, Induction Acc=85.86%\n",
      "Epoch 2180: Loss=2.1215, Acc=66.30%, Induction Acc=87.71%\n",
      "Epoch 2190: Loss=2.1355, Acc=65.86%, Induction Acc=87.18%\n",
      "Epoch 2200: Loss=2.0911, Acc=66.97%, Induction Acc=88.77%\n",
      "Epoch 2210: Loss=2.1691, Acc=66.30%, Induction Acc=87.50%\n",
      "Epoch 2220: Loss=2.1216, Acc=67.72%, Induction Acc=89.72%\n",
      "Epoch 2230: Loss=2.0747, Acc=67.84%, Induction Acc=89.88%\n",
      "Epoch 2240: Loss=2.1004, Acc=67.52%, Induction Acc=89.30%\n",
      "Epoch 2250: Loss=2.0447, Acc=68.71%, Induction Acc=90.68%\n",
      "Epoch 2260: Loss=2.0524, Acc=68.24%, Induction Acc=90.36%\n",
      "Epoch 2270: Loss=2.0674, Acc=68.20%, Induction Acc=90.15%\n",
      "Epoch 2280: Loss=2.0622, Acc=68.63%, Induction Acc=90.89%\n",
      "Epoch 2290: Loss=1.9918, Acc=69.15%, Induction Acc=91.53%\n",
      "Epoch 2300: Loss=2.0384, Acc=68.75%, Induction Acc=90.89%\n",
      "Epoch 2310: Loss=2.0025, Acc=69.78%, Induction Acc=92.00%\n",
      "Epoch 2320: Loss=1.9518, Acc=70.37%, Induction Acc=93.06%\n",
      "Epoch 2330: Loss=1.9662, Acc=70.65%, Induction Acc=93.49%\n",
      "Epoch 2340: Loss=1.9877, Acc=69.90%, Induction Acc=92.48%\n",
      "Epoch 2350: Loss=1.9233, Acc=71.24%, Induction Acc=94.17%\n",
      "Epoch 2360: Loss=1.9410, Acc=71.56%, Induction Acc=94.65%\n",
      "Epoch 2370: Loss=1.9269, Acc=71.00%, Induction Acc=94.17%\n",
      "Epoch 2380: Loss=1.9266, Acc=72.19%, Induction Acc=95.50%\n",
      "Epoch 2390: Loss=1.9640, Acc=70.89%, Induction Acc=93.70%\n",
      "Epoch 2400: Loss=1.8955, Acc=71.52%, Induction Acc=94.44%\n",
      "Epoch 2410: Loss=1.9513, Acc=70.53%, Induction Acc=93.27%\n",
      "Epoch 2420: Loss=1.8773, Acc=72.71%, Induction Acc=96.08%\n",
      "Epoch 2430: Loss=1.9003, Acc=71.88%, Induction Acc=95.18%\n",
      "Epoch 2440: Loss=1.8995, Acc=71.52%, Induction Acc=94.92%\n",
      "Epoch 2450: Loss=1.8932, Acc=72.51%, Induction Acc=95.76%\n",
      "Epoch 2460: Loss=1.8928, Acc=72.75%, Induction Acc=96.24%\n",
      "Epoch 2470: Loss=1.8861, Acc=72.98%, Induction Acc=96.66%\n",
      "Epoch 2480: Loss=1.8437, Acc=72.90%, Induction Acc=96.13%\n",
      "Epoch 2490: Loss=1.8567, Acc=72.47%, Induction Acc=95.82%\n",
      "Epoch 2500: Loss=1.8506, Acc=73.14%, Induction Acc=96.56%\n",
      "Epoch 2510: Loss=1.8191, Acc=73.58%, Induction Acc=97.09%\n",
      "Epoch 2520: Loss=1.8218, Acc=74.49%, Induction Acc=98.36%\n",
      "Epoch 2530: Loss=1.8401, Acc=73.34%, Induction Acc=96.77%\n",
      "Epoch 2540: Loss=1.7924, Acc=74.25%, Induction Acc=98.15%\n",
      "Epoch 2550: Loss=1.7946, Acc=74.29%, Induction Acc=97.72%\n",
      "Epoch 2560: Loss=1.8170, Acc=73.69%, Induction Acc=97.14%\n",
      "Epoch 2570: Loss=1.8275, Acc=73.54%, Induction Acc=97.09%\n",
      "Epoch 2580: Loss=1.8063, Acc=74.05%, Induction Acc=97.83%\n",
      "Epoch 2590: Loss=1.8132, Acc=72.78%, Induction Acc=96.13%\n",
      "Epoch 2600: Loss=1.7638, Acc=74.09%, Induction Acc=97.62%\n",
      "Epoch 2610: Loss=1.8053, Acc=74.33%, Induction Acc=97.93%\n",
      "Epoch 2620: Loss=1.7765, Acc=74.01%, Induction Acc=97.67%\n",
      "Epoch 2630: Loss=1.7241, Acc=74.80%, Induction Acc=98.46%\n",
      "Epoch 2640: Loss=1.7788, Acc=74.33%, Induction Acc=98.20%\n",
      "Epoch 2650: Loss=1.7563, Acc=74.68%, Induction Acc=98.20%\n",
      "Epoch 2660: Loss=1.7701, Acc=74.37%, Induction Acc=97.88%\n",
      "Epoch 2670: Loss=1.7533, Acc=74.76%, Induction Acc=98.62%\n",
      "Epoch 2680: Loss=1.7293, Acc=74.76%, Induction Acc=98.41%\n",
      "Epoch 2690: Loss=1.7692, Acc=74.53%, Induction Acc=98.52%\n",
      "Epoch 2700: Loss=1.7473, Acc=74.76%, Induction Acc=98.46%\n",
      "Epoch 2710: Loss=1.7272, Acc=74.84%, Induction Acc=98.57%\n",
      "Epoch 2720: Loss=1.7253, Acc=75.16%, Induction Acc=98.89%\n",
      "Epoch 2730: Loss=1.7118, Acc=75.24%, Induction Acc=98.94%\n",
      "Epoch 2740: Loss=1.7094, Acc=75.12%, Induction Acc=98.94%\n",
      "Epoch 2750: Loss=1.7194, Acc=75.16%, Induction Acc=98.99%\n",
      "Epoch 2760: Loss=1.7181, Acc=75.04%, Induction Acc=98.68%\n",
      "Epoch 2770: Loss=1.7119, Acc=74.92%, Induction Acc=98.78%\n",
      "Epoch 2780: Loss=1.7162, Acc=75.08%, Induction Acc=99.05%\n",
      "Epoch 2790: Loss=1.6983, Acc=74.96%, Induction Acc=98.89%\n",
      "Epoch 2800: Loss=1.6975, Acc=75.20%, Induction Acc=99.15%\n",
      "Epoch 2810: Loss=1.6916, Acc=75.47%, Induction Acc=99.36%\n",
      "Epoch 2820: Loss=1.6915, Acc=75.44%, Induction Acc=99.36%\n",
      "Epoch 2830: Loss=1.6923, Acc=75.47%, Induction Acc=99.31%\n",
      "Epoch 2840: Loss=1.6893, Acc=74.96%, Induction Acc=98.68%\n",
      "Epoch 2850: Loss=1.6856, Acc=75.32%, Induction Acc=99.21%\n",
      "Epoch 2860: Loss=1.7001, Acc=75.16%, Induction Acc=98.78%\n",
      "Epoch 2870: Loss=1.6826, Acc=75.51%, Induction Acc=99.47%\n",
      "Epoch 2880: Loss=1.6696, Acc=75.40%, Induction Acc=99.36%\n",
      "Epoch 2890: Loss=1.6958, Acc=75.55%, Induction Acc=99.47%\n",
      "Epoch 2900: Loss=1.6828, Acc=75.20%, Induction Acc=98.99%\n",
      "Epoch 2910: Loss=1.7090, Acc=75.51%, Induction Acc=99.47%\n",
      "Epoch 2920: Loss=1.6530, Acc=75.51%, Induction Acc=99.52%\n",
      "Epoch 2930: Loss=1.6968, Acc=75.00%, Induction Acc=98.73%\n",
      "Epoch 2940: Loss=1.6939, Acc=75.24%, Induction Acc=99.21%\n",
      "Epoch 2950: Loss=1.6582, Acc=75.28%, Induction Acc=98.94%\n",
      "Epoch 2960: Loss=1.6504, Acc=75.47%, Induction Acc=99.42%\n",
      "Epoch 2970: Loss=1.6672, Acc=75.40%, Induction Acc=99.21%\n",
      "Epoch 2980: Loss=1.6390, Acc=75.36%, Induction Acc=99.42%\n",
      "Epoch 2990: Loss=1.6559, Acc=75.44%, Induction Acc=99.47%\n",
      "Epoch 3000: Loss=1.6456, Acc=75.36%, Induction Acc=99.10%\n",
      "Epoch 3010: Loss=1.6516, Acc=75.44%, Induction Acc=99.31%\n",
      "Epoch 3020: Loss=1.6635, Acc=75.55%, Induction Acc=99.63%\n",
      "Epoch 3030: Loss=1.6307, Acc=75.63%, Induction Acc=99.42%\n",
      "Epoch 3040: Loss=1.6299, Acc=75.63%, Induction Acc=99.42%\n",
      "Epoch 3050: Loss=1.6400, Acc=75.55%, Induction Acc=99.36%\n",
      "Epoch 3060: Loss=1.6304, Acc=75.75%, Induction Acc=99.58%\n",
      "Epoch 3070: Loss=1.6314, Acc=75.55%, Induction Acc=99.42%\n",
      "Epoch 3080: Loss=1.6470, Acc=75.63%, Induction Acc=99.47%\n",
      "Epoch 3090: Loss=1.6231, Acc=75.71%, Induction Acc=99.63%\n",
      "Epoch 3100: Loss=1.6291, Acc=75.55%, Induction Acc=99.36%\n",
      "Epoch 3110: Loss=1.6160, Acc=75.87%, Induction Acc=99.63%\n",
      "Epoch 3120: Loss=1.6357, Acc=75.47%, Induction Acc=99.31%\n",
      "Epoch 3130: Loss=1.6382, Acc=75.83%, Induction Acc=99.74%\n",
      "Epoch 3140: Loss=1.6529, Acc=75.67%, Induction Acc=99.68%\n",
      "Epoch 3150: Loss=1.6244, Acc=75.79%, Induction Acc=99.84%\n",
      "Epoch 3160: Loss=1.6363, Acc=75.59%, Induction Acc=99.47%\n",
      "Epoch 3170: Loss=1.6302, Acc=75.44%, Induction Acc=99.31%\n",
      "Epoch 3180: Loss=1.6240, Acc=75.79%, Induction Acc=99.63%\n",
      "Epoch 3190: Loss=1.6087, Acc=75.83%, Induction Acc=99.68%\n",
      "Epoch 3200: Loss=1.6145, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 3210: Loss=1.6308, Acc=75.87%, Induction Acc=99.84%\n",
      "Epoch 3220: Loss=1.6114, Acc=75.67%, Induction Acc=99.47%\n",
      "Epoch 3230: Loss=1.5984, Acc=75.79%, Induction Acc=99.74%\n",
      "Epoch 3240: Loss=1.6084, Acc=75.79%, Induction Acc=99.63%\n",
      "Epoch 3250: Loss=1.6188, Acc=75.59%, Induction Acc=99.52%\n",
      "Epoch 3260: Loss=1.5960, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3270: Loss=1.6248, Acc=75.95%, Induction Acc=99.79%\n",
      "Epoch 3280: Loss=1.5986, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 3290: Loss=1.6192, Acc=75.44%, Induction Acc=99.26%\n",
      "Epoch 3300: Loss=1.6025, Acc=75.83%, Induction Acc=99.74%\n",
      "Epoch 3310: Loss=1.6059, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 3320: Loss=1.6031, Acc=75.71%, Induction Acc=99.68%\n",
      "Epoch 3330: Loss=1.6064, Acc=75.75%, Induction Acc=99.68%\n",
      "Epoch 3340: Loss=1.5888, Acc=75.71%, Induction Acc=99.52%\n",
      "Epoch 3350: Loss=1.5950, Acc=75.71%, Induction Acc=99.68%\n",
      "Epoch 3360: Loss=1.5857, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 3370: Loss=1.6008, Acc=75.79%, Induction Acc=99.68%\n",
      "Epoch 3380: Loss=1.6009, Acc=75.83%, Induction Acc=99.68%\n",
      "Epoch 3390: Loss=1.5820, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3400: Loss=1.5995, Acc=75.95%, Induction Acc=99.79%\n",
      "Epoch 3410: Loss=1.5871, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 3420: Loss=1.5919, Acc=75.75%, Induction Acc=99.63%\n",
      "Epoch 3430: Loss=1.5862, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 3440: Loss=1.5834, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 3450: Loss=1.5892, Acc=75.79%, Induction Acc=99.74%\n",
      "Epoch 3460: Loss=1.6011, Acc=75.99%, Induction Acc=99.79%\n",
      "Epoch 3470: Loss=1.5820, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 3480: Loss=1.5919, Acc=75.87%, Induction Acc=99.63%\n",
      "Epoch 3490: Loss=1.5915, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3500: Loss=1.6004, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 3510: Loss=1.5793, Acc=75.83%, Induction Acc=99.68%\n",
      "Epoch 3520: Loss=1.5676, Acc=75.99%, Induction Acc=99.84%\n",
      "Epoch 3530: Loss=1.5746, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3540: Loss=1.5980, Acc=75.63%, Induction Acc=99.58%\n",
      "Epoch 3550: Loss=1.5876, Acc=75.87%, Induction Acc=99.74%\n",
      "Epoch 3560: Loss=1.5771, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 3570: Loss=1.5615, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 3580: Loss=1.5673, Acc=75.67%, Induction Acc=99.58%\n",
      "Epoch 3590: Loss=1.5753, Acc=75.75%, Induction Acc=99.79%\n",
      "Epoch 3600: Loss=1.5835, Acc=75.83%, Induction Acc=99.89%\n",
      "Epoch 3610: Loss=1.5846, Acc=75.79%, Induction Acc=99.74%\n",
      "Epoch 3620: Loss=1.5596, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3630: Loss=1.5638, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 3640: Loss=1.5838, Acc=75.71%, Induction Acc=99.58%\n",
      "Epoch 3650: Loss=1.5918, Acc=75.87%, Induction Acc=99.74%\n",
      "Epoch 3660: Loss=1.5633, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 3670: Loss=1.5548, Acc=75.91%, Induction Acc=99.74%\n",
      "Epoch 3680: Loss=1.5637, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 3690: Loss=1.5827, Acc=75.83%, Induction Acc=99.74%\n",
      "Epoch 3700: Loss=1.5573, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 3710: Loss=1.5652, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 3720: Loss=1.5787, Acc=75.83%, Induction Acc=99.74%\n",
      "Epoch 3730: Loss=1.5613, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 3740: Loss=1.5662, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3750: Loss=1.5877, Acc=75.87%, Induction Acc=99.58%\n",
      "Epoch 3760: Loss=1.5470, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 3770: Loss=1.5772, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3780: Loss=1.5534, Acc=75.79%, Induction Acc=99.84%\n",
      "Epoch 3790: Loss=1.5961, Acc=75.55%, Induction Acc=99.52%\n",
      "Epoch 3800: Loss=1.5332, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 3810: Loss=1.5780, Acc=75.75%, Induction Acc=99.74%\n",
      "Epoch 3820: Loss=1.5523, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 3830: Loss=1.5758, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 3840: Loss=1.5600, Acc=75.83%, Induction Acc=99.74%\n",
      "Epoch 3850: Loss=1.5524, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 3860: Loss=1.5659, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 3870: Loss=1.5456, Acc=75.75%, Induction Acc=99.58%\n",
      "Epoch 3880: Loss=1.5601, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 3890: Loss=1.5571, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 3900: Loss=1.5513, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3910: Loss=1.5629, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3920: Loss=1.5452, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 3930: Loss=1.5477, Acc=75.91%, Induction Acc=99.79%\n",
      "Epoch 3940: Loss=1.5529, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 3950: Loss=1.5421, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 3960: Loss=1.5577, Acc=76.11%, Induction Acc=99.84%\n",
      "Epoch 3970: Loss=1.5601, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 3980: Loss=1.5649, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 3990: Loss=1.5409, Acc=75.83%, Induction Acc=99.74%\n",
      "Epoch 4000: Loss=1.5402, Acc=75.87%, Induction Acc=99.84%\n",
      "Epoch 4010: Loss=1.5480, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 4020: Loss=1.5640, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 4030: Loss=1.5464, Acc=75.87%, Induction Acc=99.89%\n",
      "Epoch 4040: Loss=1.5515, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 4050: Loss=1.5537, Acc=75.99%, Induction Acc=99.84%\n",
      "Epoch 4060: Loss=1.5530, Acc=75.87%, Induction Acc=99.84%\n",
      "Epoch 4070: Loss=1.5440, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 4080: Loss=1.5515, Acc=75.75%, Induction Acc=99.84%\n",
      "Epoch 4090: Loss=1.5430, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4100: Loss=1.5306, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4110: Loss=1.5513, Acc=75.87%, Induction Acc=99.89%\n",
      "Epoch 4120: Loss=1.5457, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 4130: Loss=1.5585, Acc=75.75%, Induction Acc=99.74%\n",
      "Epoch 4140: Loss=1.5582, Acc=75.87%, Induction Acc=99.84%\n",
      "Epoch 4150: Loss=1.5442, Acc=75.79%, Induction Acc=99.68%\n",
      "Epoch 4160: Loss=1.5476, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 4170: Loss=1.5439, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4180: Loss=1.5387, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 4190: Loss=1.5392, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4200: Loss=1.5431, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4210: Loss=1.5277, Acc=76.07%, Induction Acc=99.89%\n",
      "Epoch 4220: Loss=1.5263, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4230: Loss=1.5331, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4240: Loss=1.5469, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 4250: Loss=1.5341, Acc=75.91%, Induction Acc=99.79%\n",
      "Epoch 4260: Loss=1.5334, Acc=75.91%, Induction Acc=99.95%\n",
      "Epoch 4270: Loss=1.5375, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4280: Loss=1.5337, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4290: Loss=1.5529, Acc=75.79%, Induction Acc=99.58%\n",
      "Epoch 4300: Loss=1.5312, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4310: Loss=1.5502, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 4320: Loss=1.5480, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 4330: Loss=1.5217, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4340: Loss=1.5429, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4350: Loss=1.5277, Acc=75.87%, Induction Acc=99.89%\n",
      "Epoch 4360: Loss=1.5212, Acc=75.99%, Induction Acc=99.79%\n",
      "Epoch 4370: Loss=1.5307, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 4380: Loss=1.5378, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 4390: Loss=1.5307, Acc=75.83%, Induction Acc=99.84%\n",
      "Epoch 4400: Loss=1.5238, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 4410: Loss=1.5270, Acc=75.67%, Induction Acc=99.52%\n",
      "Epoch 4420: Loss=1.5206, Acc=75.79%, Induction Acc=99.79%\n",
      "Epoch 4430: Loss=1.5209, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 4440: Loss=1.5290, Acc=76.03%, Induction Acc=99.89%\n",
      "Epoch 4450: Loss=1.5157, Acc=76.19%, Induction Acc=100.00%\n",
      "Epoch 4460: Loss=1.5124, Acc=75.91%, Induction Acc=99.79%\n",
      "Epoch 4470: Loss=1.5201, Acc=75.91%, Induction Acc=99.95%\n",
      "Epoch 4480: Loss=1.5190, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 4490: Loss=1.5305, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 4500: Loss=1.5213, Acc=76.11%, Induction Acc=99.89%\n",
      "Epoch 4510: Loss=1.5313, Acc=75.87%, Induction Acc=99.84%\n",
      "Epoch 4520: Loss=1.5309, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 4530: Loss=1.5286, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4540: Loss=1.5135, Acc=76.03%, Induction Acc=99.89%\n",
      "Epoch 4550: Loss=1.5265, Acc=75.91%, Induction Acc=99.95%\n",
      "Epoch 4560: Loss=1.5317, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4570: Loss=1.5353, Acc=75.99%, Induction Acc=100.00%\n",
      "Epoch 4580: Loss=1.5149, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 4590: Loss=1.5211, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 4600: Loss=1.5441, Acc=76.11%, Induction Acc=99.95%\n",
      "Epoch 4610: Loss=1.5041, Acc=75.99%, Induction Acc=99.79%\n",
      "Epoch 4620: Loss=1.5154, Acc=76.07%, Induction Acc=100.00%\n",
      "Epoch 4630: Loss=1.5268, Acc=75.95%, Induction Acc=99.79%\n",
      "Epoch 4640: Loss=1.5272, Acc=75.95%, Induction Acc=99.84%\n",
      "Epoch 4650: Loss=1.5119, Acc=75.91%, Induction Acc=99.74%\n",
      "Epoch 4660: Loss=1.5188, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 4670: Loss=1.5309, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 4680: Loss=1.5153, Acc=76.11%, Induction Acc=99.95%\n",
      "Epoch 4690: Loss=1.5072, Acc=76.03%, Induction Acc=100.00%\n",
      "Epoch 4700: Loss=1.5239, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 4710: Loss=1.5271, Acc=75.75%, Induction Acc=99.68%\n",
      "Epoch 4720: Loss=1.5323, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 4730: Loss=1.5311, Acc=75.91%, Induction Acc=99.95%\n",
      "Epoch 4740: Loss=1.5055, Acc=76.15%, Induction Acc=99.95%\n",
      "Epoch 4750: Loss=1.5013, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 4760: Loss=1.5237, Acc=75.99%, Induction Acc=99.79%\n",
      "Epoch 4770: Loss=1.5167, Acc=75.99%, Induction Acc=99.89%\n",
      "Epoch 4780: Loss=1.5001, Acc=75.95%, Induction Acc=99.95%\n",
      "Epoch 4790: Loss=1.5041, Acc=75.95%, Induction Acc=100.00%\n",
      "Epoch 4800: Loss=1.5089, Acc=75.83%, Induction Acc=99.74%\n",
      "Epoch 4810: Loss=1.5108, Acc=75.87%, Induction Acc=99.89%\n",
      "Epoch 4820: Loss=1.5256, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 4830: Loss=1.5233, Acc=75.95%, Induction Acc=99.74%\n",
      "Epoch 4840: Loss=1.5153, Acc=75.83%, Induction Acc=99.79%\n",
      "Epoch 4850: Loss=1.5155, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4860: Loss=1.5175, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 4870: Loss=1.5085, Acc=75.71%, Induction Acc=99.68%\n",
      "Epoch 4880: Loss=1.5229, Acc=76.11%, Induction Acc=100.00%\n",
      "Epoch 4890: Loss=1.5208, Acc=76.03%, Induction Acc=99.89%\n",
      "Epoch 4900: Loss=1.5116, Acc=75.87%, Induction Acc=99.79%\n",
      "Epoch 4910: Loss=1.5005, Acc=75.95%, Induction Acc=99.89%\n",
      "Epoch 4920: Loss=1.5133, Acc=75.63%, Induction Acc=99.47%\n",
      "Epoch 4930: Loss=1.5247, Acc=76.03%, Induction Acc=99.89%\n",
      "Epoch 4940: Loss=1.5182, Acc=75.99%, Induction Acc=99.95%\n",
      "Epoch 4950: Loss=1.4887, Acc=75.91%, Induction Acc=99.84%\n",
      "Epoch 4960: Loss=1.5189, Acc=75.91%, Induction Acc=99.89%\n",
      "Epoch 4970: Loss=1.5214, Acc=75.87%, Induction Acc=99.74%\n",
      "Epoch 4980: Loss=1.4819, Acc=76.03%, Induction Acc=99.95%\n",
      "Epoch 4990: Loss=1.5083, Acc=75.87%, Induction Acc=99.68%\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,           # Relatively high for small models\n",
    "    betas=(0.9, 0.98), # Standard 'Transformer' betas\n",
    "    weight_decay=0.01  # Helps prevent the 'junk drawer' neuron problem\n",
    ")\n",
    "\n",
    "epochs = 5000\n",
    "losses = []\n",
    "accuracies = []\n",
    "induction_accuracies = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Generate data using a fixed prefix length\n",
    "    seq_len, data = generate_rrt(num_samples=32, length=MAX_CONTEXT_LEN, vocab_size=256, seq_len=SEQ_LEN)\n",
    "\n",
    "    # Move data to the specified device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    input_data = data[:, :-1]\n",
    "    target_data = data[:, 1:]\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model.forward(input_data)\n",
    "\n",
    "    # [b, s, vocab_size] -> [b*s, vocab_size]\n",
    "    input_to_ce = logits.view(-1, model.vocab_size)\n",
    "\n",
    "    # [b, vocab_size] -> [b*vocab_size], equivalent to flattening a 2D tensor into a 1D tensor\n",
    "    target_flat = target_data.reshape(-1)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(input_to_ce, target_flat)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    predictions = logits.argmax(dim=-1)\n",
    "    accuracy = (predictions == target_data).float().mean().item()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Induction accuracy (second half only where the pattern repeats)\n",
    "    induction_acc = (predictions[:, seq_len:] == target_data[:, seq_len:]).float().mean().item()\n",
    "    induction_accuracies.append(induction_acc)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Print every N epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={loss.item():.4f}, Acc={accuracy:.2%}, Induction Acc={induction_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44656cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = results_dir / \"1L_fixed_model.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
